import torch
import torch.nn as nn
import os
import time
import datetime
import logging
import traceback
import copy
import numpy as np
from tqdm import tqdm
import torch.nn.functional as F
from collections import defaultdict
from os2d.modeling.model import Os2dModel
from os2d.modeling.feature_extractor import build_feature_extractor
from src.lcp_channel_selector import OS2DChannelSelector
from src.gIoU_loss import GIoULoss

class Os2dModelInPrune(Os2dModel):
    """
    æ“´å±• OS2D æ¨¡å‹ä»¥æ”¯æŒé€šé“å‰ªæåŠŸèƒ½
    """
    def __init__(self, logger=None, is_cuda=True, backbone_arch="resnet50", 
                 use_group_norm=False, img_normalization=None, 
                 pretrained_path=None, pruned_checkpoint=None, **kwargs):
        # å¦‚æœæ²’æœ‰æä¾› loggerï¼Œå‰µå»ºä¸€å€‹
        if logger is None:
            logger = logging.getLogger("OS2D")
        
        # èª¿ç”¨çˆ¶é¡åˆå§‹åŒ–
        self.device = torch.device('cuda' if is_cuda else 'cpu')
        
        super(Os2dModelInPrune, self).__init__(
            logger=logger,
            is_cuda=is_cuda,
            backbone_arch=backbone_arch,
            use_group_norm=use_group_norm,
            img_normalization=img_normalization,
            **kwargs
        )
        
        self.backbone = self.net_feature_maps
        # å°‡æ¨¡å‹ç§»è‡³æŒ‡å®šè¨­å‚™
        if is_cuda:
            self.cuda()
            self.backbone = self.backbone.cuda()
            # ç¢ºä¿æ‰€æœ‰å­æ¨¡å¡Šéƒ½åœ¨ CUDA ä¸Š
            for module in self.modules():
                if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
                    module.cuda()
        # è¼‰å…¥é è¨“ç·´æ¬Šé‡
        if pretrained_path:
            self.init_model_from_file(pretrained_path)
        self.device = torch.device('cuda' if is_cuda else 'cpu')
        self.original_device = self.device  # ä¿å­˜åŸå§‹è¨­å‚™
        
        self.teacher_model = copy.deepcopy(self)
        self.teacher_model.eval()
        for p in self.teacher_model.parameters():
            p.requires_grad = False

        if pruned_checkpoint:
           self.load_checkpoint(pruned_checkpoint)
        if is_cuda:
            try:
                self.cuda()
                self.backbone = self.backbone.cuda()
                # ç¢ºä¿æ‰€æœ‰å­æ¨¡å¡Šéƒ½åœ¨ CUDA ä¸Š
                for module in self.modules():
                    if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
                        try:
                            module.cuda()
                        except RuntimeError as e:
                            print(f"âš ï¸ æ¨¡å¡Š {type(module)} ç„¡æ³•ç§»è‡³ CUDA: {e}")
                            # å¦‚æœå¤±æ•—ï¼Œå…ˆç”¨ CPU
                            module.cpu()
            except RuntimeError as e:
                print(f"âš ï¸ CUDA åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨ CPU ä½œç‚ºå‚™é¸: {e}")
                self.device = torch.device('cpu')
                self.cpu()
                
    
    def _safe_forward(self, x, device=None, class_images=None):
        """å®‰å…¨çš„å‰å‘å‚³æ’­ï¼Œå¦‚æœ GPU åŸ·è¡Œå¤±æ•—æœƒfallbackåˆ°CPU"""
        if x.dim() == 3:  # å¦‚æœæ˜¯ [C, H, W]
            x = x.unsqueeze(0)  # è½‰æ›ç‚º [1, C, H, W]
        try:
            if class_images is not None:
                return self(x, class_images=class_images)
            return self(x)
        except RuntimeError as e:
            if "Input type" in str(e) and device and device.type == 'cuda':
                print("âš ï¸ GPU åŸ·è¡Œå¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ CPU...")
                # æš«æ™‚å°‡æ¨¡å‹å’Œè¼¸å…¥ç§»åˆ° CPU
                original_device = next(self.parameters()).device
                cpu_model = self.cpu()
                cpu_x = x.cpu()
                cpu_class_images = class_images.cpu() if class_images is not None else None
                
                try:
                    # åœ¨ CPU ä¸ŠåŸ·è¡Œ
                    with torch.no_grad():
                        if cpu_class_images is not None:
                            output = cpu_model(cpu_x, class_images=cpu_class_images)
                        else:
                            output = cpu_model(cpu_x)
                    
                    # åŸ·è¡ŒæˆåŠŸå¾Œç§»å› GPU
                    self.to(original_device)
                    output = output.to(original_device)
                    print("âœ“ CPU åŸ·è¡ŒæˆåŠŸï¼Œå·²ç§»å› GPU")
                    return output
                
                except Exception as cpu_e:
                    print(f"âŒ CPU åŸ·è¡Œä¹Ÿå¤±æ•—: {cpu_e}")
                    # ç¢ºä¿æ¨¡å‹ç§»å›åŸå§‹è¨­å‚™
                    self.to(original_device)
                    raise cpu_e
            else:
                raise e  # å¦‚æœä¸æ˜¯è¨­å‚™å•é¡Œï¼Œå‰‡é‡æ–°å¼•ç™¼éŒ¯èª¤
    def set_layer_out_channels(self, layer_name, new_out_channels):
        """è¨­ç½®æŒ‡å®šå±¤çš„è¼¸å‡ºé€šé“æ•¸"""
        # å°‹æ‰¾ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name:
                target_layer = module
                break
                
        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False
            
        if not isinstance(target_layer, nn.Conv2d):
            print(f"âŒ å±¤ {layer_name} ä¸æ˜¯å·ç©å±¤")
            return False
            
        # å‰µå»ºæ–°çš„å·ç©å±¤
        new_conv = nn.Conv2d(
            in_channels=target_layer.in_channels,
            out_channels=new_out_channels,
            kernel_size=target_layer.kernel_size,
            stride=target_layer.stride,
            padding=target_layer.padding,
            dilation=target_layer.dilation,
            groups=target_layer.groups,
            bias=target_layer.bias is not None
        ).to(target_layer.weight.device)
        
        # å°‡åŸå§‹æ¬Šé‡è¤‡è£½åˆ°æ–°å±¤
        with torch.no_grad():
            # åªè¤‡è£½éœ€è¦çš„é€šé“
            min_channels = min(new_out_channels, target_layer.weight.size(0))
            new_conv.weight.data[:min_channels] = target_layer.weight.data[:min_channels]
            if target_layer.bias is not None:
                new_conv.bias.data[:min_channels] = target_layer.bias.data[:min_channels]
        
        # è¨­ç½®æ–°çš„å±¤
        parts = layer_name.split('.')
        parent = self.backbone
        for i in range(len(parts)-1):
            if parts[i].isdigit():
                parent = parent[int(parts[i])]
            else:
                parent = getattr(parent, parts[i])
        setattr(parent, parts[-1], new_conv)
        
        print(f"âœ“ {layer_name} out_channels å¼·åˆ¶è¨­ç‚º {new_out_channels}")
        
        # åŒæ­¥æ›´æ–° BatchNorm
        if parts[-1].startswith('conv'):
            bn_name = parts[-1].replace('conv', 'bn')
            if hasattr(parent, bn_name):
                old_bn = getattr(parent, bn_name)
                new_bn = nn.BatchNorm2d(new_out_channels).to(old_bn.weight.device)
                min_channels = min(new_out_channels, old_bn.num_features)
                new_bn.weight.data[:min_channels] = old_bn.weight.data[:min_channels]
                new_bn.bias.data[:min_channels] = old_bn.bias.data[:min_channels]
                new_bn.running_mean[:min_channels] = old_bn.running_mean[:min_channels]
                new_bn.running_var[:min_channels] = old_bn.running_var[:min_channels]
                setattr(parent, bn_name, new_bn)
                print(f"âœ“ {'.'.join(parts[:-1])}.{bn_name} é€šé“æ•¸åŒæ­¥è¨­ç‚º {new_out_channels}")
        
        # æ›´æ–°ä¸‹ä¸€å±¤çš„è¼¸å…¥é€šé“
        if parts[-1].startswith('conv'):
            conv_idx = int(parts[-1][-1])
            if conv_idx < 3:  # åªè™•ç† conv1/conv2 åˆ°ä¸‹ä¸€å±¤
                next_conv_name = f"conv{conv_idx+1}"
                if hasattr(parent, next_conv_name):
                    old_next_conv = getattr(parent, next_conv_name)
                    new_next_conv = nn.Conv2d(
                        new_out_channels,
                        old_next_conv.out_channels,
                        old_next_conv.kernel_size,
                        old_next_conv.stride,
                        old_next_conv.padding,
                        old_next_conv.dilation,
                        old_next_conv.groups,
                        bias=old_next_conv.bias is not None
                    ).to(old_next_conv.weight.device)
                    min_channels = min(new_out_channels, old_next_conv.weight.size(1))
                    new_next_conv.weight.data[:, :min_channels] = old_next_conv.weight.data[:, :min_channels]
                    if old_next_conv.bias is not None:
                        new_next_conv.bias.data = old_next_conv.bias.data.clone()
                    setattr(parent, next_conv_name, new_next_conv)
                    print(f"âœ“ {'.'.join(parts[:-1])}.{next_conv_name} in_channels åŒæ­¥è¨­ç‚º {new_out_channels}")
        
        return True
    
    def _should_skip_pruning(self, layer_name):
        """ç¢ºå®šæ˜¯å¦æ‡‰è·³éè©²å±¤çš„å‰ªæä»¥ç¶­æŒ OS2D æ¶æ§‹"""
        import re
        
        # è§£æå±¤åç¨±
        m = re.match(r'(layer\d+)\.(\d+)\.(conv\d+)', layer_name)
        if not m:
            print(layer_name + " ä¸ç¬¦åˆé æœŸæ ¼å¼ï¼Œç„¡æ³•è§£æ")
            return False
        
        layer_prefix, block_idx_str, conv_name = m.groups()
        block_idx = int(block_idx_str)
        
        # æª¢æŸ¥è©²å±¤çš„ block
        layer = getattr(self.backbone, layer_prefix)
        block = layer[block_idx]
        
        # å¦‚æœæ˜¯ layer4ï¼Œæ›´è¬¹æ…ï¼Œå› ç‚ºå®ƒæ˜¯ OS2D çš„æœ€çµ‚è¼¸å‡º
        if layer_prefix == 'layer4':
            return True  # ç‚ºäº† OS2D å…¼å®¹æ€§è·³éå‰ªæ layer4
        
        # å¼·åˆ¶è·³éæ‰€æœ‰ conv3 å±¤
        if conv_name == 'conv3':
            print(f"âš ï¸ è·³éå‰ªæ {layer_name} (åŸå› : conv3å±¤)")
            return True
        
        # å¦‚æœæ˜¯å¸¶æœ‰ downsample çš„ block ä¸­çš„ conv1ï¼Œä½¿ç”¨å°ˆç”¨çš„è™•ç†æ–¹å¼
        if conv_name == 'conv1' and hasattr(block, 'downsample') and block.downsample is not None:
            print(f"â„¹ï¸ ç™¼ç¾å¸¶æœ‰ downsample çš„ conv1: {layer_name}ï¼Œå°‡ä½¿ç”¨å°ˆç”¨è™•ç†æ–¹å¼")
            return False # 
        
        # å¦‚æœæ˜¯æœ€å¾Œä¸€å€‹ blockï¼Œè·³éæ‰€æœ‰å‰ªæä»¥ä¿è­·è·¨å±¤é€£æ¥
        if block_idx == len(layer) - 1:
            return True
        
        return False
    
    def _handle_residual_connection(self, layer_name, keep_indices):
        """è™•ç†æ®˜å·®é€£æ¥"""
        print(f"\nğŸ” é–‹å§‹è™•ç†æ®˜å·®é€£æ¥: {layer_name}")
        
        # è§£æå±¤åç¨±
        parts = layer_name.split('.')
        if len(parts) < 3:
            print(f"âš ï¸ ç„¡æ•ˆçš„å±¤åç¨±æ ¼å¼: {layer_name}")
            return
        parts = layer_name.split('.')
        if parts[-1] == 'conv1':
            print(f"â„¹ï¸ conv1 å‰ªæä¸å½±éŸ¿ downsample è¼¸å‡ºé€šé“")
            return True
        layer_str, block_idx, conv_type = parts[0], int(parts[1]), parts[2]
        
        # ç²å–ç•¶å‰ block
        layer = getattr(self.backbone, layer_str)
        current_block = layer[block_idx]
        
        if conv_type == 'conv1':
            # æ›´æ–° conv2 çš„è¼¸å…¥é€šé“
            next_conv_name = f"{layer_str}.{block_idx}.conv2"
            next_conv = None
            for name, module in self.backbone.named_modules():
                if name == next_conv_name and isinstance(module, nn.Conv2d):
                    next_conv = module
                    break
            
            if next_conv is not None:
                # æª¢æŸ¥ç´¢å¼•ç¯„åœ
                keep_indices = keep_indices[keep_indices < next_conv.weight.size(1)]
                if len(keep_indices) == 0:
                    print(f"âš ï¸ æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºç¯„åœï¼Œè·³éæ›´æ–° {next_conv_name}")
                    return
                # æ›´æ–° conv2 çš„è¼¸å…¥é€šé“
                new_conv = nn.Conv2d(
                    len(keep_indices),
                    next_conv.out_channels,
                    next_conv.kernel_size,
                    next_conv.stride,
                    next_conv.padding,
                    next_conv.dilation,
                    next_conv.groups,
                    bias=next_conv.bias is not None
                ).to(next_conv.weight.device)
                
                # æ›´æ–°æ¬Šé‡ï¼Œç¢ºä¿ç´¢å¼•åœ¨æœ‰æ•ˆç¯„åœå…§
                try:
                    # æ›´æ–°æ¬Šé‡æ™‚ç¢ºä¿ç¶­åº¦åŒ¹é…
                    if next_conv.weight.size(1) != len(keep_indices):
                        # å¦‚æœè¼¸å…¥é€šé“æ•¸ä¸åŒ¹é…ï¼Œéœ€è¦èª¿æ•´æ¬Šé‡
                        old_weight = next_conv.weight.data
                        new_weight = torch.zeros(
                            old_weight.size(0),
                            len(keep_indices),
                            old_weight.size(2),
                            old_weight.size(3),
                            device=old_weight.device
                        )
                        # åªè¤‡è£½æœ‰æ•ˆçš„é€šé“
                        valid_indices = keep_indices[keep_indices < old_weight.size(1)]
                        new_weight[:, :len(valid_indices)] = old_weight[:, valid_indices]
                        new_conv.weight.data = new_weight
                    else:
                        new_conv.weight.data = next_conv.weight.data[:, keep_indices].clone()
                    
                    if next_conv.bias is not None:
                        new_conv.bias.data = next_conv.bias.data.clone()
                except IndexError as e:
                    print(f"âš ï¸ æ›´æ–°æ¬Šé‡æ™‚ç™¼ç”Ÿç´¢å¼•éŒ¯èª¤: {e}")
                    print(f"next_conv.weight.shape: {next_conv.weight.shape}")
                    print(f"keep_indices max: {keep_indices.max()}")
                    print(f"keep_indices min: {keep_indices.min()}")
                    print(f"keep_indices shape: {keep_indices.shape}")
                    return
                
                # æ›¿æ› conv2
                parts2 = next_conv_name.split('.')
                parent = self.backbone
                for i in range(len(parts2) - 1):
                    if parts2[i].isdigit():
                        parent = parent[int(parts2[i])]
                    else:
                        parent = getattr(parent, parts2[i])
                
                setattr(parent, parts2[-1], new_conv)
                print(f"âœ“ æ›´æ–° conv2 è¼¸å…¥é€šé“: {next_conv_name} (in_channels: {new_conv.in_channels})")
                
                # åŒæ­¥æ›´æ–° BatchNorm
                bn_name = next_conv_name.replace('conv', 'bn')
                if hasattr(parent, bn_name.split('.')[-1]):
                    old_bn = getattr(parent, bn_name.split('.')[-1])
                    new_bn = nn.BatchNorm2d(new_conv.out_channels).to(old_bn.weight.device)
                    new_bn.weight.data = old_bn.weight.data.clone()
                    new_bn.bias.data = old_bn.bias.data.clone()
                    new_bn.running_mean = old_bn.running_mean.clone()
                    new_bn.running_var = old_bn.running_var.clone()
                    setattr(parent, bn_name.split('.')[-1], new_bn)
                    print(f"âœ“ æ›´æ–° BatchNorm: {bn_name}")
        
        elif conv_type == 'conv2':
            # æ›´æ–° conv3 çš„è¼¸å…¥é€šé“
            next_conv_name = f"{layer_str}.{block_idx}.conv3"
            next_conv = None
            for name, module in self.backbone.named_modules():
                if name == next_conv_name and isinstance(module, nn.Conv2d):
                    next_conv = module
                    break
            
            if next_conv is not None:
                # æª¢æŸ¥ç´¢å¼•ç¯„åœ
                keep_indices = keep_indices[keep_indices < next_conv.weight.size(1)]
                if len(keep_indices) == 0:
                    print(f"âš ï¸ æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºç¯„åœï¼Œè·³éæ›´æ–° {next_conv_name}")
                    return
                    
                new_conv = nn.Conv2d(
                    len(keep_indices),
                    next_conv.out_channels,
                    next_conv.kernel_size,
                    next_conv.stride,
                    next_conv.padding,
                    next_conv.dilation,
                    next_conv.groups,
                    bias=next_conv.bias is not None
                ).to(next_conv.weight.device)
                
                try:
                    new_conv.weight.data = next_conv.weight.data[:, keep_indices, :, :].clone()
                    if next_conv.bias is not None:
                        new_conv.bias.data = next_conv.bias.data.clone()
                except IndexError as e:
                    print(f"âš ï¸ æ›´æ–°æ¬Šé‡æ™‚ç™¼ç”Ÿç´¢å¼•éŒ¯èª¤: {e}")
                    print(f"next_conv.weight.shape: {next_conv.weight.shape}")
                    print(f"keep_indices max: {keep_indices.max()}")
                    print(f"keep_indices min: {keep_indices.min()}")
                    print(f"keep_indices shape: {keep_indices.shape}")
                    return
                
                parts2 = next_conv_name.split('.')
                parent = self.backbone
                for i in range(len(parts2) - 1):
                    if parts2[i].isdigit():
                        parent = parent[int(parts2[i])]
                    else:
                        parent = getattr(parent, parts2[i])
                
                setattr(parent, parts2[-1], new_conv)
                print(f"âœ“ æ›´æ–° conv3 è¼¸å…¥é€šé“: {next_conv_name}")
    
    def _prune_conv_layer(self, layer_name, keep_indices):
        """
        å‰ªææŒ‡å®šçš„å·ç©å±¤

        Args:
            layer_name: è¦å‰ªæçš„å±¤åç¨±
            keep_indices: è¦ä¿ç•™çš„é€šé“ç´¢å¼•
        """
        # ç²å–ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name and isinstance(module, nn.Conv2d):
                target_layer = module
                break

        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False

        # è¨­ç½®æ–°çš„è¼¸å‡ºé€šé“æ•¸
        self.set_layer_out_channels(layer_name, len(keep_indices))

        # è™•ç†æ®˜å·®é€£æ¥
        self._handle_residual_connection(layer_name, keep_indices)

        # è™•ç† downsample é€£æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
        # parts = layer_name.split('.')
        # layer_str, block_idx, conv_type = parts[0], int(parts[1]), parts[2]
        # block = getattr(self.backbone, layer_str)[block_idx]
        # if conv_type != "conv1" and hasattr(block, "downsample") and block.downsample is not None:
        #     self._handle_downsample_connection(layer_name, keep_indices)

        return True
    
    def _reset_batchnorm_stats(self):
        """é‡ç½®æ‰€æœ‰ BatchNorm å±¤çš„çµ±è¨ˆæ•¸æ“š"""
        for m in self.backbone.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.reset_running_stats()
        print("âœ“ å·²é‡ç½®æ‰€æœ‰ BatchNorm å±¤çš„çµ±è¨ˆæ•¸æ“š")
    
    def prune_channel(self, layer_name, prune_ratio=0.3, images=None, boxes=None, labels=None, auxiliary_net=None):
        """
        å°æŒ‡å®šå±¤é€²è¡Œé€šé“å‰ªæ
        
        Args:
            layer_name: è¦å‰ªæçš„å±¤åç¨±
            prune_ratio: å‰ªææ¯”ä¾‹ (0.0-1.0)
            images, boxes, labels: ç”¨æ–¼è¨ˆç®—é€šé“é‡è¦æ€§çš„æ•¸æ“š
            auxiliary_net: è¼”åŠ©ç¶²è·¯ï¼Œç”¨æ–¼è©•ä¼°é€šé“é‡è¦æ€§
        """
        # æª¢æŸ¥æ˜¯å¦æ‡‰è·³éå‰ªæ
        if self._should_skip_pruning(layer_name):
            print(f"âš ï¸ è·³éå‰ªæå±¤ {layer_name}")
            return "SKIPPED"
        
        # ç²å–ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name and isinstance(module, nn.Conv2d):
                target_layer = module
                break
        
        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False
        
        # è¨ˆç®—é€šé“é‡è¦æ€§
        if images is not None and boxes is not None and auxiliary_net is not None:
            # åˆå§‹åŒ–é€šé“é¸æ“‡å™¨
            channel_selector = OS2DChannelSelector(
                model=self, 
                auxiliary_net=auxiliary_net, 
                device=self.device
            )
            
            # è¨ˆç®—é€šé“é‡è¦æ€§
            importance_scores = channel_selector.compute_importance(layer_name, images, boxes, boxes, labels)
            
            # é¸æ“‡è¦ä¿ç•™çš„é€šé“
            keep_indices = channel_selector.select_channels(layer_name, importance_scores, prune_ratio)
            
            if keep_indices is None:
                print(f"âŒ ç„¡æ³•é¸æ“‡ {layer_name} çš„ä¿ç•™é€šé“")
                return False
        else:
            # å¦‚æœæ²’æœ‰æä¾›æ•¸æ“šï¼Œå‰‡éš¨æ©Ÿé¸æ“‡é€šé“
            num_channels = target_layer.out_channels
            num_keep = int(num_channels * (1 - prune_ratio))
            keep_indices = torch.randperm(num_channels)[:num_keep]
        
        # åŸ·è¡Œå‰ªæ
        success = self._prune_conv_layer(layer_name, keep_indices)
        
        return success
    
    def prune_model(self, prune_ratio=0.3, images=None, boxes=None, labels=None, auxiliary_net=None, prunable_layers=None):
        """
        å‰ªææ•´å€‹æ¨¡å‹ï¼Œæ ¹æ“š LCP å’Œ DCP è«–æ–‡çš„æ–¹æ³•
        
        Args:
            prune_ratio: å‰ªææ¯”ä¾‹ (0.0-1.0)
            images, boxes, labels: ç”¨æ–¼è¨ˆç®—é€šé“é‡è¦æ€§çš„æ•¸æ“š
            auxiliary_net: è¼”åŠ©ç¶²è·¯ï¼Œç”¨æ–¼è©•ä¼°é€šé“é‡è¦æ€§
            prunable_layers: è¦å‰ªæçš„å±¤åˆ—è¡¨ï¼Œå¦‚æœç‚º None å‰‡è‡ªå‹•é¸æ“‡
        """
        if prunable_layers is None:
            # ä¸»è¦é‡å° OS2D ä½¿ç”¨çš„ layer2 å’Œ layer3 é€²è¡Œå‰ªæ
            prunable_layers = []
            for layer_name in ['layer2', 'layer3']:
                layer = getattr(self.backbone, layer_name)
                for block_idx in range(len(layer)):
                    # å°æ–¼æ¯å€‹æ®˜å·®å¡Š
                    for conv_idx in [1, 2]:  # åªå‰ªæ conv1 å’Œ conv2 ä»¥ç¶­æŒæ¶æ§‹
                        conv_name = f"{layer_name}.{block_idx}.conv{conv_idx}"
                        prunable_layers.append(conv_name)
        
        print(f"ğŸ” é–‹å§‹æ¨¡å‹å‰ªæ (LCP + DCP)ï¼Œå‰ªææ¯”ä¾‹: {prune_ratio}...")
        print(f"ğŸ“‹ å¯å‰ªæå±¤: {prunable_layers}")
        
        # æŒ‰é †åºå‰ªææ¯ä¸€å±¤
        pruned_layers = []
        for layer_name in prunable_layers:
            print(f"\nğŸ”§ è™•ç†å±¤: {layer_name}")
            
            # æª¢æŸ¥æ˜¯å¦æ‡‰è·³éå‰ªæ
            if self._should_skip_pruning(layer_name):
                print(f"âš ï¸ è·³éå‰ªæå±¤ {layer_name}")
                continue
            
            # å‰ªæå±¤
            success = self.prune_channel(
                layer_name, 
                prune_ratio=prune_ratio, 
                images=images, 
                boxes=boxes, 
                labels=labels, 
                auxiliary_net=auxiliary_net
            )
            
            if success and success != "SKIPPED":
                pruned_layers.append(layer_name)
        
        # é‡ç½® BatchNorm çµ±è¨ˆæ•¸æ“š
        self._reset_batchnorm_stats()
        
        print(f"âœ… æ¨¡å‹å‰ªæå®Œæˆ! å…±å‰ªæ {len(pruned_layers)}/{len(prunable_layers)} å±¤")
        print(f"æˆåŠŸå‰ªæçš„å±¤: {pruned_layers}")
        
        return pruned_layers
    
    def visualize_model_architecture(self, output_path="model_architecture.png", input_shape=(1, 3, 224, 224)):
        """
        è¦–è¦ºåŒ–æ¨¡å‹æ¶æ§‹ä¸¦ä¿å­˜ç‚ºåœ–ç‰‡
        
        Args:
            output_path: è¼¸å‡ºåœ–ç‰‡è·¯å¾‘
            input_shape: è¼¸å…¥å¼µé‡å½¢ç‹€ï¼Œé»˜èªç‚º (1, 3, 224, 224)
        """
        try:
            import torchviz
            from graphviz import Digraph
            from tqdm import tqdm
            import os
            import time
            from datetime import datetime
            import traceback
        except ImportError:
            print("è«‹å…ˆå®‰è£å¿…è¦çš„å¥—ä»¶: pip install torchviz graphviz")
            return False
        
        # å‰µå»ºè¼¸å…¥å¼µé‡
        x = torch.randn(input_shape).to(self.device)
        
        # ç²å–è¼¸å‡º
        y = self(x)
        
        # ä½¿ç”¨ torchviz ç”Ÿæˆè¨ˆç®—åœ–
        dot = torchviz.make_dot(y, params=dict(self.named_parameters()))
        
        # è¨­ç½®åœ–çš„å±¬æ€§
        dot.attr('node', fontsize='12')
        dot.attr('graph', rankdir='TB')  # å¾ä¸Šåˆ°ä¸‹çš„ä½ˆå±€
        dot.attr('graph', size='12,12')  # åœ–çš„å¤§å°
        
        # ä¿å­˜åœ–ç‰‡
        dot.render(output_path.replace('.png', ''), format='png', cleanup=True)
        
        print(f"âœ… æ¨¡å‹æ¶æ§‹å·²ä¿å­˜è‡³: {output_path}")
        
        # è¼¸å‡ºæ¨¡å‹æ‘˜è¦
        self._print_model_summary()
        
        return True

    def get_feature_map(self, x):
        """ç²å–ç‰¹å¾µåœ–"""
        feature_maps = self.backbone(x)
        return feature_maps
    
    def forward(self, images=None, class_images=None, class_head=None, feature_maps=None, 
           max_boxes=100, nms_threshold=0.5, **kwargs):
        """
        æ”¯æ´ OS2D pipeline (class_head, feature_maps) åŠæ¨™æº– (images, class_images)
        åŠ å…¥ NMS è™•ç†ä¾†é™åˆ¶æ¡†æ•¸é‡
        """
        import torchvision.ops

        # å…ˆåŸ·è¡ŒåŸå§‹ forward
        outputs = super().forward(images, class_images, class_head, feature_maps, **kwargs)
        
        # æ‡‰ç”¨ NMS æ¸›å°‘æ¡†æ•¸é‡
        if isinstance(outputs, tuple) and len(outputs) >= 2:
            class_scores, boxes = outputs[0], outputs[1]
            
            # ç²å–ç”¨æ–¼ NMS çš„åˆ†æ•¸ (ä½¿ç”¨æ¯å€‹æ¡†çš„æœ€é«˜é¡åˆ¥åˆ†æ•¸)
            if class_scores.dim() == 2:  # [N, C] æ¨™æº–æ ¼å¼
                scores, _ = class_scores.max(dim=1)
            elif class_scores.dim() == 4:  # [B, C, 4, N] OS2D å¯†é›†æ ¼å¼
                # å°‡å¯†é›†æ ¼å¼è½‰æ›ç‚ºæ¨™æº–æ ¼å¼ï¼Œç„¶å¾Œç²å–åˆ†æ•¸
                scores_view = class_scores.view(class_scores.size(0), class_scores.size(1), -1)
                scores, _ = scores_view.max(dim=2)
                scores, _ = scores.max(dim=1)  # ç²å–æ¯å€‹æ¡†åœ¨æ‰€æœ‰é¡åˆ¥ä¸­çš„æœ€å¤§åˆ†æ•¸
            else:
                scores = torch.ones(boxes.size(0), device=boxes.device)  # é è¨­åˆ†æ•¸
                
            # æ‡‰ç”¨ NMS
            keep_indices = torchvision.ops.nms(
                boxes, 
                scores,
                iou_threshold=nms_threshold
            )[:max_boxes]  # é™åˆ¶æœ€å¤§æ¡†æ•¸
            
            # éæ¿¾è¼¸å‡º
            boxes = boxes[keep_indices]
            class_scores = class_scores[keep_indices] if class_scores.dim() <= 2 else class_scores[:, :, :, keep_indices]
            
            # é‡å»ºè¼¸å‡ºå…ƒçµ„
            if len(outputs) > 2:
                extra_outputs = []
                for extra in outputs[2:]:
                    if isinstance(extra, torch.Tensor) and extra.size(0) == outputs[0].size(0):
                        extra_outputs.append(extra[keep_indices])
                    else:
                        extra_outputs.append(extra)
                outputs = (class_scores, boxes) + tuple(extra_outputs)
            else:
                outputs = (class_scores, boxes)
        
        return outputs
    def forward(self, images=None, class_images=None, class_head=None, feature_maps=None, **kwargs):
        """
        æ”¯æ´ OS2D pipeline (class_head, feature_maps) åŠæ¨™æº– (images, class_images)
        """
        # OS2D pipeline: detection
        if class_head is not None and feature_maps is not None:
            # èª¿ç”¨çˆ¶é¡çš„ detection forward
            return super().forward(class_head=class_head, feature_maps=feature_maps, **kwargs)
        # æ¨™æº–è¨“ç·´/æ¨è«–
        if images is not None:
            if class_images is not None:
                return super().forward(images, class_images=class_images, **kwargs)
            else:
                return super().forward(images, **kwargs)
        raise ValueError("forward() éœ€è¦ (images) æˆ– (class_head, feature_maps)")

    
    def _print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦ä¿¡æ¯ï¼ŒåŒ…å«æ¯å±¤çš„é€šé“æ•¸"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        # ç²å–æ¯å±¤çš„åƒæ•¸å’Œé€šé“ä¿¡æ¯
        layer_info = {}
        for name, module in self.backbone.named_modules():
            if isinstance(module, nn.Conv2d):
                layer_info[name] = {
                    'type': 'Conv2d',
                    'params': sum(p.numel() for p in module.parameters()),
                    'in_channels': module.in_channels,
                    'out_channels': module.out_channels
                }
            elif isinstance(module, nn.BatchNorm2d):
                layer_info[name] = {
                    'type': 'BatchNorm2d',
                    'params': sum(p.numel() for p in module.parameters()),
                    'num_features': module.num_features
                }

        # æ‰“å°æ‘˜è¦
        print("\n====== æ¨¡å‹æ‘˜è¦ ======")
        print(f"ç¸½åƒæ•¸é‡: {total_params:,}")
        print(f"å¯è¨“ç·´åƒæ•¸é‡: {trainable_params:,}")
        print("\nå±¤ç´šçµæ§‹åˆ†æ:")

        # æŒ‰å±¤åæ’åºæ‰“å°
        for layer_name in sorted(layer_info.keys()):
            info = layer_info[layer_name]
            if info['type'] == 'Conv2d':
                # æª¢æŸ¥æ˜¯å¦ç‚º downsample å±¤
                is_downsample = 'downsample' in layer_name
                layer_type = 'æ®˜å·®åˆ†æ”¯' if is_downsample else 'ä¸»åˆ†æ”¯'
                print(f"\n- {layer_name} ({layer_type}):")
                print(f"  é¡å‹: {info['type']}")
                print(f"  è¼¸å…¥é€šé“: {info['in_channels']}")
                print(f"  è¼¸å‡ºé€šé“: {info['out_channels']}")
                print(f"  åƒæ•¸é‡: {info['params']:,}")
            else:  # BatchNorm2d
                print(f"\n- {layer_name}:")
                print(f"  é¡å‹: {info['type']}")
                print(f"  ç‰¹å¾µæ•¸: {info['num_features']}")
                print(f"  åƒæ•¸é‡: {info['params']:,}")

        # æ·»åŠ å±¤ç´šé—œä¿‚åˆ†æ
        print("\n====== å±¤ç´šé€£æ¥åˆ†æ ======")
        for layer_idx in range(1, 4):  # layer1 åˆ° layer4
            layer = getattr(self.backbone, f'layer{layer_idx}')
            print(f"\n[Layer {layer_idx}]")
            for block_idx in range(len(layer)):
                block = layer[block_idx]
                print(f"\nBlock {block_idx}:")
                # æ‰“å°ä¸»åˆ†æ”¯
                if hasattr(block, 'conv1'):
                    print(f"  Conv1: {block.conv1.in_channels} -> {block.conv1.out_channels}")
                if hasattr(block, 'conv2'):
                    print(f"  Conv2: {block.conv2.in_channels} -> {block.conv2.out_channels}")
                if hasattr(block, 'conv3'):
                    print(f"  Conv3: {block.conv3.in_channels} -> {block.conv3.out_channels}")
                # æ‰“å°æ®˜å·®åˆ†æ”¯
                if hasattr(block, 'downsample') and block.downsample is not None:
                    downsample_conv = block.downsample[0]
                    print(f"  Downsample: {downsample_conv.in_channels} -> {downsample_conv.out_channels}")
                    print(f"  Downsample Type: {type(downsample_conv).__name__}")
                    
    def _normalize_batch_images(self, images, device=None, target_size=(224, 224)):
        """
        æ¨™æº–åŒ–è™•ç†åœ–åƒæ‰¹æ¬¡ï¼Œç¢ºä¿æ‰€æœ‰åœ–åƒå°ºå¯¸ä¸€è‡´ä¸¦è½‰æ›ç‚ºæ‰¹æ¬¡å¼µé‡
        
        Args:
            images: åœ–åƒåˆ—è¡¨æˆ–å–®ä¸€å¼µé‡
            device: ç›®æ¨™è¨­å‚™
            target_size: ç›®æ¨™å°ºå¯¸ (H, W)
            
        Returns:
            torch.Tensor: æ‰¹æ¬¡åœ–åƒå¼µé‡ [B, C, H, W]
        """
        if device is None:
            device = self.device
            
        # è™•ç†å–®ä¸€å¼µé‡æƒ…æ³
        if isinstance(images, torch.Tensor):
            if images.dim() == 3:  # [C, H, W]
                images = images.unsqueeze(0)  # [1, C, H, W]
            return images.to(device)
            
        # è™•ç†åœ–åƒåˆ—è¡¨
        if isinstance(images, list):
            # éæ¿¾æœ‰æ•ˆåœ–åƒ
            valid_images = []
            for img in images:
                if img is None or not isinstance(img, torch.Tensor):
                    continue
                
                # ç¢ºä¿åœ–åƒæ˜¯ 3D å¼µé‡ [C, H, W]
                if img.dim() == 3:
                    # èª¿æ•´åœ–åƒå°ºå¯¸ç‚ºæ¨™æº–å°ºå¯¸ä¸¦ç§»è‡³è¨­å‚™
                    if img.shape[1] != target_size[0] or img.shape[2] != target_size[1]:
                        img = torch.nn.functional.interpolate(
                            img.unsqueeze(0),  # æ·»åŠ æ‰¹æ¬¡ç¶­åº¦
                            size=target_size,
                            mode='bilinear',
                            align_corners=False
                        ).squeeze(0)  # ç§»é™¤æ‰¹æ¬¡ç¶­åº¦
                    
                    valid_images.append(img.to(device))
                
            if len(valid_images) == 0:
                return None
                
            # å †ç–Šç‚ºæ‰¹æ¬¡å¼µé‡
            batch_tensor = torch.stack(valid_images)
            return batch_tensor
        
        return None
    
    def compute_classification_loss(self, outputs, class_ids):
        # å¾è¼¸å‡ºä¸­ç²å–åˆ†é¡åˆ†æ•¸
        if isinstance(outputs, dict) and 'class_scores' in outputs:
            class_scores = outputs['class_scores']
        else:
            class_scores = outputs
            
        # è™•ç† OS2D ç‰¹æ®Šçš„ 4D è¼¸å‡ºæ ¼å¼
        if class_scores.dim() > 2:
            # ç²å–æ‰¹æ¬¡å¤§å°å’Œé¡åˆ¥æ•¸
            batch_size = class_scores.size(0)
            num_classes = class_scores.size(1)
            
            # å°‡ [B, C, H, W] è½‰æ›ç‚º [B, C] é€šéå¹³å‡æ± åŒ–æˆ–æœ€å¤§æ± åŒ–
            class_scores = class_scores.view(batch_size, num_classes, -1)
            class_scores = class_scores.mean(dim=2)  # æˆ–ä½¿ç”¨ max(dim=2)[0]
            
            print(f"âœ“ å°‡ class_scores å¾ 4D è½‰æ›ç‚º 2D: {class_scores.shape}")
        
        # åªä½¿ç”¨æ¯å€‹åœ–åƒçš„ä¸»è¦é¡åˆ¥
        if isinstance(class_ids, list):
            main_class_ids = []
            for cls_id in class_ids:
                if isinstance(cls_id, torch.Tensor) and cls_id.numel() > 0:
                    main_class_ids.append(cls_id[0].unsqueeze(0))
            
            if main_class_ids:
                target = torch.cat(main_class_ids).long()
            else:
                target = torch.zeros(class_scores.size(0)).long()
        else:
            target = class_ids.long()
        
        # ç¢ºä¿ç›®æ¨™åœ¨æœ‰æ•ˆç¯„åœå…§
        if target.max() >= class_scores.size(1):
            target = torch.clamp(target, max=class_scores.size(1)-1)
        
        # ä½¿ç”¨äº¤å‰ç†µæå¤±
        loss_fn = torch.nn.CrossEntropyLoss()
        return loss_fn(class_scores, target)
    
    def compute_box_regression_loss(self, outputs, boxes):
        """
        è¨ˆç®—é‚Šç•Œæ¡†å›æ­¸æå¤± - å„ªåŒ–è™•ç†ä¸åŒæ•¸é‡æ¡†çš„æƒ…æ³
        
        Args:
            outputs: æ¨¡å‹è¼¸å‡º
            boxes: ç›®æ¨™é‚Šç•Œæ¡† (å¯èƒ½æ˜¯BoxListå°è±¡ã€å¼µé‡æˆ–å…¶åˆ—è¡¨)
                    
        Returns:
            torch.Tensor: å›æ­¸æå¤±
        """
        # å¾è¼¸å‡ºä¸­ç²å–é æ¸¬æ¡†
        if isinstance(outputs, dict) and 'boxes' in outputs:
            pred_boxes = outputs['boxes']
        elif isinstance(outputs, tuple):
            # å¦‚æœè¼¸å‡ºæ˜¯å…ƒçµ„ï¼Œå‡è¨­ç¬¬äºŒå€‹å…ƒç´ åŒ…å«é‚Šç•Œæ¡†
            pred_boxes = outputs[1] if len(outputs) > 1 else torch.zeros(1, 4).to(self.device)
        else:
            # å¦‚æœæ²’æœ‰æ˜ç¢ºçš„é‚Šç•Œæ¡†è¼¸å‡ºï¼Œä½¿ç”¨æ¨¡å‹çš„è¼¸å‡º
            pred_boxes = outputs
        
        # ç¢ºä¿ pred_boxes æ˜¯å¼µé‡
        if not isinstance(pred_boxes, torch.Tensor):
            print(f"è­¦å‘Š: pred_boxes ä¸æ˜¯å¼µé‡ï¼Œè€Œæ˜¯ {type(pred_boxes)}ï¼Œä½¿ç”¨é è¨­å€¼")
            pred_boxes = torch.zeros(1, 4).to(self.device)
            
        # ä½¿ç”¨ _cat_boxes_list è™•ç†ç›®æ¨™æ¡†
        if isinstance(boxes, list):
            target_boxes = self._cat_boxes_list(boxes, device=pred_boxes.device)
        else:
            # è™•ç†å–®å€‹ BoxList å°è±¡æˆ–å¼µé‡
            if hasattr(boxes, 'bbox_xyxy'):
                target_boxes = boxes.bbox_xyxy
            elif hasattr(boxes, 'bbox') and hasattr(boxes, 'size'):
                target_boxes = boxes.bbox
            else:
                target_boxes = boxes
        
        # ç¢ºä¿å¼µé‡åœ¨åŒä¸€è¨­å‚™ä¸Š
        if isinstance(target_boxes, torch.Tensor):
            target_boxes = target_boxes.to(pred_boxes.device)
        else:
            print(f"è­¦å‘Š: target_boxes ä¸æ˜¯å¼µé‡ï¼Œè€Œæ˜¯ {type(target_boxes)}ï¼Œä½¿ç”¨é è¨­å€¼")
            target_boxes = torch.zeros(1, 4).to(pred_boxes.device)
        
        # æ‰“å° debug ä¿¡æ¯
        print(f"ğŸ“Š Box å›æ­¸å…§å®¹æª¢æŸ¥:")
        print(f"  pred_boxes: å½¢ç‹€ {pred_boxes.shape}, é¡å‹ {pred_boxes.dtype}, è£ç½® {pred_boxes.device}")
        print(f"  target_boxes: å½¢ç‹€ {target_boxes.shape}, é¡å‹ {target_boxes.dtype}, è£ç½® {target_boxes.device}")
        
        # å¦‚æœæ²’æœ‰æœ‰æ•ˆçš„ç›®æ¨™æ¡†ï¼Œè¿”å›é›¶æå¤±
        if target_boxes.numel() == 0:
            print(f"âš ï¸ ç›®æ¨™æ¡†ç‚ºç©ºï¼Œè¿”å›é›¶æå¤±")
            return torch.tensor(0.0, device=pred_boxes.device)
            
        # ç¢ºä¿é æ¸¬æ¡†å’Œç›®æ¨™æ¡†çš„å½¢ç‹€åŒ¹é…
        if pred_boxes.size(-1) != 4:
            pred_boxes = pred_boxes.view(-1, 4)
        if target_boxes.size(-1) != 4:
            target_boxes = target_boxes.view(-1, 4)
        
        # æª¢æŸ¥å€¼ç¯„åœï¼Œé¿å…ç•°å¸¸å€¼å¹²æ“¾å›æ­¸
        if torch.isnan(pred_boxes).any() or torch.isinf(pred_boxes).any():
            print(f"âš ï¸ é æ¸¬æ¡†åŒ…å« NaN æˆ– Infï¼Œé€²è¡Œä¿®æ­£")
            pred_boxes = torch.nan_to_num(pred_boxes, nan=0.0, posinf=1.0, neginf=0.0)
            
        if torch.isnan(target_boxes).any() or torch.isinf(target_boxes).any():
            print(f"âš ï¸ ç›®æ¨™æ¡†åŒ…å« NaN æˆ– Infï¼Œé€²è¡Œä¿®æ­£")
            target_boxes = torch.nan_to_num(target_boxes, nan=0.0, posinf=1.0, neginf=0.0)
        
        # è™•ç† OS2D å¯†é›†æ ¼å¼ (ç‰¹åˆ¥è™•ç†)
        if pred_boxes.dim() > 2:
            # å¦‚æœæ˜¯å¯†é›†æ ¼å¼ [B, C, 4, N] æˆ– [B, C, N]
            if pred_boxes.dim() == 4:
                b, c, four, n = pred_boxes.shape
                if four == 4:
                    # èª¿æ•´ç‚ºæ¨™æº–å½¢å¼ [B*C*N, 4]
                    pred_boxes = pred_boxes.permute(0, 1, 3, 2).reshape(-1, 4)
                else:
                    # æˆ–è€…å¯èƒ½æ˜¯ [B, C, N, 4]
                    pred_boxes = pred_boxes.reshape(-1, 4)
            elif pred_boxes.dim() == 3:
                # å¯èƒ½æ˜¯ [B, N, 4] æˆ– [B, C, N]
                if pred_boxes.size(2) == 4:
                    # æ˜¯ [B, N, 4]
                    pred_boxes = pred_boxes.reshape(-1, 4)
                else:
                    # å¯èƒ½éœ€è¦å…¶ä»–è™•ç†
                    print(f"âš ï¸ æœªé æœŸçš„ pred_boxes å½¢ç‹€: {pred_boxes.shape}")
                    pred_boxes = pred_boxes.view(-1, 4)
        
        # è™•ç†æ¡†æ•¸é‡ä¸åŒ¹é…çš„æƒ…æ³
        if pred_boxes.size(0) != target_boxes.size(0):
            print(f"âš ï¸ é æ¸¬æ¡†å’Œç›®æ¨™æ¡†æ•¸é‡ä¸åŒ¹é…: {pred_boxes.size(0)} vs {target_boxes.size(0)}")
            
            # å„ªåŒ–ï¼šåªé¸æ“‡å‰Kå€‹é æ¸¬æ¡†é€²è¡Œæå¤±è¨ˆç®—
            if pred_boxes.size(0) > target_boxes.size(0) * 5:
                # å¦‚æœé æ¸¬æ¡†æ•¸é å¤§æ–¼ç›®æ¨™æ¡†ï¼Œéš¨æ©ŸæŠ½æ¨£æˆ–é¸é ‚éƒ¨
                k = min(max(100, target_boxes.size(0) * 5), pred_boxes.size(0))
                # éš¨æ©ŸæŠ½æ¨£ä»¥é¿å…è¨“ç·´åå·®
                indices = torch.randperm(pred_boxes.size(0), device=pred_boxes.device)[:k]
                pred_boxes_sampled = pred_boxes[indices]
                
                # è¨ˆç®—æ‰€æœ‰é æ¸¬æ¡†èˆ‡æ‰€æœ‰ç›®æ¨™æ¡†çš„ IoU
                try:
                    from torchvision.ops import box_iou
                    ious = box_iou(pred_boxes_sampled, target_boxes)
                    # ç‚ºæ¯å€‹é æ¸¬æ¡†é¸æ“‡æœ€ä½³åŒ¹é…çš„ç›®æ¨™æ¡†
                    best_target_idx = ious.max(dim=1)[1]
                    matched_targets = target_boxes[best_target_idx]
                    
                    # è¨ˆç®—æå¤±
                    loss = F.smooth_l1_loss(pred_boxes_sampled, matched_targets)
                    print(f"âœ“ ä½¿ç”¨ IoU åŒ¹é…æå¤± (æŠ½æ¨£ {k}/{pred_boxes.size(0)} æ¡†): {loss.item():.4f}")
                    return loss
                except Exception as e:
                    print(f"âš ï¸ IoU è¨ˆç®—å¤±æ•—ï¼Œä½¿ç”¨å‚™é¸æ–¹æ³•: {e}")
                    # ä½¿ç”¨æœ€ç°¡å–®çš„æ–¹æ³•ï¼šåªé¸æ“‡èˆ‡ç›®æ¨™æ¡†æ•¸é‡ç›¸åŒçš„é æ¸¬æ¡†
                    pred_boxes = pred_boxes[:target_boxes.size(0)]
            
            # æ¡†æ•¸é‡è¼ƒæ¥è¿‘æ™‚ï¼Œä½¿ç”¨ IoU åŒ¹é…
            try:
                # å‰µå»ºæˆæœ¬çŸ©é™£
                cost_matrix = torch.zeros(pred_boxes.size(0), target_boxes.size(0), device=pred_boxes.device)
                
                # åŸºæ–¼ L1 è·é›¢è¨ˆç®—æˆæœ¬
                for i in range(pred_boxes.size(0)):
                    cost_matrix[i] = torch.sum(torch.abs(pred_boxes[i].unsqueeze(0) - target_boxes), dim=1)
                
                # åˆ©ç”¨åŒˆç‰™åˆ©ç®—æ³•åŒ¹é… (å¦‚æœå¯ç”¨)
                try:
                    from scipy.optimize import linear_sum_assignment
                    cost_np = cost_matrix.detach().cpu().numpy()
                    pred_idx, target_idx = linear_sum_assignment(cost_np)
                    pred_idx = torch.tensor(pred_idx, device=pred_boxes.device)
                    target_idx = torch.tensor(target_idx, device=target_boxes.device)
                    
                    # è¨ˆç®—åŒ¹é…çš„æ¡†ä¹‹é–“çš„æå¤±
                    matched_pred_boxes = pred_boxes[pred_idx]
                    matched_target_boxes = target_boxes[target_idx]
                    
                    loss = F.smooth_l1_loss(matched_pred_boxes, matched_target_boxes)
                    print(f"âœ“ ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•åŒ¹é…æå¤±: {loss.item():.4f}")
                    return loss
                except (ImportError, ModuleNotFoundError):
                    print("âš ï¸ åŒˆç‰™åˆ©ç®—æ³•ä¸å¯ç”¨ï¼Œä½¿ç”¨è²ªå©ªåŒ¹é…")
                    
                    # è²ªå©ªåŒ¹é…
                    min_cost, min_idx = cost_matrix.min(dim=1)
                    matched_targets = target_boxes[min_idx]
                    
                    loss = F.smooth_l1_loss(pred_boxes, matched_targets)
                    print(f"âœ“ ä½¿ç”¨è²ªå©ªåŒ¹é…æå¤±: {loss.item():.4f}")
                    return loss
                    
            except Exception as e:
                print(f"âš ï¸ åŒ¹é…è¨ˆç®—å¤±æ•—: {e}")
                # å¦‚æœæ‰€æœ‰å˜—è©¦éƒ½å¤±æ•—ï¼Œå‰‡ä½¿ç”¨æœ€ç°¡å–®çš„è™•ç†æ–¹æ³•
                min_len = min(pred_boxes.size(0), target_boxes.size(0))
                loss = F.smooth_l1_loss(pred_boxes[:min_len], target_boxes[:min_len])
                print(f"âœ“ ä½¿ç”¨ç°¡å–®æˆªæ–·åŒ¹é…æå¤±: {loss.item():.4f}")
                return loss
        
        # æ¨™æº–æƒ…æ³ï¼šæ¡†æ•¸é‡åŒ¹é…
        try:
            loss = F.smooth_l1_loss(pred_boxes, target_boxes)
            print(f"âœ“ ä½¿ç”¨æ¨™æº– Smooth L1 æå¤±: {loss.item():.4f}")
            return loss
        except Exception as e:
            print(f"âŒ Smooth L1 æå¤±è¨ˆç®—å¤±æ•—: {e}")
            # å˜—è©¦ L1 æå¤±ä½œç‚ºå‚™é¸
            try:
                loss = F.l1_loss(pred_boxes, target_boxes)
                print(f"âœ“ ä½¿ç”¨ L1 æå¤±ä½œç‚ºå‚™é¸: {loss.item():.4f}")
                return loss
            except Exception as e2:
                print(f"âŒ L1 æå¤±ä¹Ÿå¤±æ•—: {e2}")
                return torch.tensor(1.0, device=pred_boxes.device, requires_grad=True)
    
    def _cat_boxes_list(self, boxes, device=None):
        """
        å°‡ list of BoxList æˆ– tensor è½‰ç‚º [N,4] tensorï¼Œéæ¿¾ç©º box
        """
        valid_boxes = []
        for b in boxes:
            # BoxList ç‰©ä»¶
            if hasattr(b, "bbox_xyxy"):
                t = b.bbox_xyxy
                if t.numel() > 0:
                    valid_boxes.append(t)
            # tensor
            elif isinstance(b, torch.Tensor) and b.numel() > 0:
                valid_boxes.append(b)
        if not valid_boxes:
            # è¿”å›ä¸€å€‹ç©º tensor [0,4]ï¼Œé˜²æ­¢ cat å ±éŒ¯
            if device is None and len(boxes) > 0:
                if hasattr(boxes[0], "bbox_xyxy"):
                    device = boxes[0].bbox_xyxy.device
                elif isinstance(boxes[0], torch.Tensor):
                    device = boxes[0].device
            return torch.zeros((0, 4), device=device)
        return torch.cat(valid_boxes, dim=0)
    
    def analyze_os2d_outputs(self, outputs, targets=None):
        """
        è§£æ OS2D æ¨¡å‹è¼¸å‡ºï¼Œæå–æœ‰ç”¨çš„ä¿¡æ¯
        
        Args:
            outputs: æ¨¡å‹è¼¸å‡ºï¼Œå¯èƒ½æ˜¯å…ƒçµ„æˆ–å­—å…¸
            targets: ç›®æ¨™æ•¸æ“šï¼Œå¯é¸
            
        Returns:
            dict: åŒ…å«è§£æçµæœçš„å­—å…¸
        """
        results = {}
        
        # è§£æè¼¸å‡º
        if isinstance(outputs, tuple):
            # å…¸å‹çš„ OS2D è¼¸å‡ºæ˜¯ä¸€å€‹ 5 å…ƒç´ å…ƒçµ„
            if len(outputs) >= 2:
                class_scores = outputs[0]
                boxes = outputs[1]
                
                results['output_type'] = 'tuple'
                results['num_elements'] = len(outputs)
                
                # è§£æåˆ†é¡åˆ†æ•¸
                if isinstance(class_scores, torch.Tensor):
                    # åˆ†æç¶­åº¦
                    if class_scores.dim() == 4:  # [B, C, 4, N]
                        results['class_scores_shape'] = list(class_scores.shape)
                        results['batch_size'] = class_scores.shape[0]
                        results['num_classes'] = class_scores.shape[1]
                        results['num_positions'] = class_scores.shape[3]
                        results['dense_format'] = True
                        
                        # æå–æ¯å€‹é¡åˆ¥çš„æœ€é«˜åˆ†æ•¸
                        scores_view = class_scores.view(results['batch_size'], results['num_classes'], -1)
                        max_scores, _ = scores_view.max(dim=2)
                        results['class_confidence'] = max_scores.detach().cpu().tolist()
                        
                    elif class_scores.dim() == 2:  # [N, C]
                        results['class_scores_shape'] = list(class_scores.shape)
                        results['num_detections'] = class_scores.shape[0]
                        results['num_classes'] = class_scores.shape[1]
                        results['dense_format'] = False
                        
                        # æå–é¡åˆ¥çš„æœ€é«˜åˆ†æ•¸
                        max_scores, pred_classes = class_scores.max(dim=1)
                        results['class_predictions'] = pred_classes.detach().cpu().tolist()
                        results['detection_scores'] = max_scores.detach().cpu().tolist()
                
                # è§£æé‚Šç•Œæ¡†
                if isinstance(boxes, torch.Tensor):
                    results['boxes_shape'] = list(boxes.shape)
                    
                    if boxes.dim() == 2 and boxes.shape[1] == 4:  # [N, 4] æ¨™æº–æ ¼å¼
                        results['num_boxes'] = boxes.shape[0]
                        # æå–ä¸€äº›æ¡†é€²è¡Œæª¢æŸ¥
                        if boxes.shape[0] > 0:
                            sample_boxes = boxes[:min(5, boxes.shape[0])].detach().cpu().tolist()
                            results['sample_boxes'] = sample_boxes
                    
                    elif boxes.dim() == 4:  # [B, C, 4, N] OS2D å¯†é›†æ ¼å¼
                        results['dense_boxes'] = True
                        results['batch_size'] = boxes.shape[0]
                        results['num_classes'] = boxes.shape[1]
                        results['num_positions'] = boxes.shape[3]
                    
                    elif boxes.dim() == 3:  # [B, N, 4] æ‰¹æ¬¡æ ¼å¼
                        results['dense_boxes'] = False
                        results['batch_size'] = boxes.shape[0]
                        results['num_boxes'] = boxes.shape[1]
        
        # è§£æç›®æ¨™
        if targets is not None and isinstance(targets, dict):
            results['target_info'] = {}
            
            # è§£æé¡åˆ¥ ID
            if 'class_ids' in targets:
                class_ids = targets['class_ids']
                if isinstance(class_ids, list):
                    results['target_info']['num_classes'] = len(class_ids)
                    results['target_info']['class_ids'] = class_ids
                elif isinstance(class_ids, torch.Tensor):
                    results['target_info']['num_classes'] = class_ids.shape[0]
                    results['target_info']['class_ids'] = class_ids.detach().cpu().tolist()
            
            # è§£æç›®æ¨™æ¡†
            if 'boxes' in targets:
                boxes = targets['boxes']
                if isinstance(boxes, list):
                    num_boxes = sum(1 for box in boxes if isinstance(box, torch.Tensor) and box.numel() > 0)
                    results['target_info']['num_boxes'] = num_boxes
                elif isinstance(boxes, torch.Tensor):
                    if boxes.dim() > 1:
                        results['target_info']['num_boxes'] = boxes.shape[0]
                        results['target_info']['boxes_shape'] = list(boxes.shape)
        
        return results
    
    def compute_losses(self, outputs, targets, class_num=0, auxiliary_net=None, use_lcp_loss=True, loss_weights=None):
        """
        è¨ˆç®—è¨“ç·´æå¤±çš„çµ„åˆï¼ˆåˆ†é¡ã€æ¡†å›æ­¸ã€æ•™å¸«è’¸é¤¾å’Œ LCPï¼‰
        
        Args:
            outputs: æ¨¡å‹è¼¸å‡ºï¼Œå­—å…¸æˆ–å…ƒçµ„
            targets: ç›®æ¨™æ•¸æ“š
            class_num: é¡åˆ¥æ•¸é‡
            auxiliary_net: è¼”åŠ©ç¶²çµ¡ï¼Œç”¨æ–¼ LCP æå¤±
            use_lcp_loss: æ˜¯å¦ä½¿ç”¨ LCP æå¤±
            loss_weights: å„æå¤±çš„æ¬Šé‡
            
        Returns:
            tuple: (ç¸½æå¤±, æå¤±å­—å…¸)
        """
        # é è¨­æå¤±æ¬Šé‡
        if loss_weights is None:
            loss_weights = {'cls': 1.0, 'box_reg': 1.0, 'teacher': 0.5, 'lcp': 0.1}
        
        # åˆå§‹åŒ–æå¤±
        cls_loss = torch.tensor(0.0, device=self.device)
        box_loss = torch.tensor(0.0, device=self.device)
        teacher_loss = torch.tensor(0.0, device=self.device)
        lcp_loss = torch.tensor(0.0, device=self.device)
        
        # 1. åˆ†é¡æå¤±
        try:
            cls_loss = self.compute_classification_loss(outputs, targets['class_ids'])
            print(f"   âœ“ åˆ†é¡æå¤±è¨ˆç®—å®Œæˆ: {cls_loss.item():.4f}")
        except Exception as e:
            print(f"   âŒ åˆ†é¡æå¤±è¨ˆç®—å¤±æ•—: {e}")
        
        # 2. æ¡†å›æ­¸æå¤±
        try:
            box_loss = self.compute_box_regression_loss(outputs, targets['boxes'])
            print(f"   âœ“ æ¡†å›æ­¸æå¤±è¨ˆç®—å®Œæˆ: {box_loss.item():.4f}")
        except Exception as e:
            print(f"   âŒ æ¡†å›æ­¸æå¤±è¨ˆç®—å¤±æ•—: {e}")
        
        # 3. æ•™å¸«çŸ¥è­˜è’¸é¤¾æå¤±
        try:
            if 'teacher_outputs' in targets and targets['teacher_outputs'] is not None:
                teacher_outputs = targets['teacher_outputs']
                
                # æå–æ•™å¸«å’Œå­¸ç”Ÿçš„é æ¸¬
                if isinstance(teacher_outputs, tuple) and len(teacher_outputs) >= 2:
                    teacher_scores = teacher_outputs[0]
                    teacher_boxes = teacher_outputs[1]
                    
                    if isinstance(outputs, dict):
                        student_scores = outputs.get('class_scores')
                        student_boxes = outputs.get('boxes')
                    elif isinstance(outputs, tuple) and len(outputs) >= 2:
                        student_scores = outputs[0]
                        student_boxes = outputs[1]
                    
                    # ç¢ºä¿é æ¸¬å½¢ç‹€ä¸€è‡´
                    if teacher_scores.dim() == student_scores.dim():
                        # åˆ†é¡è’¸é¤¾æå¤± (KL æ•£åº¦)
                        if student_scores.dim() <= 2:
                            cls_distill_loss = F.kl_div(
                                F.log_softmax(student_scores, dim=1),
                                F.softmax(teacher_scores, dim=1),
                                reduction='batchmean'
                            )
                        else:
                            # è™•ç†å¯†é›†æ ¼å¼ [B, C, 4, N]
                            b, c, f, n = student_scores.shape
                            student_flat = student_scores.view(b*c*n, f)
                            teacher_flat = teacher_scores.view(b*c*n, f)
                            cls_distill_loss = F.kl_div(
                                F.log_softmax(student_flat, dim=1),
                                F.softmax(teacher_flat, dim=1),
                                reduction='batchmean'
                            )
                        
                        # æ¡†å›æ­¸è’¸é¤¾æå¤± (L2 æå¤±)
                        box_distill_loss = F.mse_loss(student_boxes, teacher_boxes)
                        
                        # çµ„åˆè’¸é¤¾æå¤±
                        teacher_loss = cls_distill_loss + box_distill_loss
                        print(f"   âœ“ æ•™å¸«æå¤±è¨ˆç®—å®Œæˆ: {teacher_loss.item():.4f}")
        except Exception as e:
            print(f"   âŒ æ•™å¸«æå¤±è¨ˆç®—å¤±æ•—: {e}")
        
        # 4. LCP æå¤±
        if use_lcp_loss and auxiliary_net is not None:
            try:
                # ç²å–ç‰¹å¾µåœ–
                if isinstance(outputs, dict) and 'images' in outputs:
                    feature_maps = self.get_feature_map(outputs['images'])
                    
                    # ä½¿ç”¨è¼”åŠ©ç¶²çµ¡è¨ˆç®— LCP æå¤±
                    if isinstance(feature_maps, torch.Tensor):
                        aux_outputs = auxiliary_net(feature_maps)
                        aux_cls_loss = self.compute_classification_loss(aux_outputs, targets['class_ids'])
                        lcp_loss = aux_cls_loss
                        print(f"   âœ“ LCP æå¤±è¨ˆç®—å®Œæˆ: {lcp_loss.item():.4f}")
            except Exception as e:
                print(f"   âŒ LCP æå¤±è¨ˆç®—å¤±æ•—: {e}")
        
        # è¨ˆç®—ç¸½æå¤±
        total_loss = (
            loss_weights['cls'] * cls_loss +
            loss_weights['box_reg'] * box_loss +
            loss_weights['teacher'] * teacher_loss +
            loss_weights['lcp'] * lcp_loss
        )
        
        # æ§‹å»ºæå¤±å­—å…¸
        loss_dict = {
            'cls_loss': cls_loss,
            'box_loss': box_loss,
            'teacher_loss': teacher_loss,
            'lcp_loss': lcp_loss
        }
        
        return total_loss, loss_dict
    
    from tqdm import tqdm
    def train_one_epoch(self, train_loader, optimizer, 
                  auxiliary_net=None, device=None, 
                  print_freq=10, scheduler=None, 
                  loss_weights=None, use_lcp_loss=True, 
                  max_batches=None, max_predictions=100,
                  use_feature_pyramid=True,  # ç‰¹å¾µé‡‘å­—å¡”é–‹é—œ
                  pyramid_scales=[1.0, 0.75, 0.5],  # é‡‘å­—å¡”å°ºåº¦
                  nms_threshold=0.5,  # NMS IoU é–¾å€¼
                  apply_nms=True):  # æ˜¯å¦ä½¿ç”¨ NMS
        """
        è¨“ç·´æ¨¡å‹ä¸€å€‹ epochï¼Œæ”¯æ´ LCP æå¤±ã€ç‰¹å¾µé‡‘å­—å¡”èˆ‡ NMS æ¡†æ•¸é‡æ§åˆ¶
        """
        import torch
        import torch.nn.functional as F
        import torchvision.ops
        import time
        from tqdm import tqdm
        import traceback
        
        start_time = time.time()
        self.train()
        if auxiliary_net is not None:
            auxiliary_net.train()
            print(f"âœ“ è¼”åŠ©ç¶²è·¯è¨­ç‚ºè¨“ç·´æ¨¡å¼ï¼Œè¼¸å…¥é€šé“æ•¸: {auxiliary_net.get_current_channels()}")

        if device is None:
            device = self.device
        print(f"â„¹ï¸ ä½¿ç”¨è¨­å‚™: {device}")
        
        if loss_weights is None:
            loss_weights = {'cls': 1.0, 'box_reg': 1.0, 'teacher': 0.5, 'lcp': 0.1}
        
        # ç²å–é¡åˆ¥æ•¸é‡
        class_num = len(train_loader.dataset.get_class_ids())
        print(f"âœ“ é¡åˆ¥æ•¸é‡: {class_num}")
        
        # ç¢ºä¿æ¨¡å‹è¼¸å‡ºåŒ¹é…é¡åˆ¥æ•¸é‡
        if hasattr(self, 'classifier') and hasattr(self.classifier, 'out_features'):
            if self.classifier.out_features != class_num:
                print(f"âš ï¸ æ›´æ–°åˆ†é¡å™¨è¼¸å‡ºç¶­åº¦: {self.classifier.out_features} â†’ {class_num}")
            in_features = self.classifier.in_features
            self.classifier = nn.Linear(in_features, class_num).to(device)
        
        # æ›´æ–°è¼”åŠ©ç¶²è·¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if auxiliary_net is not None and hasattr(auxiliary_net, 'classifier') and hasattr(auxiliary_net.classifier, 'out_features'):
            if auxiliary_net.classifier.out_features != class_num:
                print(f"âš ï¸ æ›´æ–°è¼”åŠ©ç¶²è·¯åˆ†é¡å™¨ç¶­åº¦: {auxiliary_net.classifier.out_features} â†’ {class_num}")
            aux_in_features = auxiliary_net.classifier.in_features
            auxiliary_net.classifier = nn.Linear(aux_in_features, class_num).to(device)
        
        # åˆå§‹åŒ–æå¤±è¨˜éŒ„å’Œçµ±è¨ˆä¿¡æ¯
        loss_history = []
        total_loss = 0
        total_cls_loss = 0
        total_box_loss = 0
        total_teacher_loss = 0
        total_lcp_loss = 0
        batch_count = 0
        
        # ç¢ºå®šè¦è™•ç†çš„æ‰¹æ¬¡æ•¸
        num_batches = len(train_loader)
        if max_batches is not None:
            num_batches = min(num_batches, max_batches)
        
        print(f"ğŸ” ç‰¹å¾µé‡‘å­—å¡”ç‹€æ…‹: {'å•Ÿç”¨' if use_feature_pyramid else 'åœç”¨'}")
        if use_feature_pyramid:
            print(f"ğŸ“Š é‡‘å­—å¡”å°ºåº¦: {pyramid_scales}")
        print(f"ğŸ§¹ NMS ç‹€æ…‹: {'å•Ÿç”¨' if apply_nms else 'åœç”¨'}, é–¾å€¼: {nms_threshold}")
        
        # NMS æ¡†æ¡†éæ¿¾å‡½æ•¸
        def apply_nms_to_outputs(outputs, max_boxes=100, iou_threshold=0.5):
            """å°æ¨¡å‹è¼¸å‡ºæ‡‰ç”¨ NMS ä»¥æ¸›å°‘æ¡†æ•¸é‡"""
            if not isinstance(outputs, tuple) or len(outputs) < 2:
                return outputs
            
            class_scores, boxes = outputs[0], outputs[1]
            original_box_count = boxes.shape[0]
            
            # å¦‚æœæ¡†æ•¸é‡å·²ç¶“å°æ–¼é™åˆ¶ï¼Œç›´æ¥è¿”å›
            if original_box_count <= max_boxes:
                return outputs
            
            # æå–åˆ†æ•¸ç”¨æ–¼ NMS
            if class_scores.dim() == 2:  # [N, C]
                nms_scores, _ = class_scores.max(dim=1)
            elif class_scores.dim() == 4:  # [B, C, 4, N] OS2D å¯†é›†æ ¼å¼
                # å¯†é›†æ ¼å¼è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
                scores_view = class_scores.view(class_scores.size(0), class_scores.size(1), -1)
                nms_scores, _ = scores_view.max(dim=2)
                nms_scores, _ = nms_scores.max(dim=1)
            else:
                # é è¨­åˆ†æ•¸
                nms_scores = torch.ones(boxes.size(0), device=boxes.device)
            
            # æ‡‰ç”¨ NMS
            keep_indices = torchvision.ops.nms(
                boxes,
                nms_scores,
                iou_threshold=iou_threshold
            )[:max_boxes]
            
            # éæ¿¾çµæœ
            filtered_scores = class_scores[keep_indices]
            filtered_boxes = boxes[keep_indices]
            
            # é‡å»ºè¼¸å‡ºå…ƒçµ„
            filtered_extras = tuple(
                extra[keep_indices] if isinstance(extra, torch.Tensor) and extra.shape[0] == original_box_count 
                else extra for extra in outputs[2:]
            ) if len(outputs) > 2 else ()
            
            filtered_outputs = (filtered_scores, filtered_boxes) + filtered_extras
            
            print(f"âœ“ NMS: æ¡†æ•¸é‡ {original_box_count} â†’ {len(keep_indices)} (é–¾å€¼={iou_threshold}, æœ€å¤§æ¡†æ•¸={max_boxes})")
            return filtered_outputs
        
        # å‰µå»ºé€²åº¦æ¢
        with tqdm(range(num_batches), desc="è¨“ç·´é€²åº¦") as pbar:
            for batch_idx in pbar:
                try:
                    # è¨˜éŒ„æ‰¹æ¬¡é–‹å§‹æ™‚é–“
                    batch_start_time = time.time()
                    
                    # ç²å–ç•¶å‰æ‰¹æ¬¡æ•¸æ“š
                    batch_data = train_loader.get_batch(batch_idx)
                    images, class_images, loc_targets, class_targets, batch_class_ids, class_image_sizes, box_inverse_transform, batch_boxes, batch_img_size = batch_data
                    
                    print(f"âœ“ æ‰¹æ¬¡ {batch_idx+1}/{num_batches} æ•¸æ“šè¼‰å…¥å®Œæˆ")
                    print(f"   - åœ–åƒå½¢ç‹€: {images.shape}")
                    print(f"   - é¡åˆ¥æ•¸é‡: {len(batch_class_ids)}")
                    print(f"   - é‚Šç•Œæ¡†æ•¸é‡: {sum(1 for box in batch_boxes if isinstance(box, torch.Tensor) and box.numel() > 0)}")
                    
                    # å°‡æ•¸æ“šç§»è‡³æŒ‡å®šè¨­å‚™
                    images = images.to(device)
                    
                    # è™•ç†é¡åˆ¥åœ–åƒ
                    if isinstance(class_images, list):
                        class_images = [img.to(device) if isinstance(img, torch.Tensor) else img for img in class_images]
                    
                    # æ›´æ–°è¼”åŠ©ç¶²è·¯é€šé“æ•¸ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if auxiliary_net is not None:
                        feature_maps = self.get_feature_map(images)
                        if isinstance(feature_maps, torch.Tensor):
                            current_channels = feature_maps.size(1)
                            if auxiliary_net.get_current_channels() != current_channels:
                                print(f"âœ“ æ›´æ–°è¼”åŠ©ç¶²è·¯è¼¸å…¥é€šé“: {auxiliary_net.get_current_channels()} â†’ {current_channels}")
                                auxiliary_net.update_input_channels(current_channels)
                    
                    # æ¸…é›¶æ¢¯åº¦
                    optimizer.zero_grad()
                    
                    # ä½¿ç”¨ç‰¹å¾µé‡‘å­—å¡”
                    if use_feature_pyramid:
                        print(f"\nğŸ“Š ä½¿ç”¨ç‰¹å¾µé‡‘å­—å¡”é€²è¡Œæ¨ç†ï¼Œå°ºåº¦: {pyramid_scales}")
                        
                        # ----- æ•™å¸«æ¨¡å‹ç‰¹å¾µé‡‘å­—å¡”é æ¸¬ -----
                        all_teacher_scores = []
                        all_teacher_boxes = []
                        all_teacher_extras = []
                        
                        with torch.no_grad():
                            for scale_idx, scale in enumerate(pyramid_scales):
                                scale_start = time.time()
                                print(f"\nğŸ” è™•ç†æ•™å¸«æ¨¡å‹å°ºåº¦ {scale_idx+1}/{len(pyramid_scales)}: {scale}")
                                
                                # ç¸®æ”¾è¼¸å…¥åœ–åƒ
                                if scale == 1.0:
                                    scaled_images = images
                                    scaled_class_images = class_images
                                else:
                                    # ç¸®æ”¾åœ–åƒ
                                    if images.dim() == 4:  # [B, C, H, W]
                                        h, w = images.shape[2:]
                                        new_h, new_w = int(h * scale), int(w * scale)
                                        print(f"   ç¸®æ”¾åœ–åƒ: {h}x{w} â†’ {new_h}x{new_w}")
                                        scaled_images = F.interpolate(
                                            images, size=(new_h, new_w), mode='bilinear', align_corners=False
                                        )
                                    else:
                                        scaled_images = images
                                        print(f"   è·³éåœ–åƒç¸®æ”¾ï¼Œç¶­åº¦ä¸æ˜¯4D: {images.dim()}")
                                    
                                    # ç¸®æ”¾é¡åˆ¥åœ–åƒ
                                    if isinstance(class_images, list) and class_images:
                                        scaled_class_images = []
                                        for idx, img in enumerate(class_images):
                                            if isinstance(img, torch.Tensor) and img.dim() >= 3:  # [C, H, W] æˆ– [B, C, H, W]
                                                h, w = img.shape[-2:]
                                                new_h, new_w = int(h * scale), int(w * scale)
                                                print(f"   ç¸®æ”¾é¡åˆ¥åœ–åƒ {idx+1}: {h}x{w} â†’ {new_h}x{new_w}")
                                                scaled_img = F.interpolate(
                                                    img.unsqueeze(0) if img.dim() == 3 else img, 
                                                    size=(new_h, new_w), 
                                                    mode='bilinear', 
                                                    align_corners=False
                                                )
                                                scaled_img = scaled_img.squeeze(0) if img.dim() == 3 else scaled_img
                                                scaled_class_images.append(scaled_img)
                                            else:
                                                scaled_class_images.append(img)
                                                print(f"   è·³éé¡åˆ¥åœ–åƒ {idx+1} ç¸®æ”¾ï¼Œä¸æ˜¯æœ‰æ•ˆå¼µé‡")
                                    else:
                                        scaled_class_images = class_images
                                        print(f"   è·³éé¡åˆ¥åœ–åƒç¸®æ”¾ï¼Œä¸æ˜¯å¼µé‡åˆ—è¡¨")
                                
                                # æ•™å¸«æ¨¡å‹é æ¸¬
                                print(f"   é‹è¡Œæ•™å¸«æ¨¡å‹æ¨ç†ä¸­...")
                                teacher_outputs = self.teacher_model(scaled_images, class_images=scaled_class_images)
                                
                                # æ‡‰ç”¨ NMS æ¸›å°‘æ¡†æ•¸é‡
                                if apply_nms:
                                    teacher_outputs = apply_nms_to_outputs(
                                        teacher_outputs, 
                                        max_boxes=max_predictions, 
                                        iou_threshold=nms_threshold
                                    )
                                
                                if isinstance(teacher_outputs, tuple) and len(teacher_outputs) >= 2:
                                    teacher_scores, teacher_boxes = teacher_outputs[0], teacher_outputs[1]
                                    
                                    # å¦‚æœæ²’æ‡‰ç”¨NMSï¼Œé™åˆ¶é æ¸¬æ•¸é‡
                                    if not apply_nms and teacher_boxes.shape[0] > max_predictions:
                                        keep_idx = torch.randperm(teacher_boxes.shape[0])[:max_predictions]
                                        teacher_scores = teacher_scores[keep_idx]
                                        teacher_boxes = teacher_boxes[keep_idx]
                                        if len(teacher_outputs) > 2:
                                            teacher_extras = tuple(extra[keep_idx] if isinstance(extra, torch.Tensor) else extra 
                                                                for extra in teacher_outputs[2:])
                                        else:
                                            teacher_extras = teacher_outputs[2:] if len(teacher_outputs) > 2 else ()
                                    else:
                                        teacher_extras = teacher_outputs[2:] if len(teacher_outputs) > 2 else ()
                                    
                                    # æ”¶é›†æ‰€æœ‰å°ºåº¦çš„çµæœ
                                    all_teacher_scores.append(teacher_scores)
                                    all_teacher_boxes.append(teacher_boxes)
                                    if teacher_extras:
                                        all_teacher_extras.append(teacher_extras)
                                    
                                    # åˆ†ææ•™å¸«æ¨¡å‹è¼¸å‡º
                                    output_info = self.analyze_os2d_outputs((teacher_scores, teacher_boxes))
                                    if output_info.get('dense_format', False):
                                        print(f"   æ•™å¸«æ¨¡å‹å°ºåº¦ {scale}: å¯†é›†æ ¼å¼ {teacher_scores.shape}, ä½ç½®æ•¸é‡: {output_info.get('num_positions', 'N/A')}")
                                    else:
                                        print(f"   æ•™å¸«æ¨¡å‹å°ºåº¦ {scale}: æª¢æ¸¬æ¡†æ•¸é‡: {teacher_boxes.shape[0]}, åˆ†æ•¸å½¢ç‹€: {teacher_scores.shape}")
                                
                                scale_time = time.time() - scale_start
                                print(f"   å°ºåº¦ {scale} è™•ç†è€—æ™‚: {scale_time:.2f}ç§’")
                        
                        # åˆä½µæ•™å¸«æ¨¡å‹çµæœ
                        try:
                            if all_teacher_scores and all_teacher_boxes:
                                merge_start = time.time()
                                print(f"\nğŸ”„ åˆä½µæ•™å¸«æ¨¡å‹ç‰¹å¾µé‡‘å­—å¡”çµæœ...")
                                
                                # æª¢æŸ¥å½¢ç‹€ä¸€è‡´æ€§
                                all_shapes = [s.shape for s in all_teacher_scores]
                                print(f"   åˆ†æ•¸å¼µé‡å½¢ç‹€: {all_shapes}")
                                
                                # æ ¹æ“šè¼¸å‡ºå½¢ç‹€æ±ºå®šåˆä½µæ–¹å¼
                                dense_format = all_teacher_scores[0].dim() == 4
                                
                                if dense_format:
                                    # å¯†é›†æ ¼å¼ [B, C, 4, N]ï¼Œåœ¨æœ€å¾Œä¸€å€‹ç¶­åº¦ (ç‰¹å¾µä½ç½®) åˆä½µ
                                    print(f"ğŸ’¡ æª¢æ¸¬åˆ°å¯†é›†æ ¼å¼è¼¸å‡ºï¼Œåœ¨ç‰¹å¾µä½ç½®ç¶­åº¦ (dim=3) åˆä½µ")
                                    try:
                                        teacher_scores = torch.cat(all_teacher_scores, dim=3)
                                        teacher_boxes = torch.cat(all_teacher_boxes, dim=2) # é€šå¸¸ boxes æ˜¯ [B, C, N] æ ¼å¼
                                        print(f"âœ“ åˆä½µæˆåŠŸ: scores {teacher_scores.shape}, boxes {teacher_boxes.shape}")
                                        
                                        # æ­£ç¢ºåˆä½µé¡å¤–è¼¸å‡º
                                        if all_teacher_extras:
                                            teacher_extras = []
                                            for i in range(len(all_teacher_extras[0])):
                                                extras = []
                                                for scale_extras in all_teacher_extras:
                                                    if i < len(scale_extras) and scale_extras[i] is not None:
                                                        extras.append(scale_extras[i])
                                                        
                                                if extras and all(isinstance(e, torch.Tensor) for e in extras):
                                                    # ç¢ºå®šåˆä½µç¶­åº¦
                                                    if extras[0].dim() == teacher_scores.dim():
                                                        teacher_extras.append(torch.cat(extras, dim=3)) # èˆ‡ scores ç›¸åŒç¶­åº¦
                                                    elif extras[0].dim() == teacher_boxes.dim():
                                                        teacher_extras.append(torch.cat(extras, dim=2)) # èˆ‡ boxes ç›¸åŒç¶­åº¦
                                                    else:
                                                        teacher_extras.append(extras[0]) # ç„¡æ³•ç¢ºå®šï¼Œä½¿ç”¨ç¬¬ä¸€å€‹
                                                else:
                                                    teacher_extras.append(None)
                                                    
                                            teacher_outputs = (teacher_scores, teacher_boxes) + tuple(e for e in teacher_extras if e is not None)
                                        else:
                                            teacher_outputs = (teacher_scores, teacher_boxes)
                                    except RuntimeError as e:
                                        print(f"âŒ å¯†é›†æ ¼å¼åˆä½µå¤±æ•—: {e}")
                                        # ä½¿ç”¨ç¬¬ä¸€å€‹å°ºåº¦çš„çµæœä½œç‚ºå‚™é¸
                                        teacher_outputs = (all_teacher_scores[0], all_teacher_boxes[0])
                                        print(f"âš ï¸ ä½¿ç”¨å°ºåº¦ {pyramid_scales[0]} çš„çµæœä½œç‚ºå‚™é¸")
                                else:
                                    # æ¨™æº–æ ¼å¼ [N, C] å’Œ [N, 4]ï¼Œåœ¨ç¬¬0ç¶­åº¦ (æ¨£æœ¬æ•¸) åˆä½µ
                                    print(f"ğŸ’¡ æª¢æ¸¬åˆ°æ¨™æº–æ ¼å¼è¼¸å‡ºï¼Œåœ¨æ¨£æœ¬ç¶­åº¦ (dim=0) åˆä½µ")
                                    try:
                                        teacher_scores = torch.cat(all_teacher_scores, dim=0)
                                        teacher_boxes = torch.cat(all_teacher_boxes, dim=0)
                                        print(f"âœ“ åˆä½µæˆåŠŸ: scores {teacher_scores.shape}, boxes {teacher_boxes.shape}")
                                        
                                        # åˆä½µé¡å¤–è¼¸å‡º
                                        if all_teacher_extras:
                                            teacher_extras = []
                                            for i in range(len(all_teacher_extras[0])):
                                                extras = [scale_extras[i] for scale_extras in all_teacher_extras 
                                                        if i < len(scale_extras) and scale_extras[i] is not None 
                                                        and isinstance(scale_extras[i], torch.Tensor)]
                                                if extras:
                                                    try:
                                                        teacher_extras.append(torch.cat(extras, dim=0))
                                                    except RuntimeError:
                                                        teacher_extras.append(extras[0])
                                                else:
                                                    first_valid = next((x for scale_extras in all_teacher_extras 
                                                                    if i < len(scale_extras) 
                                                                    for x in [scale_extras[i]] if x is not None), None)
                                                    teacher_extras.append(first_valid)
                                                    
                                            teacher_outputs = (teacher_scores, teacher_boxes) + tuple(e for e in teacher_extras if e is not None)
                                        else:
                                            teacher_outputs = (teacher_scores, teacher_boxes)
                                    except RuntimeError as e:
                                        print(f"âŒ æ¨™æº–æ ¼å¼åˆä½µå¤±æ•—: {e}")
                                        # ä½¿ç”¨ç¬¬ä¸€å€‹å°ºåº¦çš„çµæœä½œç‚ºå‚™é¸
                                        teacher_outputs = (all_teacher_scores[0], all_teacher_boxes[0])
                                        print(f"âš ï¸ ä½¿ç”¨å°ºåº¦ {pyramid_scales[0]} çš„çµæœä½œç‚ºå‚™é¸")
                                
                                # æ‡‰ç”¨ NMS
                                if apply_nms:
                                    teacher_outputs = apply_nms_to_outputs(
                                        teacher_outputs, 
                                        max_boxes=max_predictions, 
                                        iou_threshold=nms_threshold
                                    )
                                    
                                merge_time = time.time() - merge_start
                                print(f"âœ“ æ•™å¸«æ¨¡å‹ç‰¹å¾µé‡‘å­—å¡”åˆä½µå®Œæˆï¼Œè€—æ™‚: {merge_time:.2f}ç§’")
                                
                            else:
                                print("âš ï¸ æ•™å¸«æ¨¡å‹æœªç”¢ç”Ÿæœ‰æ•ˆè¼¸å‡ºï¼Œä½¿ç”¨åŸå§‹æ¨¡å¼")
                                with torch.no_grad():
                                    teacher_outputs = self.teacher_model(images, class_images=class_images)
                                    if apply_nms:
                                        teacher_outputs = apply_nms_to_outputs(
                                            teacher_outputs, 
                                            max_boxes=max_predictions, 
                                            iou_threshold=nms_threshold
                                        )
                        except Exception as e:
                            print(f"âŒ åˆä½µæ•™å¸«æ¨¡å‹çµæœå¤±æ•—: {e}")
                            print(traceback.format_exc())
                            # ä½¿ç”¨åŸå§‹æ¨¡å¼ä½œç‚ºå‚™é¸æ–¹æ¡ˆ
                            with torch.no_grad():
                                teacher_outputs = self.teacher_model(images, class_images=class_images)
                                if apply_nms:
                                    teacher_outputs = apply_nms_to_outputs(
                                        teacher_outputs, 
                                        max_boxes=max_predictions, 
                                        iou_threshold=nms_threshold
                                    )
                        # ----- å­¸ç”Ÿæ¨¡å‹ç‰¹å¾µé‡‘å­—å¡”é æ¸¬ -----
                        all_student_scores = []
                        all_student_boxes = []
                        all_student_extras = []
                        
                        for scale_idx, scale in enumerate(pyramid_scales):
                            scale_start = time.time()
                            print(f"\nğŸ” è™•ç†å­¸ç”Ÿæ¨¡å‹å°ºåº¦ {scale_idx+1}/{len(pyramid_scales)}: {scale}")
                            
                            # ç¸®æ”¾è¼¸å…¥åœ–åƒ
                            if scale == 1.0:
                                scaled_images = images
                                scaled_class_images = class_images
                            else:
                                # ç¸®æ”¾åœ–åƒ
                                if images.dim() == 4:  # [B, C, H, W]
                                    h, w = images.shape[2:]
                                    new_h, new_w = int(h * scale), int(w * scale)
                                    print(f"   ç¸®æ”¾åœ–åƒ: {h}x{w} â†’ {new_h}x{new_w}")
                                    scaled_images = F.interpolate(
                                        images, size=(new_h, new_w), mode='bilinear', align_corners=False
                                    )
                                else:
                                    scaled_images = images
                                    print(f"   è·³éåœ–åƒç¸®æ”¾ï¼Œç¶­åº¦ä¸æ˜¯4D: {images.dim()}")
                                
                                # ç¸®æ”¾é¡åˆ¥åœ–åƒ
                                if isinstance(class_images, list) and class_images:
                                    scaled_class_images = []
                                    for idx, img in enumerate(class_images):
                                        if isinstance(img, torch.Tensor) and img.dim() >= 3:  # [C, H, W] æˆ– [B, C, H, W]
                                            h, w = img.shape[-2:]
                                            new_h, new_w = int(h * scale), int(w * scale)
                                            print(f"   ç¸®æ”¾é¡åˆ¥åœ–åƒ {idx+1}: {h}x{w} â†’ {new_h}x{new_w}")
                                            scaled_img = F.interpolate(
                                                img.unsqueeze(0) if img.dim() == 3 else img, 
                                                size=(new_h, new_w), 
                                                mode='bilinear', 
                                                align_corners=False
                                            )
                                            scaled_img = scaled_img.squeeze(0) if img.dim() == 3 else scaled_img
                                            scaled_class_images.append(scaled_img)
                                        else:
                                            scaled_class_images.append(img)
                                            print(f"   è·³éé¡åˆ¥åœ–åƒ {idx+1} ç¸®æ”¾ï¼Œä¸æ˜¯æœ‰æ•ˆå¼µé‡")
                                else:
                                    scaled_class_images = class_images
                                    print(f"   è·³éé¡åˆ¥åœ–åƒç¸®æ”¾ï¼Œä¸æ˜¯å¼µé‡åˆ—è¡¨")
                            
                            # å­¸ç”Ÿæ¨¡å‹é æ¸¬
                            print(f"   é‹è¡Œå­¸ç”Ÿæ¨¡å‹æ¨ç†ä¸­...")
                            outputs = self(scaled_images, class_images=scaled_class_images)
                            
                            # æ‡‰ç”¨ NMS æ¸›å°‘æ¡†æ•¸é‡
                            if apply_nms:
                                outputs = apply_nms_to_outputs(
                                    outputs, 
                                    max_boxes=max_predictions, 
                                    iou_threshold=nms_threshold
                                )
                            
                            if isinstance(outputs, tuple) and len(outputs) >= 2:
                                student_scores, student_boxes = outputs[0], outputs[1]
                                
                                # å¦‚æœæ²’æ‡‰ç”¨ NMSï¼Œé™åˆ¶é æ¸¬æ•¸é‡
                                if not apply_nms and student_boxes.shape[0] > max_predictions:
                                    keep_idx = torch.randperm(student_boxes.shape[0])[:max_predictions]
                                    student_scores = student_scores[keep_idx]
                                    student_boxes = student_boxes[keep_idx]
                                    if len(outputs) > 2:
                                        student_extras = tuple(extra[keep_idx] if isinstance(extra, torch.Tensor) else extra 
                                                            for extra in outputs[2:])
                                    else:
                                        student_extras = outputs[2:] if len(outputs) > 2 else ()
                                else:
                                    student_extras = outputs[2:] if len(outputs) > 2 else ()
                                
                                # æ”¶é›†æ‰€æœ‰å°ºåº¦çš„çµæœ
                                all_student_scores.append(student_scores)
                                all_student_boxes.append(student_boxes)
                                if student_extras:
                                    all_student_extras.append(student_extras)
                                
                                # åˆ†æå­¸ç”Ÿæ¨¡å‹è¼¸å‡º
                                output_info = self.analyze_os2d_outputs((student_scores, student_boxes))
                                if output_info.get('dense_format', False):
                                    print(f"   å­¸ç”Ÿæ¨¡å‹å°ºåº¦ {scale}: å¯†é›†æ ¼å¼ {student_scores.shape}, ä½ç½®æ•¸é‡: {output_info.get('num_positions', 'N/A')}")
                                else:
                                    print(f"   å­¸ç”Ÿæ¨¡å‹å°ºåº¦ {scale}: æª¢æ¸¬æ¡†æ•¸é‡: {student_boxes.shape[0]}, åˆ†æ•¸å½¢ç‹€: {student_scores.shape}")
                            
                            scale_time = time.time() - scale_start
                            print(f"   å°ºåº¦ {scale} è™•ç†è€—æ™‚: {scale_time:.2f}ç§’")
                        
                        # åˆä½µå­¸ç”Ÿæ¨¡å‹çµæœ
                        try:
                            if all_student_scores and all_student_boxes:
                                merge_start = time.time()
                                print(f"\nğŸ”„ åˆä½µå­¸ç”Ÿæ¨¡å‹ç‰¹å¾µé‡‘å­—å¡”çµæœ...")
                                
                                # æª¢æŸ¥å½¢ç‹€ä¸€è‡´æ€§
                                all_shapes = [s.shape for s in all_student_scores]
                                print(f"   åˆ†æ•¸å¼µé‡å½¢ç‹€: {all_shapes}")
                                
                                # æ ¹æ“šè¼¸å‡ºå½¢ç‹€æ±ºå®šåˆä½µæ–¹å¼
                                dense_format = all_student_scores[0].dim() == 4
                                
                                if dense_format:
                                    # å¯†é›†æ ¼å¼ [B, C, 4, N]ï¼Œåœ¨æœ€å¾Œä¸€å€‹ç¶­åº¦ (ç‰¹å¾µä½ç½®) åˆä½µ
                                    print(f"ğŸ’¡ æª¢æ¸¬åˆ°å¯†é›†æ ¼å¼è¼¸å‡ºï¼Œåœ¨ç‰¹å¾µä½ç½®ç¶­åº¦ (dim=3) åˆä½µ")
                                    try:
                                        student_scores = torch.cat(all_student_scores, dim=3)
                                        student_boxes = torch.cat(all_student_boxes, dim=2) # é€šå¸¸ boxes æ˜¯ [B, C, N] æ ¼å¼
                                        print(f"âœ“ åˆä½µæˆåŠŸ: scores {student_scores.shape}, boxes {student_boxes.shape}")
                                        
                                        # æ­£ç¢ºåˆä½µé¡å¤–è¼¸å‡º
                                        if all_student_extras:
                                            student_extras = []
                                            for i in range(len(all_student_extras[0])):
                                                extras = []
                                                for scale_extras in all_student_extras:
                                                    if i < len(scale_extras) and scale_extras[i] is not None:
                                                        extras.append(scale_extras[i])
                                                        
                                                if extras and all(isinstance(e, torch.Tensor) for e in extras):
                                                    # ç¢ºå®šåˆä½µç¶­åº¦
                                                    if extras[0].dim() == student_scores.dim():
                                                        student_extras.append(torch.cat(extras, dim=3)) # èˆ‡ scores ç›¸åŒç¶­åº¦
                                                    elif extras[0].dim() == student_boxes.dim():
                                                        student_extras.append(torch.cat(extras, dim=2)) # èˆ‡ boxes ç›¸åŒç¶­åº¦
                                                    else:
                                                        student_extras.append(extras[0]) # ç„¡æ³•ç¢ºå®šï¼Œä½¿ç”¨ç¬¬ä¸€å€‹
                                                else:
                                                    student_extras.append(None)
                                                    
                                            outputs = (student_scores, student_boxes) + tuple(e for e in student_extras if e is not None)
                                        else:
                                            outputs = (student_scores, student_boxes)
                                    except RuntimeError as e:
                                        print(f"âŒ å¯†é›†æ ¼å¼åˆä½µå¤±æ•—: {e}")
                                        # ä½¿ç”¨ç¬¬ä¸€å€‹å°ºåº¦çš„çµæœä½œç‚ºå‚™é¸
                                        outputs = (all_student_scores[0], all_student_boxes[0])
                                        print(f"âš ï¸ ä½¿ç”¨å°ºåº¦ {pyramid_scales[0]} çš„çµæœä½œç‚ºå‚™é¸")
                                else:
                                    # æ¨™æº–æ ¼å¼ [N, C] å’Œ [N, 4]ï¼Œåœ¨ç¬¬0ç¶­åº¦ (æ¨£æœ¬æ•¸) åˆä½µ
                                    print(f"ğŸ’¡ æª¢æ¸¬åˆ°æ¨™æº–æ ¼å¼è¼¸å‡ºï¼Œåœ¨æ¨£æœ¬ç¶­åº¦ (dim=0) åˆä½µ")
                                    try:
                                        student_scores = torch.cat(all_student_scores, dim=0)
                                        student_boxes = torch.cat(all_student_boxes, dim=0)
                                        print(f"âœ“ åˆä½µæˆåŠŸ: scores {student_scores.shape}, boxes {student_boxes.shape}")
                                        
                                        # åˆä½µé¡å¤–è¼¸å‡º
                                        if all_student_extras:
                                            student_extras = []
                                            for i in range(len(all_student_extras[0])):
                                                extras = [scale_extras[i] for scale_extras in all_student_extras 
                                                        if i < len(scale_extras) and scale_extras[i] is not None 
                                                        and isinstance(scale_extras[i], torch.Tensor)]
                                                if extras:
                                                    try:
                                                        student_extras.append(torch.cat(extras, dim=0))
                                                    except RuntimeError:
                                                        student_extras.append(extras[0])
                                                else:
                                                    first_valid = next((x for scale_extras in all_student_extras 
                                                                    if i < len(scale_extras) 
                                                                    for x in [scale_extras[i]] if x is not None), None)
                                                    student_extras.append(first_valid)
                                                    
                                            outputs = (student_scores, student_boxes) + tuple(e for e in student_extras if e is not None)
                                        else:
                                            outputs = (student_scores, student_boxes)
                                    except RuntimeError as e:
                                        print(f"âŒ æ¨™æº–æ ¼å¼åˆä½µå¤±æ•—: {e}")
                                        # ä½¿ç”¨ç¬¬ä¸€å€‹å°ºåº¦çš„çµæœä½œç‚ºå‚™é¸
                                        outputs = (all_student_scores[0], all_student_boxes[0])
                                        print(f"âš ï¸ ä½¿ç”¨å°ºåº¦ {pyramid_scales[0]} çš„çµæœä½œç‚ºå‚™é¸")
                                
                                # æ‡‰ç”¨ NMS
                                if apply_nms:
                                    outputs = apply_nms_to_outputs(
                                        outputs, 
                                        max_boxes=max_predictions, 
                                        iou_threshold=nms_threshold
                                    )
                                    
                                merge_time = time.time() - merge_start
                                print(f"âœ“ å­¸ç”Ÿæ¨¡å‹ç‰¹å¾µé‡‘å­—å¡”åˆä½µå®Œæˆï¼Œè€—æ™‚: {merge_time:.2f}ç§’")
                                
                            else:
                                print("âš ï¸ å­¸ç”Ÿæ¨¡å‹æœªç”¢ç”Ÿæœ‰æ•ˆè¼¸å‡ºï¼Œä½¿ç”¨åŸå§‹æ¨¡å¼")
                                outputs = self(images, class_images=class_images)
                                if apply_nms:
                                    outputs = apply_nms_to_outputs(
                                        outputs, 
                                        max_boxes=max_predictions, 
                                        iou_threshold=nms_threshold
                                    )
                        except Exception as e:
                            print(f"âŒ åˆä½µå­¸ç”Ÿæ¨¡å‹çµæœå¤±æ•—: {e}")
                            print(traceback.format_exc())
                            # ä½¿ç”¨åŸå§‹æ¨¡å¼ä½œç‚ºå‚™é¸æ–¹æ¡ˆ
                            outputs = self(images, class_images=class_images)
                            if apply_nms:
                                outputs = apply_nms_to_outputs(
                                    outputs, 
                                    max_boxes=max_predictions, 
                                    iou_threshold=nms_threshold
                                )
                    
                    else:
                        # ä¸ä½¿ç”¨ç‰¹å¾µé‡‘å­—å¡”çš„æ¨™æº–è™•ç†æ–¹å¼
                        print(f"\nâš¡ ä½¿ç”¨æ¨™æº–æ¨¡å¼ (ç„¡ç‰¹å¾µé‡‘å­—å¡”)")
                        
                        # æ•™å¸«æ¨¡å‹é æ¸¬ï¼ˆçŸ¥è­˜è’¸é¤¾ï¼‰
                        with torch.no_grad():
                            teacher_outputs = self.teacher_model(images, class_images=class_images)
                            if apply_nms:
                                teacher_outputs = apply_nms_to_outputs(
                                    teacher_outputs, 
                                    max_boxes=max_predictions, 
                                    iou_threshold=nms_threshold
                                )
                        
                        # å­¸ç”Ÿæ¨¡å‹é æ¸¬
                        outputs = self(images, class_images=class_images)
                        if apply_nms:
                            outputs = apply_nms_to_outputs(
                                outputs, 
                                max_boxes=max_predictions, 
                                iou_threshold=nms_threshold
                            )
                    
                    # åˆ†æå’Œé¡¯ç¤ºè¼¸å‡ºä¿¡æ¯
                    print("\nğŸ“Š æ¨¡å‹è¼¸å‡ºåˆ†æ:")
                    
                    if isinstance(outputs, tuple) and len(outputs) >= 2:
                        student_boxes = outputs[1]
                        student_scores = outputs[0]
                        
                        # è¼¸å‡ºå¼µé‡å½¢ç‹€ä¿¡æ¯
                        student_shape_info = f"å¼µé‡å½¢ç‹€: scores {student_scores.shape}, boxes {student_boxes.shape}"
                        
                        # åˆ†æè¼¸å‡ºç¶­åº¦ä¿¡æ¯
                        if student_scores.dim() == 2:
                            print(f"âœ“ å­¸ç”Ÿæ¨¡å‹è¼¸å‡º: {len(outputs)} å…ƒç´ ï¼Œæ¡†æ•¸é‡: {student_boxes.shape[0]}")
                            print(f"   - æ¯æ¡†é¡åˆ¥æ•¸: {student_scores.shape[1]}")
                            print(f"   - {student_shape_info}")
                        elif student_scores.dim() == 4:
                            print(f"âœ“ å­¸ç”Ÿæ¨¡å‹è¼¸å‡º: {len(outputs)} å…ƒç´ ï¼Œæ‰¹æ¬¡å¤§å°: {student_scores.shape[0]}")
                            print(f"   - é¡åˆ¥æ•¸: {student_scores.shape[1]}")
                            print(f"   - ç‰¹å¾µä½ç½®æ•¸: {student_scores.shape[3]}")
                            print(f"   - {student_shape_info}")
                        else:
                            print(f"âœ“ å­¸ç”Ÿæ¨¡å‹è¼¸å‡º: {len(outputs)} å…ƒç´ ï¼Œå½¢ç‹€: {student_shape_info}")

                    if isinstance(teacher_outputs, tuple) and len(teacher_outputs) >= 2:
                        teacher_boxes = teacher_outputs[1]
                        teacher_scores = teacher_outputs[0]
                        
                        # è¼¸å‡ºå¼µé‡å½¢ç‹€ä¿¡æ¯
                        teacher_shape_info = f"å¼µé‡å½¢ç‹€: scores {teacher_scores.shape}, boxes {teacher_boxes.shape}"
                        
                        # åˆ†æè¼¸å‡ºç¶­åº¦ä¿¡æ¯
                        if teacher_scores.dim() == 2:
                            print(f"âœ“ æ•™å¸«æ¨¡å‹è¼¸å‡º: {len(teacher_outputs)} å…ƒç´ ï¼Œæ¡†æ•¸é‡: {teacher_boxes.shape[0]}")
                            print(f"   - æ¯æ¡†é¡åˆ¥æ•¸: {teacher_scores.shape[1]}")
                            print(f"   - {teacher_shape_info}")
                        elif teacher_scores.dim() == 4:
                            print(f"âœ“ æ•™å¸«æ¨¡å‹è¼¸å‡º: {len(teacher_outputs)} å…ƒç´ ï¼Œæ‰¹æ¬¡å¤§å°: {teacher_scores.shape[0]}")
                            print(f"   - é¡åˆ¥æ•¸: {teacher_scores.shape[1]}")
                            print(f"   - ç‰¹å¾µä½ç½®æ•¸: {teacher_scores.shape[3]}")
                            print(f"   - {teacher_shape_info}")
                        else:
                            print(f"âœ“ æ•™å¸«æ¨¡å‹è¼¸å‡º: {len(teacher_outputs)} å…ƒç´ ï¼Œå½¢ç‹€: {teacher_shape_info}")
                    
                    # æ§‹å»º targets å­—å…¸
                    targets = {
                        'class_ids': batch_class_ids,
                        'boxes': batch_boxes,
                        'images': images,
                        'class_targets': class_targets,
                        'loc_targets': loc_targets,
                        'teacher_outputs': teacher_outputs
                    }
                    
                    # è™•ç† outputs çµæ§‹
                    if not isinstance(outputs, dict):
                        if isinstance(outputs, tuple) and len(outputs) >= 2:
                            outputs_dict = {
                                'class_scores': outputs[0],
                                'boxes': outputs[1],
                                'images': images,
                                'class_images': class_images,
                                'feature_pyramid': use_feature_pyramid,  # æ·»åŠ æ¨™è¨˜ä»¥ä¾¿æå¤±è¨ˆç®—æ™‚çŸ¥é“ä½¿ç”¨äº†ç‰¹å¾µé‡‘å­—å¡”
                                'pyramid_scales': pyramid_scales  # æ·»åŠ ä½¿ç”¨çš„å°ºåº¦ä¿¡æ¯
                            }
                        else:
                            outputs_dict = {
                                'class_scores': outputs,
                                'images': images,
                                'class_images': class_images
                            }
                    else:
                        outputs_dict = outputs
                        outputs_dict['feature_pyramid'] = use_feature_pyramid
                        outputs_dict['pyramid_scales'] = pyramid_scales
                    
                    # è¨ˆç®—æå¤±
                    print(f"\nğŸ“Š é–‹å§‹è¨ˆç®—æå¤±...")
                    loss_start = time.time()
                    
                    loss, loss_dict = self.compute_losses(
                        outputs_dict, 
                        targets,
                        class_num=class_num,
                        auxiliary_net=auxiliary_net,
                        use_lcp_loss=use_lcp_loss,
                        loss_weights=loss_weights
                    )
                    
                    loss_time = time.time() - loss_start
                    print(f"âœ“ æå¤±è¨ˆç®—å®Œæˆï¼Œè€—æ™‚: {loss_time:.2f}ç§’")
                    print(f"   - åˆ†é¡æå¤±: {loss_dict['cls_loss'].item():.4f}")
                    print(f"   - æ¡†å›æ­¸æå¤±: {loss_dict['box_loss'].item():.4f}")
                    print(f"   - æ•™å¸«æå¤±: {loss_dict['teacher_loss'].item():.4f}")
                    print(f"   - LCPæå¤±: {loss_dict['lcp_loss'].item():.4f}")
                    
                    # åå‘å‚³æ’­
                    backprop_start = time.time()
                    print(f"\nğŸ”„ é–‹å§‹åå‘å‚³æ’­...")
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=10.0)
                    if auxiliary_net is not None:
                        torch.nn.utils.clip_grad_norm_(auxiliary_net.parameters(), max_norm=10.0)
                    
                    # å„ªåŒ–å™¨æ­¥é€²
                    optimizer.step()
                    if scheduler is not None:
                        scheduler.step()
                    
                    backprop_time = time.time() - backprop_start
                    print(f"âœ“ åå‘å‚³æ’­å®Œæˆï¼Œè€—æ™‚: {backprop_time:.2f}ç§’")
                    
                    # è¨˜éŒ„æå¤±
                    loss_value = loss.item()
                    loss_history.append(loss_value)
                    
                    # æ›´æ–°ç¸½æå¤±çµ±è¨ˆ
                    total_loss += loss_value
                    total_cls_loss += loss_dict['cls_loss'].item()
                    total_box_loss += loss_dict['box_loss'].item()
                    total_teacher_loss += loss_dict['teacher_loss'].item()
                    total_lcp_loss += loss_dict['lcp_loss'].item()
                    batch_count += 1
                    
                    # æ›´æ–°é€²åº¦æ¢æè¿°
                    pbar.set_description(
                        f"Loss: {loss_value:.4f} (cls: {loss_dict['cls_loss'].item():.4f}, "
                        f"box: {loss_dict['box_loss'].item():.4f})"
                    )
                    
                    # æ‰“å°è©³ç´°ä¿¡æ¯
                    if print_freq > 0 and (batch_idx % print_freq == 0 or batch_idx == num_batches - 1):
                        print(f"\næ‰¹æ¬¡ {batch_idx+1}/{num_batches} æ‘˜è¦:")
                        print(f"  åˆ†é¡æå¤±: {loss_dict['cls_loss'].item():.4f}")
                        print(f"  æ¡†å›æ­¸æå¤±: {loss_dict['box_loss'].item():.4f}")
                        print(f"  æ•™å¸«æå¤±: {loss_dict['teacher_loss'].item():.4f}")
                        print(f"  LCPæå¤±: {loss_dict['lcp_loss'].item():.4f}")
                        print(f"  ç¸½æå¤±: {loss_value:.4f}")
                    
                    # è¨ˆç®—æ‰¹æ¬¡ç¸½è€—æ™‚
                    batch_time = time.time() - batch_start_time
                    print(f"\nâœ“ æ‰¹æ¬¡ {batch_idx+1}/{num_batches} å®Œæˆï¼Œç¸½è€—æ™‚: {batch_time:.2f}ç§’\n")
                    print("-" * 80)
                
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡ {batch_idx+1} è™•ç†å‡ºéŒ¯: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
        
        # è¨ˆç®—å¹³å‡æå¤±
        avg_loss = total_loss / batch_count if batch_count > 0 else 0
        avg_cls_loss = total_cls_loss / batch_count if batch_count > 0 else 0
        avg_box_loss = total_box_loss / batch_count if batch_count > 0 else 0
        avg_teacher_loss = total_teacher_loss / batch_count if batch_count > 0 else 0
        avg_lcp_loss = total_lcp_loss / batch_count if batch_count > 0 else 0
        
        # è¼¸å‡ºè¨“ç·´çµæœæ‘˜è¦
        import datetime
        elapsed_time = time.time() - start_time
        print("\n" + "=" * 50)
        print(f"âœ… è¨“ç·´å®Œæˆ! è™•ç†äº† {batch_count}/{num_batches} æ‰¹æ¬¡")
        print(f"âœ… ç¸½è€—æ™‚: {elapsed_time:.2f}ç§’ ({datetime.timedelta(seconds=int(elapsed_time))})")
        print(f"âœ… ç‰¹å¾µé‡‘å­—å¡”: {'å•Ÿç”¨' if use_feature_pyramid else 'åœç”¨'}")
        if use_feature_pyramid:
            print(f"âœ… ç‰¹å¾µé‡‘å­—å¡”å°ºåº¦: {pyramid_scales}")
        print(f"âœ… NMS: {'å•Ÿç”¨' if apply_nms else 'åœç”¨'}, é–¾å€¼: {nms_threshold}")
        print(f"âœ… å¹³å‡æå¤±: {avg_loss:.4f}")
        print(f"   - å¹³å‡åˆ†é¡æå¤±: {avg_cls_loss:.4f}")
        print(f"   - å¹³å‡æ¡†å›æ­¸æå¤±: {avg_box_loss:.4f}")
        print(f"   - å¹³å‡æ•™å¸«æå¤±: {avg_teacher_loss:.4f}")
        print(f"   - å¹³å‡LCPæå¤±: {avg_lcp_loss:.4f}")
        print("=" * 50)
        
        return loss_history
    def finetune(self, train_loader, auxiliary_net, prune_layers, prune_ratio=0.3,
                optimizer=None, device=None, epochs_per_layer=1, print_freq=1, max_batches=3):
        pass

    
    def load_checkpoint(self, checkpoint_path, device=None):
        pass

    
    def save_checkpoint(self, checkpoint_path):
        pass

    def _eval(self, dataloader, iou_thresh=0.5, batch_size=4, cfg=None, criterion=None, print_per_class_results=False):
        pass