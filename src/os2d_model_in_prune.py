import torch
import torch.nn as nn
import logging
from os2d.modeling.model import Os2dModel
from os2d.modeling.feature_extractor import build_feature_extractor
from src.lcp_channel_selector import OS2DChannelSelector
from src.gIoU_loss import GIoULoss

class Os2dModelInPrune(Os2dModel):
    """
    æ“´å±• OS2D æ¨¡å‹ä»¥æ”¯æŒé€šé“å‰ªæåŠŸèƒ½
    """
    
    def __init__(self, logger=None, is_cuda=False, backbone_arch="resnet50", 
                 use_group_norm=False, img_normalization=None, 
                 pretrained_path=None, **kwargs):
        # å¦‚æœæ²’æœ‰æä¾› loggerï¼Œå‰µå»ºä¸€å€‹
        if logger is None:
            logger = logging.getLogger("OS2D")
        
        # èª¿ç”¨çˆ¶é¡åˆå§‹åŒ–
        self.device = torch.device('cuda' if is_cuda else 'cpu')
        
        super(Os2dModelInPrune, self).__init__(
            logger=logger,
            is_cuda=is_cuda,
            backbone_arch=backbone_arch,
            use_group_norm=use_group_norm,
            img_normalization=img_normalization,
            **kwargs
        )
        
        self.backbone = self.net_feature_maps
        # å°‡æ¨¡å‹ç§»è‡³æŒ‡å®šè¨­å‚™
        if is_cuda:
            self.cuda()
            self.backbone = self.backbone.cuda()
            # ç¢ºä¿æ‰€æœ‰å­æ¨¡å¡Šéƒ½åœ¨ CUDA ä¸Š
            for module in self.modules():
                if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
                    module.cuda()
        # è¼‰å…¥é è¨“ç·´æ¬Šé‡
        if pretrained_path:
            self.init_model_from_file(pretrained_path)
        self.device = torch.device('cuda' if is_cuda else 'cpu')
        self.original_device = self.device  # ä¿å­˜åŸå§‹è¨­å‚™
        
        if is_cuda:
            try:
                self.cuda()
                self.backbone = self.backbone.cuda()
                # ç¢ºä¿æ‰€æœ‰å­æ¨¡å¡Šéƒ½åœ¨ CUDA ä¸Š
                for module in self.modules():
                    if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
                        try:
                            module.cuda()
                        except RuntimeError as e:
                            print(f"âš ï¸ æ¨¡å¡Š {type(module)} ç„¡æ³•ç§»è‡³ CUDA: {e}")
                            # å¦‚æœå¤±æ•—ï¼Œå…ˆç”¨ CPU
                            module.cpu()
            except RuntimeError as e:
                print(f"âš ï¸ CUDA åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨ CPU ä½œç‚ºå‚™é¸: {e}")
                self.device = torch.device('cpu')
                self.cpu()
                
    def _safe_forward(self, x, device=None, class_images=None):
        """å®‰å…¨çš„å‰å‘å‚³æ’­ï¼Œå¦‚æœ GPU åŸ·è¡Œå¤±æ•—æœƒfallbackåˆ°CPU"""
        try:
            if class_images is not None:
                return self(x, class_images=class_images)
            return self(x)
        except RuntimeError as e:
            if "Input type" in str(e) and device and device.type == 'cuda':
                print("âš ï¸ GPU åŸ·è¡Œå¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ CPU...")
                # æš«æ™‚å°‡æ¨¡å‹å’Œè¼¸å…¥ç§»åˆ° CPU
                original_device = next(self.parameters()).device
                cpu_model = self.cpu()
                cpu_x = x.cpu()
                cpu_class_images = class_images.cpu() if class_images is not None else None
                
                try:
                    # åœ¨ CPU ä¸ŠåŸ·è¡Œ
                    with torch.no_grad():
                        if cpu_class_images is not None:
                            output = cpu_model(cpu_x, class_images=cpu_class_images)
                        else:
                            output = cpu_model(cpu_x)
                    
                    # åŸ·è¡ŒæˆåŠŸå¾Œç§»å› GPU
                    self.to(original_device)
                    output = output.to(original_device)
                    print("âœ“ CPU åŸ·è¡ŒæˆåŠŸï¼Œå·²ç§»å› GPU")
                    return output
                
                except Exception as cpu_e:
                    print(f"âŒ CPU åŸ·è¡Œä¹Ÿå¤±æ•—: {cpu_e}")
                    # ç¢ºä¿æ¨¡å‹ç§»å›åŸå§‹è¨­å‚™
                    self.to(original_device)
                    raise cpu_e
            else:
                raise e  # å¦‚æœä¸æ˜¯è¨­å‚™å•é¡Œï¼Œå‰‡é‡æ–°å¼•ç™¼éŒ¯èª¤
    def set_layer_out_channels(self, layer_name, new_out_channels):
        """è¨­ç½®æŒ‡å®šå±¤çš„è¼¸å‡ºé€šé“æ•¸"""
        # å°‹æ‰¾ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name:
                target_layer = module
                break
                
        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False
            
        if not isinstance(target_layer, nn.Conv2d):
            print(f"âŒ å±¤ {layer_name} ä¸æ˜¯å·ç©å±¤")
            return False
            
        # å‰µå»ºæ–°çš„å·ç©å±¤
        new_conv = nn.Conv2d(
            in_channels=target_layer.in_channels,
            out_channels=new_out_channels,
            kernel_size=target_layer.kernel_size,
            stride=target_layer.stride,
            padding=target_layer.padding,
            dilation=target_layer.dilation,
            groups=target_layer.groups,
            bias=target_layer.bias is not None
        ).to(target_layer.weight.device)
        
        # å°‡åŸå§‹æ¬Šé‡è¤‡è£½åˆ°æ–°å±¤
        with torch.no_grad():
            # åªè¤‡è£½éœ€è¦çš„é€šé“
            min_channels = min(new_out_channels, target_layer.weight.size(0))
            new_conv.weight.data[:min_channels] = target_layer.weight.data[:min_channels]
            if target_layer.bias is not None:
                new_conv.bias.data[:min_channels] = target_layer.bias.data[:min_channels]
        
        # è¨­ç½®æ–°çš„å±¤
        parts = layer_name.split('.')
        parent = self.backbone
        for i in range(len(parts)-1):
            if parts[i].isdigit():
                parent = parent[int(parts[i])]
            else:
                parent = getattr(parent, parts[i])
        setattr(parent, parts[-1], new_conv)
        
        print(f"âœ“ {layer_name} out_channels å¼·åˆ¶è¨­ç‚º {new_out_channels}")
        
        # åŒæ­¥æ›´æ–° BatchNorm
        if parts[-1].startswith('conv'):
            bn_name = parts[-1].replace('conv', 'bn')
            if hasattr(parent, bn_name):
                old_bn = getattr(parent, bn_name)
                new_bn = nn.BatchNorm2d(new_out_channels).to(old_bn.weight.device)
                min_channels = min(new_out_channels, old_bn.num_features)
                new_bn.weight.data[:min_channels] = old_bn.weight.data[:min_channels]
                new_bn.bias.data[:min_channels] = old_bn.bias.data[:min_channels]
                new_bn.running_mean[:min_channels] = old_bn.running_mean[:min_channels]
                new_bn.running_var[:min_channels] = old_bn.running_var[:min_channels]
                setattr(parent, bn_name, new_bn)
                print(f"âœ“ {'.'.join(parts[:-1])}.{bn_name} é€šé“æ•¸åŒæ­¥è¨­ç‚º {new_out_channels}")
        
        # æ›´æ–°ä¸‹ä¸€å±¤çš„è¼¸å…¥é€šé“
        if parts[-1].startswith('conv'):
            conv_idx = int(parts[-1][-1])
            if conv_idx < 3:  # åªè™•ç† conv1/conv2 åˆ°ä¸‹ä¸€å±¤
                next_conv_name = f"conv{conv_idx+1}"
                if hasattr(parent, next_conv_name):
                    old_next_conv = getattr(parent, next_conv_name)
                    new_next_conv = nn.Conv2d(
                        new_out_channels,
                        old_next_conv.out_channels,
                        old_next_conv.kernel_size,
                        old_next_conv.stride,
                        old_next_conv.padding,
                        old_next_conv.dilation,
                        old_next_conv.groups,
                        bias=old_next_conv.bias is not None
                    ).to(old_next_conv.weight.device)
                    min_channels = min(new_out_channels, old_next_conv.weight.size(1))
                    new_next_conv.weight.data[:, :min_channels] = old_next_conv.weight.data[:, :min_channels]
                    if old_next_conv.bias is not None:
                        new_next_conv.bias.data = old_next_conv.bias.data.clone()
                    setattr(parent, next_conv_name, new_next_conv)
                    print(f"âœ“ {'.'.join(parts[:-1])}.{next_conv_name} in_channels åŒæ­¥è¨­ç‚º {new_out_channels}")
        
        return True
    
    def _should_skip_pruning(self, layer_name):
        """ç¢ºå®šæ˜¯å¦æ‡‰è·³éè©²å±¤çš„å‰ªæä»¥ç¶­æŒ OS2D æ¶æ§‹"""
        import re
        
        # è§£æå±¤åç¨±
        m = re.match(r'(layer\d+)\.(\d+)\.(conv\d+)', layer_name)
        if not m:
            print(layer_name + " ä¸ç¬¦åˆé æœŸæ ¼å¼ï¼Œç„¡æ³•è§£æ")
            return False
        
        layer_prefix, block_idx_str, conv_name = m.groups()
        block_idx = int(block_idx_str)
        
        # æª¢æŸ¥è©²å±¤çš„ block
        layer = getattr(self.backbone, layer_prefix)
        block = layer[block_idx]
        
        # å¦‚æœæ˜¯ layer4ï¼Œæ›´è¬¹æ…ï¼Œå› ç‚ºå®ƒæ˜¯ OS2D çš„æœ€çµ‚è¼¸å‡º
        if layer_prefix == 'layer4':
            return True  # ç‚ºäº† OS2D å…¼å®¹æ€§è·³éå‰ªæ layer4
        
        # å¼·åˆ¶è·³éæ‰€æœ‰ conv3 å±¤
        if conv_name == 'conv3':
            print(f"âš ï¸ è·³éå‰ªæ {layer_name} (åŸå› : conv3å±¤)")
            return True
        
        # å¦‚æœæ˜¯å¸¶æœ‰ downsample çš„ block ä¸­çš„ conv1ï¼Œä½¿ç”¨å°ˆç”¨çš„è™•ç†æ–¹å¼
        if conv_name == 'conv1' and hasattr(block, 'downsample') and block.downsample is not None:
            print(f"â„¹ï¸ ç™¼ç¾å¸¶æœ‰ downsample çš„ conv1: {layer_name}ï¼Œå°‡ä½¿ç”¨å°ˆç”¨è™•ç†æ–¹å¼")
            return False # 
        
        # å¦‚æœæ˜¯æœ€å¾Œä¸€å€‹ blockï¼Œè·³éæ‰€æœ‰å‰ªæä»¥ä¿è­·è·¨å±¤é€£æ¥
        if block_idx == len(layer) - 1:
            return True
        
        return False
    
    def _handle_residual_connection(self, layer_name, keep_indices):
        """è™•ç†æ®˜å·®é€£æ¥"""
        print(f"\nğŸ” é–‹å§‹è™•ç†æ®˜å·®é€£æ¥: {layer_name}")
        
        # è§£æå±¤åç¨±
        parts = layer_name.split('.')
        if len(parts) < 3:
            print(f"âš ï¸ ç„¡æ•ˆçš„å±¤åç¨±æ ¼å¼: {layer_name}")
            return
        parts = layer_name.split('.')
        if parts[-1] == 'conv1':
            print(f"â„¹ï¸ conv1 å‰ªæä¸å½±éŸ¿ downsample è¼¸å‡ºé€šé“")
            return True
        layer_str, block_idx, conv_type = parts[0], int(parts[1]), parts[2]
        
        # ç²å–ç•¶å‰ block
        layer = getattr(self.backbone, layer_str)
        current_block = layer[block_idx]
        
        if conv_type == 'conv1':
            # æ›´æ–° conv2 çš„è¼¸å…¥é€šé“
            next_conv_name = f"{layer_str}.{block_idx}.conv2"
            next_conv = None
            for name, module in self.backbone.named_modules():
                if name == next_conv_name and isinstance(module, nn.Conv2d):
                    next_conv = module
                    break
            
            if next_conv is not None:
                # æª¢æŸ¥ç´¢å¼•ç¯„åœ
                keep_indices = keep_indices[keep_indices < next_conv.weight.size(1)]
                if len(keep_indices) == 0:
                    print(f"âš ï¸ æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºç¯„åœï¼Œè·³éæ›´æ–° {next_conv_name}")
                    return
                # æ›´æ–° conv2 çš„è¼¸å…¥é€šé“
                new_conv = nn.Conv2d(
                    len(keep_indices),
                    next_conv.out_channels,
                    next_conv.kernel_size,
                    next_conv.stride,
                    next_conv.padding,
                    next_conv.dilation,
                    next_conv.groups,
                    bias=next_conv.bias is not None
                ).to(next_conv.weight.device)
                
                # æ›´æ–°æ¬Šé‡ï¼Œç¢ºä¿ç´¢å¼•åœ¨æœ‰æ•ˆç¯„åœå…§
                try:
                    # æ›´æ–°æ¬Šé‡æ™‚ç¢ºä¿ç¶­åº¦åŒ¹é…
                    if next_conv.weight.size(1) != len(keep_indices):
                        # å¦‚æœè¼¸å…¥é€šé“æ•¸ä¸åŒ¹é…ï¼Œéœ€è¦èª¿æ•´æ¬Šé‡
                        old_weight = next_conv.weight.data
                        new_weight = torch.zeros(
                            old_weight.size(0),
                            len(keep_indices),
                            old_weight.size(2),
                            old_weight.size(3),
                            device=old_weight.device
                        )
                        # åªè¤‡è£½æœ‰æ•ˆçš„é€šé“
                        valid_indices = keep_indices[keep_indices < old_weight.size(1)]
                        new_weight[:, :len(valid_indices)] = old_weight[:, valid_indices]
                        new_conv.weight.data = new_weight
                    else:
                        new_conv.weight.data = next_conv.weight.data[:, keep_indices].clone()
                    
                    if next_conv.bias is not None:
                        new_conv.bias.data = next_conv.bias.data.clone()
                except IndexError as e:
                    print(f"âš ï¸ æ›´æ–°æ¬Šé‡æ™‚ç™¼ç”Ÿç´¢å¼•éŒ¯èª¤: {e}")
                    print(f"next_conv.weight.shape: {next_conv.weight.shape}")
                    print(f"keep_indices max: {keep_indices.max()}")
                    print(f"keep_indices min: {keep_indices.min()}")
                    print(f"keep_indices shape: {keep_indices.shape}")
                    return
                
                # æ›¿æ› conv2
                parts2 = next_conv_name.split('.')
                parent = self.backbone
                for i in range(len(parts2) - 1):
                    if parts2[i].isdigit():
                        parent = parent[int(parts2[i])]
                    else:
                        parent = getattr(parent, parts2[i])
                
                setattr(parent, parts2[-1], new_conv)
                print(f"âœ“ æ›´æ–° conv2 è¼¸å…¥é€šé“: {next_conv_name} (in_channels: {new_conv.in_channels})")
                
                # åŒæ­¥æ›´æ–° BatchNorm
                bn_name = next_conv_name.replace('conv', 'bn')
                if hasattr(parent, bn_name.split('.')[-1]):
                    old_bn = getattr(parent, bn_name.split('.')[-1])
                    new_bn = nn.BatchNorm2d(new_conv.out_channels).to(old_bn.weight.device)
                    new_bn.weight.data = old_bn.weight.data.clone()
                    new_bn.bias.data = old_bn.bias.data.clone()
                    new_bn.running_mean = old_bn.running_mean.clone()
                    new_bn.running_var = old_bn.running_var.clone()
                    setattr(parent, bn_name.split('.')[-1], new_bn)
                    print(f"âœ“ æ›´æ–° BatchNorm: {bn_name}")
        
        elif conv_type == 'conv2':
            # æ›´æ–° conv3 çš„è¼¸å…¥é€šé“
            next_conv_name = f"{layer_str}.{block_idx}.conv3"
            next_conv = None
            for name, module in self.backbone.named_modules():
                if name == next_conv_name and isinstance(module, nn.Conv2d):
                    next_conv = module
                    break
            
            if next_conv is not None:
                # æª¢æŸ¥ç´¢å¼•ç¯„åœ
                keep_indices = keep_indices[keep_indices < next_conv.weight.size(1)]
                if len(keep_indices) == 0:
                    print(f"âš ï¸ æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºç¯„åœï¼Œè·³éæ›´æ–° {next_conv_name}")
                    return
                    
                new_conv = nn.Conv2d(
                    len(keep_indices),
                    next_conv.out_channels,
                    next_conv.kernel_size,
                    next_conv.stride,
                    next_conv.padding,
                    next_conv.dilation,
                    next_conv.groups,
                    bias=next_conv.bias is not None
                ).to(next_conv.weight.device)
                
                try:
                    new_conv.weight.data = next_conv.weight.data[:, keep_indices, :, :].clone()
                    if next_conv.bias is not None:
                        new_conv.bias.data = next_conv.bias.data.clone()
                except IndexError as e:
                    print(f"âš ï¸ æ›´æ–°æ¬Šé‡æ™‚ç™¼ç”Ÿç´¢å¼•éŒ¯èª¤: {e}")
                    print(f"next_conv.weight.shape: {next_conv.weight.shape}")
                    print(f"keep_indices max: {keep_indices.max()}")
                    print(f"keep_indices min: {keep_indices.min()}")
                    print(f"keep_indices shape: {keep_indices.shape}")
                    return
                
                parts2 = next_conv_name.split('.')
                parent = self.backbone
                for i in range(len(parts2) - 1):
                    if parts2[i].isdigit():
                        parent = parent[int(parts2[i])]
                    else:
                        parent = getattr(parent, parts2[i])
                
                setattr(parent, parts2[-1], new_conv)
                print(f"âœ“ æ›´æ–° conv3 è¼¸å…¥é€šé“: {next_conv_name}")
    
    def _prune_conv_layer(self, layer_name, keep_indices):
        """
        å‰ªææŒ‡å®šçš„å·ç©å±¤

        Args:
            layer_name: è¦å‰ªæçš„å±¤åç¨±
            keep_indices: è¦ä¿ç•™çš„é€šé“ç´¢å¼•
        """
        # ç²å–ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name and isinstance(module, nn.Conv2d):
                target_layer = module
                break

        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False

        # è¨­ç½®æ–°çš„è¼¸å‡ºé€šé“æ•¸
        self.set_layer_out_channels(layer_name, len(keep_indices))

        # è™•ç†æ®˜å·®é€£æ¥
        self._handle_residual_connection(layer_name, keep_indices)

        # è™•ç† downsample é€£æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
        # parts = layer_name.split('.')
        # layer_str, block_idx, conv_type = parts[0], int(parts[1]), parts[2]
        # block = getattr(self.backbone, layer_str)[block_idx]
        # if conv_type != "conv1" and hasattr(block, "downsample") and block.downsample is not None:
        #     self._handle_downsample_connection(layer_name, keep_indices)

        return True
    
    def _reset_batchnorm_stats(self):
        """é‡ç½®æ‰€æœ‰ BatchNorm å±¤çš„çµ±è¨ˆæ•¸æ“š"""
        for m in self.backbone.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.reset_running_stats()
        print("âœ“ å·²é‡ç½®æ‰€æœ‰ BatchNorm å±¤çš„çµ±è¨ˆæ•¸æ“š")
    
    def prune_channel(self, layer_name, prune_ratio=0.3, images=None, boxes=None, labels=None, auxiliary_net=None):
        """
        å°æŒ‡å®šå±¤é€²è¡Œé€šé“å‰ªæ
        
        Args:
            layer_name: è¦å‰ªæçš„å±¤åç¨±
            prune_ratio: å‰ªææ¯”ä¾‹ (0.0-1.0)
            images, boxes, labels: ç”¨æ–¼è¨ˆç®—é€šé“é‡è¦æ€§çš„æ•¸æ“š
            auxiliary_net: è¼”åŠ©ç¶²è·¯ï¼Œç”¨æ–¼è©•ä¼°é€šé“é‡è¦æ€§
        """
        # æª¢æŸ¥æ˜¯å¦æ‡‰è·³éå‰ªæ
        if self._should_skip_pruning(layer_name):
            print(f"âš ï¸ è·³éå‰ªæå±¤ {layer_name}")
            return "SKIPPED"
        
        # ç²å–ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name and isinstance(module, nn.Conv2d):
                target_layer = module
                break
        
        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False
        
        # è¨ˆç®—é€šé“é‡è¦æ€§
        if images is not None and boxes is not None and auxiliary_net is not None:
            # åˆå§‹åŒ–é€šé“é¸æ“‡å™¨
            channel_selector = OS2DChannelSelector(
                model=self, 
                auxiliary_net=auxiliary_net, 
                device=self.device
            )
            
            # è¨ˆç®—é€šé“é‡è¦æ€§
            importance_scores = channel_selector.compute_importance(layer_name, images, boxes, boxes, labels)
            
            # é¸æ“‡è¦ä¿ç•™çš„é€šé“
            keep_indices = channel_selector.select_channels(layer_name, importance_scores, prune_ratio)
            
            if keep_indices is None:
                print(f"âŒ ç„¡æ³•é¸æ“‡ {layer_name} çš„ä¿ç•™é€šé“")
                return False
        else:
            # å¦‚æœæ²’æœ‰æä¾›æ•¸æ“šï¼Œå‰‡éš¨æ©Ÿé¸æ“‡é€šé“
            num_channels = target_layer.out_channels
            num_keep = int(num_channels * (1 - prune_ratio))
            keep_indices = torch.randperm(num_channels)[:num_keep]
        
        # åŸ·è¡Œå‰ªæ
        success = self._prune_conv_layer(layer_name, keep_indices)
        
        return success
    
    def prune_model(self, prune_ratio=0.3, images=None, boxes=None, labels=None, auxiliary_net=None, prunable_layers=None):
        """
        å‰ªææ•´å€‹æ¨¡å‹ï¼Œæ ¹æ“š LCP å’Œ DCP è«–æ–‡çš„æ–¹æ³•
        
        Args:
            prune_ratio: å‰ªææ¯”ä¾‹ (0.0-1.0)
            images, boxes, labels: ç”¨æ–¼è¨ˆç®—é€šé“é‡è¦æ€§çš„æ•¸æ“š
            auxiliary_net: è¼”åŠ©ç¶²è·¯ï¼Œç”¨æ–¼è©•ä¼°é€šé“é‡è¦æ€§
            prunable_layers: è¦å‰ªæçš„å±¤åˆ—è¡¨ï¼Œå¦‚æœç‚º None å‰‡è‡ªå‹•é¸æ“‡
        """
        if prunable_layers is None:
            # ä¸»è¦é‡å° OS2D ä½¿ç”¨çš„ layer2 å’Œ layer3 é€²è¡Œå‰ªæ
            prunable_layers = []
            for layer_name in ['layer2', 'layer3']:
                layer = getattr(self.backbone, layer_name)
                for block_idx in range(len(layer)):
                    # å°æ–¼æ¯å€‹æ®˜å·®å¡Š
                    for conv_idx in [1, 2]:  # åªå‰ªæ conv1 å’Œ conv2 ä»¥ç¶­æŒæ¶æ§‹
                        conv_name = f"{layer_name}.{block_idx}.conv{conv_idx}"
                        prunable_layers.append(conv_name)
        
        print(f"ğŸ” é–‹å§‹æ¨¡å‹å‰ªæ (LCP + DCP)ï¼Œå‰ªææ¯”ä¾‹: {prune_ratio}...")
        print(f"ğŸ“‹ å¯å‰ªæå±¤: {prunable_layers}")
        
        # æŒ‰é †åºå‰ªææ¯ä¸€å±¤
        pruned_layers = []
        for layer_name in prunable_layers:
            print(f"\nğŸ”§ è™•ç†å±¤: {layer_name}")
            
            # æª¢æŸ¥æ˜¯å¦æ‡‰è·³éå‰ªæ
            if self._should_skip_pruning(layer_name):
                print(f"âš ï¸ è·³éå‰ªæå±¤ {layer_name}")
                continue
            
            # å‰ªæå±¤
            success = self.prune_channel(
                layer_name, 
                prune_ratio=prune_ratio, 
                images=images, 
                boxes=boxes, 
                labels=labels, 
                auxiliary_net=auxiliary_net
            )
            
            if success and success != "SKIPPED":
                pruned_layers.append(layer_name)
        
        # é‡ç½® BatchNorm çµ±è¨ˆæ•¸æ“š
        self._reset_batchnorm_stats()
        
        print(f"âœ… æ¨¡å‹å‰ªæå®Œæˆ! å…±å‰ªæ {len(pruned_layers)}/{len(prunable_layers)} å±¤")
        print(f"æˆåŠŸå‰ªæçš„å±¤: {pruned_layers}")
        
        return pruned_layers
    
    def visualize_model_architecture(self, output_path="model_architecture.png", input_shape=(1, 3, 224, 224)):
        """
        è¦–è¦ºåŒ–æ¨¡å‹æ¶æ§‹ä¸¦ä¿å­˜ç‚ºåœ–ç‰‡
        
        Args:
            output_path: è¼¸å‡ºåœ–ç‰‡è·¯å¾‘
            input_shape: è¼¸å…¥å¼µé‡å½¢ç‹€ï¼Œé»˜èªç‚º (1, 3, 224, 224)
        """
        try:
            import torchviz
            from graphviz import Digraph
        except ImportError:
            print("è«‹å…ˆå®‰è£å¿…è¦çš„å¥—ä»¶: pip install torchviz graphviz")
            return False
        
        # å‰µå»ºè¼¸å…¥å¼µé‡
        x = torch.randn(input_shape).to(self.device)
        
        # ç²å–è¼¸å‡º
        y = self(x)
        
        # ä½¿ç”¨ torchviz ç”Ÿæˆè¨ˆç®—åœ–
        dot = torchviz.make_dot(y, params=dict(self.named_parameters()))
        
        # è¨­ç½®åœ–çš„å±¬æ€§
        dot.attr('node', fontsize='12')
        dot.attr('graph', rankdir='TB')  # å¾ä¸Šåˆ°ä¸‹çš„ä½ˆå±€
        dot.attr('graph', size='12,12')  # åœ–çš„å¤§å°
        
        # ä¿å­˜åœ–ç‰‡
        dot.render(output_path.replace('.png', ''), format='png', cleanup=True)
        
        print(f"âœ… æ¨¡å‹æ¶æ§‹å·²ä¿å­˜è‡³: {output_path}")
        
        # è¼¸å‡ºæ¨¡å‹æ‘˜è¦
        self._print_model_summary()
        
        return True
    
    def _handle_downsample_connection(self, layer_name, keep_indices):
        """è™•ç† downsample é€£æ¥"""
        print(f"\nğŸ” è™•ç† downsample é€£æ¥: {layer_name}")
        
        # è§£æå±¤åç¨±
        parts = layer_name.split('.')
        if len(parts) < 3:
            print(f"âš ï¸ ç„¡æ•ˆçš„å±¤åç¨±æ ¼å¼: {layer_name}")
            return False
            
        layer_str, block_idx = parts[0], int(parts[1])
        
        # ç²å–ç•¶å‰ block å’Œ downsample
        layer = getattr(self.backbone, layer_str)
        current_block = layer[block_idx]
        
        if not hasattr(current_block, 'downsample') or current_block.downsample is None:
            return True
            
        downsample = current_block.downsample
        old_conv = downsample[0]  # downsample çš„ç¬¬ä¸€å±¤æ˜¯ conv
        old_bn = downsample[1]    # ç¬¬äºŒå±¤æ˜¯ bn
        
        # æ›´æ–° downsample çš„ conv å±¤
        new_conv = nn.Conv2d(
            old_conv.in_channels,
            len(keep_indices),  # æ–°çš„è¼¸å‡ºé€šé“æ•¸
            old_conv.kernel_size,
            old_conv.stride,
            old_conv.padding,
            old_conv.dilation,
            old_conv.groups,
            bias=old_conv.bias is not None
        ).to(old_conv.weight.device)
        
        # æ›´æ–°æ¬Šé‡
        new_conv.weight.data = old_conv.weight.data[keep_indices].clone()
        if old_conv.bias is not None:
            new_conv.bias.data = old_conv.bias.data[keep_indices].clone()
        
        # æ›´æ–° downsample çš„ bn å±¤
        new_bn = nn.BatchNorm2d(len(keep_indices)).to(old_bn.weight.device)
        new_bn.weight.data = old_bn.weight.data[keep_indices].clone()
        new_bn.bias.data = old_bn.bias.data[keep_indices].clone()
        new_bn.running_mean = old_bn.running_mean[keep_indices].clone()
        new_bn.running_var = old_bn.running_var[keep_indices].clone()
        
        # å‰µå»ºæ–°çš„ downsample sequential
    def get_feature_map(self, x):
        """ç²å–ç‰¹å¾µåœ–"""
        feature_maps = self.backbone(x)
        return feature_maps
        
    def forward(self, images, class_images=None, **kwargs):
        """æ”¯æ´æ¥æ”¶ class_images åƒæ•¸çš„å‰å‘å‚³æ’­"""
        if class_images is not None:
            return super().forward(images, class_images=class_images, **kwargs)
        else:
            return super().forward(images, **kwargs)
        
    
    def get_feature_map(self, x):
        """ç²å–ç‰¹å¾µåœ–"""
        feature_maps = self.backbone(x)
        return feature_maps
    
    def _print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦ä¿¡æ¯ï¼ŒåŒ…å«æ¯å±¤çš„é€šé“æ•¸"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        # ç²å–æ¯å±¤çš„åƒæ•¸å’Œé€šé“ä¿¡æ¯
        layer_info = {}
        for name, module in self.backbone.named_modules():
            if isinstance(module, nn.Conv2d):
                layer_info[name] = {
                    'type': 'Conv2d',
                    'params': sum(p.numel() for p in module.parameters()),
                    'in_channels': module.in_channels,
                    'out_channels': module.out_channels
                }
            elif isinstance(module, nn.BatchNorm2d):
                layer_info[name] = {
                    'type': 'BatchNorm2d',
                    'params': sum(p.numel() for p in module.parameters()),
                    'num_features': module.num_features
                }

        # æ‰“å°æ‘˜è¦
        print("\n====== æ¨¡å‹æ‘˜è¦ ======")
        print(f"ç¸½åƒæ•¸é‡: {total_params:,}")
        print(f"å¯è¨“ç·´åƒæ•¸é‡: {trainable_params:,}")
        print("\nå±¤ç´šçµæ§‹åˆ†æ:")

        # æŒ‰å±¤åæ’åºæ‰“å°
        for layer_name in sorted(layer_info.keys()):
            info = layer_info[layer_name]
            if info['type'] == 'Conv2d':
                # æª¢æŸ¥æ˜¯å¦ç‚º downsample å±¤
                is_downsample = 'downsample' in layer_name
                layer_type = 'æ®˜å·®åˆ†æ”¯' if is_downsample else 'ä¸»åˆ†æ”¯'
                print(f"\n- {layer_name} ({layer_type}):")
                print(f"  é¡å‹: {info['type']}")
                print(f"  è¼¸å…¥é€šé“: {info['in_channels']}")
                print(f"  è¼¸å‡ºé€šé“: {info['out_channels']}")
                print(f"  åƒæ•¸é‡: {info['params']:,}")
            else:  # BatchNorm2d
                print(f"\n- {layer_name}:")
                print(f"  é¡å‹: {info['type']}")
                print(f"  ç‰¹å¾µæ•¸: {info['num_features']}")
                print(f"  åƒæ•¸é‡: {info['params']:,}")

        # æ·»åŠ å±¤ç´šé—œä¿‚åˆ†æ
        print("\n====== å±¤ç´šé€£æ¥åˆ†æ ======")
        for layer_idx in range(1, 4):  # layer1 åˆ° layer4
            layer = getattr(self.backbone, f'layer{layer_idx}')
            print(f"\n[Layer {layer_idx}]")
            for block_idx in range(len(layer)):
                block = layer[block_idx]
                print(f"\nBlock {block_idx}:")
                # æ‰“å°ä¸»åˆ†æ”¯
                if hasattr(block, 'conv1'):
                    print(f"  Conv1: {block.conv1.in_channels} -> {block.conv1.out_channels}")
                if hasattr(block, 'conv2'):
                    print(f"  Conv2: {block.conv2.in_channels} -> {block.conv2.out_channels}")
                if hasattr(block, 'conv3'):
                    print(f"  Conv3: {block.conv3.in_channels} -> {block.conv3.out_channels}")
                # æ‰“å°æ®˜å·®åˆ†æ”¯
                if hasattr(block, 'downsample') and block.downsample is not None:
                    downsample_conv = block.downsample[0]
                    print(f"  Downsample: {downsample_conv.in_channels} -> {downsample_conv.out_channels}")