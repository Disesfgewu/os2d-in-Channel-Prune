import torch
import torch.nn as nn
import os
import time
import datetime
import logging
import traceback
import copy
from os2d.modeling.model import Os2dModel
from os2d.modeling.feature_extractor import build_feature_extractor
from src.lcp_channel_selector import OS2DChannelSelector
from src.gIoU_loss import GIoULoss

class Os2dModelInPrune(Os2dModel):
    """
    æ“´å±• OS2D æ¨¡å‹ä»¥æ”¯æŒé€šé“å‰ªæåŠŸèƒ½
    """
    def __init__(self, logger=None, is_cuda=False, backbone_arch="resnet50", 
                 use_group_norm=False, img_normalization=None, 
                 pretrained_path=None, pruned_checkpoint=None, **kwargs):
        # å¦‚æœæ²’æœ‰æä¾› loggerï¼Œå‰µå»ºä¸€å€‹
        if logger is None:
            logger = logging.getLogger("OS2D")
        
        # èª¿ç”¨çˆ¶é¡åˆå§‹åŒ–
        self.device = torch.device('cuda' if is_cuda else 'cpu')
        
        super(Os2dModelInPrune, self).__init__(
            logger=logger,
            is_cuda=is_cuda,
            backbone_arch=backbone_arch,
            use_group_norm=use_group_norm,
            img_normalization=img_normalization,
            **kwargs
        )
        
        self.backbone = self.net_feature_maps
        # å°‡æ¨¡å‹ç§»è‡³æŒ‡å®šè¨­å‚™
        if is_cuda:
            self.cuda()
            self.backbone = self.backbone.cuda()
            # ç¢ºä¿æ‰€æœ‰å­æ¨¡å¡Šéƒ½åœ¨ CUDA ä¸Š
            for module in self.modules():
                if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
                    module.cuda()
        # è¼‰å…¥é è¨“ç·´æ¬Šé‡
        if pretrained_path:
            self.init_model_from_file(pretrained_path)
        self.device = torch.device('cuda' if is_cuda else 'cpu')
        self.original_device = self.device  # ä¿å­˜åŸå§‹è¨­å‚™
        
        self.teacher_model = copy.deepcopy(self)
        self.teacher_model.eval()
        for p in self.teacher_model.parameters():
            p.requires_grad = False

        if pruned_checkpoint:
           self.load_checkpoint(pruned_checkpoint)
        if is_cuda:
            try:
                self.cuda()
                self.backbone = self.backbone.cuda()
                # ç¢ºä¿æ‰€æœ‰å­æ¨¡å¡Šéƒ½åœ¨ CUDA ä¸Š
                for module in self.modules():
                    if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
                        try:
                            module.cuda()
                        except RuntimeError as e:
                            print(f"âš ï¸ æ¨¡å¡Š {type(module)} ç„¡æ³•ç§»è‡³ CUDA: {e}")
                            # å¦‚æœå¤±æ•—ï¼Œå…ˆç”¨ CPU
                            module.cpu()
            except RuntimeError as e:
                print(f"âš ï¸ CUDA åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨ CPU ä½œç‚ºå‚™é¸: {e}")
                self.device = torch.device('cpu')
                self.cpu()
                
    
    def _safe_forward(self, x, device=None, class_images=None):
        """å®‰å…¨çš„å‰å‘å‚³æ’­ï¼Œå¦‚æœ GPU åŸ·è¡Œå¤±æ•—æœƒfallbackåˆ°CPU"""
        if x.dim() == 3:  # å¦‚æœæ˜¯ [C, H, W]
            x = x.unsqueeze(0)  # è½‰æ›ç‚º [1, C, H, W]
        try:
            if class_images is not None:
                return self(x, class_images=class_images)
            return self(x)
        except RuntimeError as e:
            if "Input type" in str(e) and device and device.type == 'cuda':
                print("âš ï¸ GPU åŸ·è¡Œå¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ CPU...")
                # æš«æ™‚å°‡æ¨¡å‹å’Œè¼¸å…¥ç§»åˆ° CPU
                original_device = next(self.parameters()).device
                cpu_model = self.cpu()
                cpu_x = x.cpu()
                cpu_class_images = class_images.cpu() if class_images is not None else None
                
                try:
                    # åœ¨ CPU ä¸ŠåŸ·è¡Œ
                    with torch.no_grad():
                        if cpu_class_images is not None:
                            output = cpu_model(cpu_x, class_images=cpu_class_images)
                        else:
                            output = cpu_model(cpu_x)
                    
                    # åŸ·è¡ŒæˆåŠŸå¾Œç§»å› GPU
                    self.to(original_device)
                    output = output.to(original_device)
                    print("âœ“ CPU åŸ·è¡ŒæˆåŠŸï¼Œå·²ç§»å› GPU")
                    return output
                
                except Exception as cpu_e:
                    print(f"âŒ CPU åŸ·è¡Œä¹Ÿå¤±æ•—: {cpu_e}")
                    # ç¢ºä¿æ¨¡å‹ç§»å›åŸå§‹è¨­å‚™
                    self.to(original_device)
                    raise cpu_e
            else:
                raise e  # å¦‚æœä¸æ˜¯è¨­å‚™å•é¡Œï¼Œå‰‡é‡æ–°å¼•ç™¼éŒ¯èª¤
    def set_layer_out_channels(self, layer_name, new_out_channels):
        """è¨­ç½®æŒ‡å®šå±¤çš„è¼¸å‡ºé€šé“æ•¸"""
        # å°‹æ‰¾ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name:
                target_layer = module
                break
                
        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False
            
        if not isinstance(target_layer, nn.Conv2d):
            print(f"âŒ å±¤ {layer_name} ä¸æ˜¯å·ç©å±¤")
            return False
            
        # å‰µå»ºæ–°çš„å·ç©å±¤
        new_conv = nn.Conv2d(
            in_channels=target_layer.in_channels,
            out_channels=new_out_channels,
            kernel_size=target_layer.kernel_size,
            stride=target_layer.stride,
            padding=target_layer.padding,
            dilation=target_layer.dilation,
            groups=target_layer.groups,
            bias=target_layer.bias is not None
        ).to(target_layer.weight.device)
        
        # å°‡åŸå§‹æ¬Šé‡è¤‡è£½åˆ°æ–°å±¤
        with torch.no_grad():
            # åªè¤‡è£½éœ€è¦çš„é€šé“
            min_channels = min(new_out_channels, target_layer.weight.size(0))
            new_conv.weight.data[:min_channels] = target_layer.weight.data[:min_channels]
            if target_layer.bias is not None:
                new_conv.bias.data[:min_channels] = target_layer.bias.data[:min_channels]
        
        # è¨­ç½®æ–°çš„å±¤
        parts = layer_name.split('.')
        parent = self.backbone
        for i in range(len(parts)-1):
            if parts[i].isdigit():
                parent = parent[int(parts[i])]
            else:
                parent = getattr(parent, parts[i])
        setattr(parent, parts[-1], new_conv)
        
        print(f"âœ“ {layer_name} out_channels å¼·åˆ¶è¨­ç‚º {new_out_channels}")
        
        # åŒæ­¥æ›´æ–° BatchNorm
        if parts[-1].startswith('conv'):
            bn_name = parts[-1].replace('conv', 'bn')
            if hasattr(parent, bn_name):
                old_bn = getattr(parent, bn_name)
                new_bn = nn.BatchNorm2d(new_out_channels).to(old_bn.weight.device)
                min_channels = min(new_out_channels, old_bn.num_features)
                new_bn.weight.data[:min_channels] = old_bn.weight.data[:min_channels]
                new_bn.bias.data[:min_channels] = old_bn.bias.data[:min_channels]
                new_bn.running_mean[:min_channels] = old_bn.running_mean[:min_channels]
                new_bn.running_var[:min_channels] = old_bn.running_var[:min_channels]
                setattr(parent, bn_name, new_bn)
                print(f"âœ“ {'.'.join(parts[:-1])}.{bn_name} é€šé“æ•¸åŒæ­¥è¨­ç‚º {new_out_channels}")
        
        # æ›´æ–°ä¸‹ä¸€å±¤çš„è¼¸å…¥é€šé“
        if parts[-1].startswith('conv'):
            conv_idx = int(parts[-1][-1])
            if conv_idx < 3:  # åªè™•ç† conv1/conv2 åˆ°ä¸‹ä¸€å±¤
                next_conv_name = f"conv{conv_idx+1}"
                if hasattr(parent, next_conv_name):
                    old_next_conv = getattr(parent, next_conv_name)
                    new_next_conv = nn.Conv2d(
                        new_out_channels,
                        old_next_conv.out_channels,
                        old_next_conv.kernel_size,
                        old_next_conv.stride,
                        old_next_conv.padding,
                        old_next_conv.dilation,
                        old_next_conv.groups,
                        bias=old_next_conv.bias is not None
                    ).to(old_next_conv.weight.device)
                    min_channels = min(new_out_channels, old_next_conv.weight.size(1))
                    new_next_conv.weight.data[:, :min_channels] = old_next_conv.weight.data[:, :min_channels]
                    if old_next_conv.bias is not None:
                        new_next_conv.bias.data = old_next_conv.bias.data.clone()
                    setattr(parent, next_conv_name, new_next_conv)
                    print(f"âœ“ {'.'.join(parts[:-1])}.{next_conv_name} in_channels åŒæ­¥è¨­ç‚º {new_out_channels}")
        
        return True
    
    def _should_skip_pruning(self, layer_name):
        """ç¢ºå®šæ˜¯å¦æ‡‰è·³éè©²å±¤çš„å‰ªæä»¥ç¶­æŒ OS2D æ¶æ§‹"""
        import re
        
        # è§£æå±¤åç¨±
        m = re.match(r'(layer\d+)\.(\d+)\.(conv\d+)', layer_name)
        if not m:
            print(layer_name + " ä¸ç¬¦åˆé æœŸæ ¼å¼ï¼Œç„¡æ³•è§£æ")
            return False
        
        layer_prefix, block_idx_str, conv_name = m.groups()
        block_idx = int(block_idx_str)
        
        # æª¢æŸ¥è©²å±¤çš„ block
        layer = getattr(self.backbone, layer_prefix)
        block = layer[block_idx]
        
        # å¦‚æœæ˜¯ layer4ï¼Œæ›´è¬¹æ…ï¼Œå› ç‚ºå®ƒæ˜¯ OS2D çš„æœ€çµ‚è¼¸å‡º
        if layer_prefix == 'layer4':
            return True  # ç‚ºäº† OS2D å…¼å®¹æ€§è·³éå‰ªæ layer4
        
        # å¼·åˆ¶è·³éæ‰€æœ‰ conv3 å±¤
        if conv_name == 'conv3':
            print(f"âš ï¸ è·³éå‰ªæ {layer_name} (åŸå› : conv3å±¤)")
            return True
        
        # å¦‚æœæ˜¯å¸¶æœ‰ downsample çš„ block ä¸­çš„ conv1ï¼Œä½¿ç”¨å°ˆç”¨çš„è™•ç†æ–¹å¼
        if conv_name == 'conv1' and hasattr(block, 'downsample') and block.downsample is not None:
            print(f"â„¹ï¸ ç™¼ç¾å¸¶æœ‰ downsample çš„ conv1: {layer_name}ï¼Œå°‡ä½¿ç”¨å°ˆç”¨è™•ç†æ–¹å¼")
            return False # 
        
        # å¦‚æœæ˜¯æœ€å¾Œä¸€å€‹ blockï¼Œè·³éæ‰€æœ‰å‰ªæä»¥ä¿è­·è·¨å±¤é€£æ¥
        if block_idx == len(layer) - 1:
            return True
        
        return False
    
    def _handle_residual_connection(self, layer_name, keep_indices):
        """è™•ç†æ®˜å·®é€£æ¥"""
        print(f"\nğŸ” é–‹å§‹è™•ç†æ®˜å·®é€£æ¥: {layer_name}")
        
        # è§£æå±¤åç¨±
        parts = layer_name.split('.')
        if len(parts) < 3:
            print(f"âš ï¸ ç„¡æ•ˆçš„å±¤åç¨±æ ¼å¼: {layer_name}")
            return
        parts = layer_name.split('.')
        if parts[-1] == 'conv1':
            print(f"â„¹ï¸ conv1 å‰ªæä¸å½±éŸ¿ downsample è¼¸å‡ºé€šé“")
            return True
        layer_str, block_idx, conv_type = parts[0], int(parts[1]), parts[2]
        
        # ç²å–ç•¶å‰ block
        layer = getattr(self.backbone, layer_str)
        current_block = layer[block_idx]
        
        if conv_type == 'conv1':
            # æ›´æ–° conv2 çš„è¼¸å…¥é€šé“
            next_conv_name = f"{layer_str}.{block_idx}.conv2"
            next_conv = None
            for name, module in self.backbone.named_modules():
                if name == next_conv_name and isinstance(module, nn.Conv2d):
                    next_conv = module
                    break
            
            if next_conv is not None:
                # æª¢æŸ¥ç´¢å¼•ç¯„åœ
                keep_indices = keep_indices[keep_indices < next_conv.weight.size(1)]
                if len(keep_indices) == 0:
                    print(f"âš ï¸ æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºç¯„åœï¼Œè·³éæ›´æ–° {next_conv_name}")
                    return
                # æ›´æ–° conv2 çš„è¼¸å…¥é€šé“
                new_conv = nn.Conv2d(
                    len(keep_indices),
                    next_conv.out_channels,
                    next_conv.kernel_size,
                    next_conv.stride,
                    next_conv.padding,
                    next_conv.dilation,
                    next_conv.groups,
                    bias=next_conv.bias is not None
                ).to(next_conv.weight.device)
                
                # æ›´æ–°æ¬Šé‡ï¼Œç¢ºä¿ç´¢å¼•åœ¨æœ‰æ•ˆç¯„åœå…§
                try:
                    # æ›´æ–°æ¬Šé‡æ™‚ç¢ºä¿ç¶­åº¦åŒ¹é…
                    if next_conv.weight.size(1) != len(keep_indices):
                        # å¦‚æœè¼¸å…¥é€šé“æ•¸ä¸åŒ¹é…ï¼Œéœ€è¦èª¿æ•´æ¬Šé‡
                        old_weight = next_conv.weight.data
                        new_weight = torch.zeros(
                            old_weight.size(0),
                            len(keep_indices),
                            old_weight.size(2),
                            old_weight.size(3),
                            device=old_weight.device
                        )
                        # åªè¤‡è£½æœ‰æ•ˆçš„é€šé“
                        valid_indices = keep_indices[keep_indices < old_weight.size(1)]
                        new_weight[:, :len(valid_indices)] = old_weight[:, valid_indices]
                        new_conv.weight.data = new_weight
                    else:
                        new_conv.weight.data = next_conv.weight.data[:, keep_indices].clone()
                    
                    if next_conv.bias is not None:
                        new_conv.bias.data = next_conv.bias.data.clone()
                except IndexError as e:
                    print(f"âš ï¸ æ›´æ–°æ¬Šé‡æ™‚ç™¼ç”Ÿç´¢å¼•éŒ¯èª¤: {e}")
                    print(f"next_conv.weight.shape: {next_conv.weight.shape}")
                    print(f"keep_indices max: {keep_indices.max()}")
                    print(f"keep_indices min: {keep_indices.min()}")
                    print(f"keep_indices shape: {keep_indices.shape}")
                    return
                
                # æ›¿æ› conv2
                parts2 = next_conv_name.split('.')
                parent = self.backbone
                for i in range(len(parts2) - 1):
                    if parts2[i].isdigit():
                        parent = parent[int(parts2[i])]
                    else:
                        parent = getattr(parent, parts2[i])
                
                setattr(parent, parts2[-1], new_conv)
                print(f"âœ“ æ›´æ–° conv2 è¼¸å…¥é€šé“: {next_conv_name} (in_channels: {new_conv.in_channels})")
                
                # åŒæ­¥æ›´æ–° BatchNorm
                bn_name = next_conv_name.replace('conv', 'bn')
                if hasattr(parent, bn_name.split('.')[-1]):
                    old_bn = getattr(parent, bn_name.split('.')[-1])
                    new_bn = nn.BatchNorm2d(new_conv.out_channels).to(old_bn.weight.device)
                    new_bn.weight.data = old_bn.weight.data.clone()
                    new_bn.bias.data = old_bn.bias.data.clone()
                    new_bn.running_mean = old_bn.running_mean.clone()
                    new_bn.running_var = old_bn.running_var.clone()
                    setattr(parent, bn_name.split('.')[-1], new_bn)
                    print(f"âœ“ æ›´æ–° BatchNorm: {bn_name}")
        
        elif conv_type == 'conv2':
            # æ›´æ–° conv3 çš„è¼¸å…¥é€šé“
            next_conv_name = f"{layer_str}.{block_idx}.conv3"
            next_conv = None
            for name, module in self.backbone.named_modules():
                if name == next_conv_name and isinstance(module, nn.Conv2d):
                    next_conv = module
                    break
            
            if next_conv is not None:
                # æª¢æŸ¥ç´¢å¼•ç¯„åœ
                keep_indices = keep_indices[keep_indices < next_conv.weight.size(1)]
                if len(keep_indices) == 0:
                    print(f"âš ï¸ æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºç¯„åœï¼Œè·³éæ›´æ–° {next_conv_name}")
                    return
                    
                new_conv = nn.Conv2d(
                    len(keep_indices),
                    next_conv.out_channels,
                    next_conv.kernel_size,
                    next_conv.stride,
                    next_conv.padding,
                    next_conv.dilation,
                    next_conv.groups,
                    bias=next_conv.bias is not None
                ).to(next_conv.weight.device)
                
                try:
                    new_conv.weight.data = next_conv.weight.data[:, keep_indices, :, :].clone()
                    if next_conv.bias is not None:
                        new_conv.bias.data = next_conv.bias.data.clone()
                except IndexError as e:
                    print(f"âš ï¸ æ›´æ–°æ¬Šé‡æ™‚ç™¼ç”Ÿç´¢å¼•éŒ¯èª¤: {e}")
                    print(f"next_conv.weight.shape: {next_conv.weight.shape}")
                    print(f"keep_indices max: {keep_indices.max()}")
                    print(f"keep_indices min: {keep_indices.min()}")
                    print(f"keep_indices shape: {keep_indices.shape}")
                    return
                
                parts2 = next_conv_name.split('.')
                parent = self.backbone
                for i in range(len(parts2) - 1):
                    if parts2[i].isdigit():
                        parent = parent[int(parts2[i])]
                    else:
                        parent = getattr(parent, parts2[i])
                
                setattr(parent, parts2[-1], new_conv)
                print(f"âœ“ æ›´æ–° conv3 è¼¸å…¥é€šé“: {next_conv_name}")
    
    def _prune_conv_layer(self, layer_name, keep_indices):
        """
        å‰ªææŒ‡å®šçš„å·ç©å±¤

        Args:
            layer_name: è¦å‰ªæçš„å±¤åç¨±
            keep_indices: è¦ä¿ç•™çš„é€šé“ç´¢å¼•
        """
        # ç²å–ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name and isinstance(module, nn.Conv2d):
                target_layer = module
                break

        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False

        # è¨­ç½®æ–°çš„è¼¸å‡ºé€šé“æ•¸
        self.set_layer_out_channels(layer_name, len(keep_indices))

        # è™•ç†æ®˜å·®é€£æ¥
        self._handle_residual_connection(layer_name, keep_indices)

        # è™•ç† downsample é€£æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
        # parts = layer_name.split('.')
        # layer_str, block_idx, conv_type = parts[0], int(parts[1]), parts[2]
        # block = getattr(self.backbone, layer_str)[block_idx]
        # if conv_type != "conv1" and hasattr(block, "downsample") and block.downsample is not None:
        #     self._handle_downsample_connection(layer_name, keep_indices)

        return True
    
    def _reset_batchnorm_stats(self):
        """é‡ç½®æ‰€æœ‰ BatchNorm å±¤çš„çµ±è¨ˆæ•¸æ“š"""
        for m in self.backbone.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.reset_running_stats()
        print("âœ“ å·²é‡ç½®æ‰€æœ‰ BatchNorm å±¤çš„çµ±è¨ˆæ•¸æ“š")
    
    def prune_channel(self, layer_name, prune_ratio=0.3, images=None, boxes=None, labels=None, auxiliary_net=None):
        """
        å°æŒ‡å®šå±¤é€²è¡Œé€šé“å‰ªæ
        
        Args:
            layer_name: è¦å‰ªæçš„å±¤åç¨±
            prune_ratio: å‰ªææ¯”ä¾‹ (0.0-1.0)
            images, boxes, labels: ç”¨æ–¼è¨ˆç®—é€šé“é‡è¦æ€§çš„æ•¸æ“š
            auxiliary_net: è¼”åŠ©ç¶²è·¯ï¼Œç”¨æ–¼è©•ä¼°é€šé“é‡è¦æ€§
        """
        # æª¢æŸ¥æ˜¯å¦æ‡‰è·³éå‰ªæ
        if self._should_skip_pruning(layer_name):
            print(f"âš ï¸ è·³éå‰ªæå±¤ {layer_name}")
            return "SKIPPED"
        
        # ç²å–ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name and isinstance(module, nn.Conv2d):
                target_layer = module
                break
        
        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False
        
        # è¨ˆç®—é€šé“é‡è¦æ€§
        if images is not None and boxes is not None and auxiliary_net is not None:
            # åˆå§‹åŒ–é€šé“é¸æ“‡å™¨
            channel_selector = OS2DChannelSelector(
                model=self, 
                auxiliary_net=auxiliary_net, 
                device=self.device
            )
            
            # è¨ˆç®—é€šé“é‡è¦æ€§
            importance_scores = channel_selector.compute_importance(layer_name, images, boxes, boxes, labels)
            
            # é¸æ“‡è¦ä¿ç•™çš„é€šé“
            keep_indices = channel_selector.select_channels(layer_name, importance_scores, prune_ratio)
            
            if keep_indices is None:
                print(f"âŒ ç„¡æ³•é¸æ“‡ {layer_name} çš„ä¿ç•™é€šé“")
                return False
        else:
            # å¦‚æœæ²’æœ‰æä¾›æ•¸æ“šï¼Œå‰‡éš¨æ©Ÿé¸æ“‡é€šé“
            num_channels = target_layer.out_channels
            num_keep = int(num_channels * (1 - prune_ratio))
            keep_indices = torch.randperm(num_channels)[:num_keep]
        
        # åŸ·è¡Œå‰ªæ
        success = self._prune_conv_layer(layer_name, keep_indices)
        
        return success
    
    def prune_model(self, prune_ratio=0.3, images=None, boxes=None, labels=None, auxiliary_net=None, prunable_layers=None):
        """
        å‰ªææ•´å€‹æ¨¡å‹ï¼Œæ ¹æ“š LCP å’Œ DCP è«–æ–‡çš„æ–¹æ³•
        
        Args:
            prune_ratio: å‰ªææ¯”ä¾‹ (0.0-1.0)
            images, boxes, labels: ç”¨æ–¼è¨ˆç®—é€šé“é‡è¦æ€§çš„æ•¸æ“š
            auxiliary_net: è¼”åŠ©ç¶²è·¯ï¼Œç”¨æ–¼è©•ä¼°é€šé“é‡è¦æ€§
            prunable_layers: è¦å‰ªæçš„å±¤åˆ—è¡¨ï¼Œå¦‚æœç‚º None å‰‡è‡ªå‹•é¸æ“‡
        """
        if prunable_layers is None:
            # ä¸»è¦é‡å° OS2D ä½¿ç”¨çš„ layer2 å’Œ layer3 é€²è¡Œå‰ªæ
            prunable_layers = []
            for layer_name in ['layer2', 'layer3']:
                layer = getattr(self.backbone, layer_name)
                for block_idx in range(len(layer)):
                    # å°æ–¼æ¯å€‹æ®˜å·®å¡Š
                    for conv_idx in [1, 2]:  # åªå‰ªæ conv1 å’Œ conv2 ä»¥ç¶­æŒæ¶æ§‹
                        conv_name = f"{layer_name}.{block_idx}.conv{conv_idx}"
                        prunable_layers.append(conv_name)
        
        print(f"ğŸ” é–‹å§‹æ¨¡å‹å‰ªæ (LCP + DCP)ï¼Œå‰ªææ¯”ä¾‹: {prune_ratio}...")
        print(f"ğŸ“‹ å¯å‰ªæå±¤: {prunable_layers}")
        
        # æŒ‰é †åºå‰ªææ¯ä¸€å±¤
        pruned_layers = []
        for layer_name in prunable_layers:
            print(f"\nğŸ”§ è™•ç†å±¤: {layer_name}")
            
            # æª¢æŸ¥æ˜¯å¦æ‡‰è·³éå‰ªæ
            if self._should_skip_pruning(layer_name):
                print(f"âš ï¸ è·³éå‰ªæå±¤ {layer_name}")
                continue
            
            # å‰ªæå±¤
            success = self.prune_channel(
                layer_name, 
                prune_ratio=prune_ratio, 
                images=images, 
                boxes=boxes, 
                labels=labels, 
                auxiliary_net=auxiliary_net
            )
            
            if success and success != "SKIPPED":
                pruned_layers.append(layer_name)
        
        # é‡ç½® BatchNorm çµ±è¨ˆæ•¸æ“š
        self._reset_batchnorm_stats()
        
        print(f"âœ… æ¨¡å‹å‰ªæå®Œæˆ! å…±å‰ªæ {len(pruned_layers)}/{len(prunable_layers)} å±¤")
        print(f"æˆåŠŸå‰ªæçš„å±¤: {pruned_layers}")
        
        return pruned_layers
    
    def visualize_model_architecture(self, output_path="model_architecture.png", input_shape=(1, 3, 224, 224)):
        """
        è¦–è¦ºåŒ–æ¨¡å‹æ¶æ§‹ä¸¦ä¿å­˜ç‚ºåœ–ç‰‡
        
        Args:
            output_path: è¼¸å‡ºåœ–ç‰‡è·¯å¾‘
            input_shape: è¼¸å…¥å¼µé‡å½¢ç‹€ï¼Œé»˜èªç‚º (1, 3, 224, 224)
        """
        try:
            import torchviz
            from graphviz import Digraph
            from tqdm import tqdm
            import os
            import time
            from datetime import datetime
            import traceback
        except ImportError:
            print("è«‹å…ˆå®‰è£å¿…è¦çš„å¥—ä»¶: pip install torchviz graphviz")
            return False
        
        # å‰µå»ºè¼¸å…¥å¼µé‡
        x = torch.randn(input_shape).to(self.device)
        
        # ç²å–è¼¸å‡º
        y = self(x)
        
        # ä½¿ç”¨ torchviz ç”Ÿæˆè¨ˆç®—åœ–
        dot = torchviz.make_dot(y, params=dict(self.named_parameters()))
        
        # è¨­ç½®åœ–çš„å±¬æ€§
        dot.attr('node', fontsize='12')
        dot.attr('graph', rankdir='TB')  # å¾ä¸Šåˆ°ä¸‹çš„ä½ˆå±€
        dot.attr('graph', size='12,12')  # åœ–çš„å¤§å°
        
        # ä¿å­˜åœ–ç‰‡
        dot.render(output_path.replace('.png', ''), format='png', cleanup=True)
        
        print(f"âœ… æ¨¡å‹æ¶æ§‹å·²ä¿å­˜è‡³: {output_path}")
        
        # è¼¸å‡ºæ¨¡å‹æ‘˜è¦
        self._print_model_summary()
        
        return True

    def get_feature_map(self, x):
        """ç²å–ç‰¹å¾µåœ–"""
        feature_maps = self.backbone(x)
        return feature_maps
        
    def forward(self, images, class_images=None, **kwargs):
        """æ”¯æ´æ¥æ”¶ class_images åƒæ•¸çš„å‰å‘å‚³æ’­"""
        if class_images is not None:
            return super().forward(images, class_images=class_images, **kwargs)
        else:
            return super().forward(images, **kwargs)
    
    def _print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦ä¿¡æ¯ï¼ŒåŒ…å«æ¯å±¤çš„é€šé“æ•¸"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        # ç²å–æ¯å±¤çš„åƒæ•¸å’Œé€šé“ä¿¡æ¯
        layer_info = {}
        for name, module in self.backbone.named_modules():
            if isinstance(module, nn.Conv2d):
                layer_info[name] = {
                    'type': 'Conv2d',
                    'params': sum(p.numel() for p in module.parameters()),
                    'in_channels': module.in_channels,
                    'out_channels': module.out_channels
                }
            elif isinstance(module, nn.BatchNorm2d):
                layer_info[name] = {
                    'type': 'BatchNorm2d',
                    'params': sum(p.numel() for p in module.parameters()),
                    'num_features': module.num_features
                }

        # æ‰“å°æ‘˜è¦
        print("\n====== æ¨¡å‹æ‘˜è¦ ======")
        print(f"ç¸½åƒæ•¸é‡: {total_params:,}")
        print(f"å¯è¨“ç·´åƒæ•¸é‡: {trainable_params:,}")
        print("\nå±¤ç´šçµæ§‹åˆ†æ:")

        # æŒ‰å±¤åæ’åºæ‰“å°
        for layer_name in sorted(layer_info.keys()):
            info = layer_info[layer_name]
            if info['type'] == 'Conv2d':
                # æª¢æŸ¥æ˜¯å¦ç‚º downsample å±¤
                is_downsample = 'downsample' in layer_name
                layer_type = 'æ®˜å·®åˆ†æ”¯' if is_downsample else 'ä¸»åˆ†æ”¯'
                print(f"\n- {layer_name} ({layer_type}):")
                print(f"  é¡å‹: {info['type']}")
                print(f"  è¼¸å…¥é€šé“: {info['in_channels']}")
                print(f"  è¼¸å‡ºé€šé“: {info['out_channels']}")
                print(f"  åƒæ•¸é‡: {info['params']:,}")
            else:  # BatchNorm2d
                print(f"\n- {layer_name}:")
                print(f"  é¡å‹: {info['type']}")
                print(f"  ç‰¹å¾µæ•¸: {info['num_features']}")
                print(f"  åƒæ•¸é‡: {info['params']:,}")

        # æ·»åŠ å±¤ç´šé—œä¿‚åˆ†æ
        print("\n====== å±¤ç´šé€£æ¥åˆ†æ ======")
        for layer_idx in range(1, 4):  # layer1 åˆ° layer4
            layer = getattr(self.backbone, f'layer{layer_idx}')
            print(f"\n[Layer {layer_idx}]")
            for block_idx in range(len(layer)):
                block = layer[block_idx]
                print(f"\nBlock {block_idx}:")
                # æ‰“å°ä¸»åˆ†æ”¯
                if hasattr(block, 'conv1'):
                    print(f"  Conv1: {block.conv1.in_channels} -> {block.conv1.out_channels}")
                if hasattr(block, 'conv2'):
                    print(f"  Conv2: {block.conv2.in_channels} -> {block.conv2.out_channels}")
                if hasattr(block, 'conv3'):
                    print(f"  Conv3: {block.conv3.in_channels} -> {block.conv3.out_channels}")
                # æ‰“å°æ®˜å·®åˆ†æ”¯
                if hasattr(block, 'downsample') and block.downsample is not None:
                    downsample_conv = block.downsample[0]
                    print(f"  Downsample: {downsample_conv.in_channels} -> {downsample_conv.out_channels}")
                    print(f"  Downsample Type: {type(downsample_conv).__name__}")
                    
    def _normalize_batch_images(self, images, device=None, target_size=(224, 224)):
        """
        æ¨™æº–åŒ–è™•ç†åœ–åƒæ‰¹æ¬¡ï¼Œç¢ºä¿æ‰€æœ‰åœ–åƒå°ºå¯¸ä¸€è‡´ä¸¦è½‰æ›ç‚ºæ‰¹æ¬¡å¼µé‡
        
        Args:
            images: åœ–åƒåˆ—è¡¨æˆ–å–®ä¸€å¼µé‡
            device: ç›®æ¨™è¨­å‚™
            target_size: ç›®æ¨™å°ºå¯¸ (H, W)
            
        Returns:
            torch.Tensor: æ‰¹æ¬¡åœ–åƒå¼µé‡ [B, C, H, W]
        """
        if device is None:
            device = self.device
            
        # è™•ç†å–®ä¸€å¼µé‡æƒ…æ³
        if isinstance(images, torch.Tensor):
            if images.dim() == 3:  # [C, H, W]
                images = images.unsqueeze(0)  # [1, C, H, W]
            return images.to(device)
            
        # è™•ç†åœ–åƒåˆ—è¡¨
        if isinstance(images, list):
            # éæ¿¾æœ‰æ•ˆåœ–åƒ
            valid_images = []
            for img in images:
                if img is None or not isinstance(img, torch.Tensor):
                    continue
                
                # ç¢ºä¿åœ–åƒæ˜¯ 3D å¼µé‡ [C, H, W]
                if img.dim() == 3:
                    # èª¿æ•´åœ–åƒå°ºå¯¸ç‚ºæ¨™æº–å°ºå¯¸ä¸¦ç§»è‡³è¨­å‚™
                    if img.shape[1] != target_size[0] or img.shape[2] != target_size[1]:
                        img = torch.nn.functional.interpolate(
                            img.unsqueeze(0),  # æ·»åŠ æ‰¹æ¬¡ç¶­åº¦
                            size=target_size,
                            mode='bilinear',
                            align_corners=False
                        ).squeeze(0)  # ç§»é™¤æ‰¹æ¬¡ç¶­åº¦
                    
                    valid_images.append(img.to(device))
                
            if len(valid_images) == 0:
                return None
                
            # å †ç–Šç‚ºæ‰¹æ¬¡å¼µé‡
            batch_tensor = torch.stack(valid_images)
            return batch_tensor
        
        return None
    
    def compute_classification_loss(self, outputs, class_ids):
        """
        è¨ˆç®—åˆ†é¡æå¤±
        
        Args:
            outputs: æ¨¡å‹è¼¸å‡º
            class_ids: ç›®æ¨™é¡åˆ¥ ID
            
        Returns:
            torch.Tensor: åˆ†é¡æå¤±
        """
        # å¾è¼¸å‡ºä¸­ç²å–åˆ†é¡åˆ†æ•¸
        if isinstance(outputs, dict) and 'class_scores' in outputs:
            class_scores = outputs['class_scores']
        else:
            # å¦‚æœè¼¸å‡ºä¸åŒ…å«åˆ†é¡åˆ†æ•¸ï¼Œå˜—è©¦ä½¿ç”¨æ•´å€‹è¼¸å‡ºä½œç‚ºåˆ†é¡åˆ†æ•¸
            class_scores = outputs

        # å°‡ç›®æ¨™è½‰æ›ç‚ºé©ç•¶çš„æ ¼å¼
        if isinstance(class_ids, list):
            # ç¢ºä¿åˆ—è¡¨ä¸­çš„æ¯å€‹å…ƒç´ éƒ½æ˜¯å¼µé‡ï¼Œä¸¦å°‡å®ƒå€‘æ‹¼æ¥
            tensor_items = [item for item in class_ids if isinstance(item, torch.Tensor)]
            if tensor_items:
                target = torch.cat(tensor_items).long()
            else:
                # å¦‚æœåˆ—è¡¨ä¸­æ²’æœ‰å¼µé‡ï¼Œå‰µå»ºä¸€å€‹é»˜èªå¼µé‡
                target = torch.zeros(1).long()
        elif isinstance(class_ids, tuple):
            # è™•ç†å…ƒçµ„é¡å‹ï¼Œå°‡å…¶è½‰æ›ç‚ºåˆ—è¡¨ä¸¦å†æ¬¡è™•ç†
            tensor_items = [item for item in class_ids if isinstance(item, torch.Tensor)]
            if tensor_items:
                target = torch.cat(tensor_items).long()
            else:
                target = torch.zeros(1).long()
        else:
            target = class_ids.long()
        

        device = next(self.parameters()).device
        target = target.to(device)
        
        # ä½¿ç”¨äº¤å‰ç†µæå¤±
        loss_fn = torch.nn.CrossEntropyLoss()
        
        # è™•ç† class_scores å¯èƒ½æ˜¯å…ƒçµ„çš„æƒ…æ³
        if isinstance(class_scores, tuple):
            # ä½¿ç”¨ç¬¬ä¸€å€‹å…ƒç´ ï¼Œé€šå¸¸åŒ…å«åˆ†é¡åˆ†æ•¸
            class_scores = class_scores[0]
        elif not isinstance(class_scores, torch.Tensor):
            print(f"Warning: class_scores é¡å‹ç‚º {type(class_scores)}ï¼Œç„¡æ³•è¨ˆç®—åˆ†é¡æå¤±")
            return torch.tensor(0.0, device=device)
        
        # æª¢æŸ¥ä¸¦ä¿®æ­£æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…çš„å•é¡Œ
        batch_size = class_scores.size(0)
        
        # ç¢ºä¿ target è‡³å°‘æ˜¯ 1D å¼µé‡
        if target.dim() == 0:
            target = target.unsqueeze(0)
            
        if target.size(0) != batch_size:
            # print(f"Warning: ä¿®æ­£ç›®æ¨™æ‰¹æ¬¡å¤§å° ({target.size(0)}) ä¸åŒ¹é…è¼¸å‡ºæ‰¹æ¬¡å¤§å° ({batch_size})")
            if target.size(0) > batch_size:
                # å¦‚æœç›®æ¨™æ‰¹æ¬¡è¼ƒå¤§ï¼Œæˆªæ–·ä»¥åŒ¹é…è¼¸å‡ºæ‰¹æ¬¡
                target = target[:batch_size]
            else:
                # å¦‚æœç›®æ¨™æ‰¹æ¬¡è¼ƒå°ï¼Œä½¿ç”¨é‡è¤‡ä¾†æ“´å±•
                repeats = (batch_size + target.size(0) - 1) // target.size(0)
                target = target.repeat(repeats)[:batch_size]
        
        # ç¢ºä¿ class_scores å½¢ç‹€æ­£ç¢º (batch_size, num_classes)
        if class_scores.dim() > 2:
            class_scores = class_scores.view(batch_size, -1)
            
        return loss_fn(class_scores, target)
    
    def compute_box_regression_loss(self, outputs, boxes):
        """
        è¨ˆç®—é‚Šç•Œæ¡†å›æ­¸æå¤±
        
        Args:
            outputs: æ¨¡å‹è¼¸å‡º
            boxes: ç›®æ¨™é‚Šç•Œæ¡†
            
        Returns:
            torch.Tensor: å›æ­¸æå¤±
        """
        # å¾è¼¸å‡ºä¸­ç²å–é æ¸¬æ¡†
        if isinstance(outputs, dict) and 'boxes' in outputs:
            pred_boxes = outputs['boxes']
        elif isinstance(outputs, tuple):
            # å¦‚æœè¼¸å‡ºæ˜¯å…ƒçµ„ï¼Œå‡è¨­ç¬¬äºŒå€‹å…ƒç´ åŒ…å«é‚Šç•Œæ¡†
            pred_boxes = outputs[1] if len(outputs) > 1 else torch.zeros(1, 4).to(self.device)
        else:
            # å¦‚æœæ²’æœ‰æ˜ç¢ºçš„é‚Šç•Œæ¡†è¼¸å‡ºï¼Œä½¿ç”¨é»˜èªå€¼
            pred_boxes = torch.zeros(1, 4).to(self.device)
            
        # å°‡ç›®æ¨™æ¡†è½‰æ›ç‚ºé©ç•¶çš„æ ¼å¼
        if isinstance(boxes, list):
            valid_boxes = [b for b in boxes if b is not None and b.numel() > 0]
            if not valid_boxes:
                return torch.tensor(0.0, device=pred_boxes.device)
            target_boxes = torch.cat(valid_boxes)
        else:
            target_boxes = boxes
            
        # ç¢ºä¿å¼µé‡åœ¨åŒä¸€è¨­å‚™ä¸Š
        target_boxes = target_boxes.to(pred_boxes.device)
        
        # å¦‚æœæ²’æœ‰æœ‰æ•ˆçš„ç›®æ¨™æ¡†ï¼Œè¿”å›é›¶æå¤±
        if target_boxes.numel() == 0:
            return torch.tensor(0.0, device=pred_boxes.device)
            
        # ç¢ºä¿é æ¸¬æ¡†å’Œç›®æ¨™æ¡†çš„å½¢ç‹€åŒ¹é…
        if pred_boxes.size(-1) != 4:
            pred_boxes = pred_boxes.view(-1, 4)
        if target_boxes.size(-1) != 4:
            target_boxes = target_boxes.view(-1, 4)
            
        # èª¿æ•´æ‰¹æ¬¡å¤§å°ä»¥åŒ¹é…
        if pred_boxes.size(0) != target_boxes.size(0):
            # å¦‚æœé æ¸¬æ¡†æ¯”ç›®æ¨™æ¡†å¤šï¼Œå–å‰Nå€‹
            if pred_boxes.size(0) > target_boxes.size(0):
                pred_boxes = pred_boxes[:target_boxes.size(0)]
            # å¦‚æœç›®æ¨™æ¡†æ¯”é æ¸¬æ¡†å¤šï¼Œå–å‰Nå€‹
            else:
                target_boxes = target_boxes[:pred_boxes.size(0)]
                
        # ä½¿ç”¨ L1 æå¤±ï¼Œå› ç‚ºå®ƒæ›´ç©©å®š
        loss_fn = torch.nn.L1Loss()
        try:
            loss = loss_fn(pred_boxes, target_boxes)
        except RuntimeError as e:
            print(f"è­¦å‘Š: L1æå¤±è¨ˆç®—å¤±æ•— - pred_boxes: {pred_boxes.shape}, target_boxes: {target_boxes.shape}")
            return torch.tensor(0.0, device=pred_boxes.device)
            
        return loss
    def train_one_epoch(self, train_loader, optimizer, 
                   auxiliary_net=None, device=None, 
                   print_freq=10, scheduler=None, 
                   loss_weights=None, use_lcp_loss=True, 
                   max_batches=None):
        """
        è¨“ç·´æ¨¡å‹ä¸€å€‹ epoch
        
        Args:
            train_loader: è³‡æ–™åŠ è¼‰å™¨
            optimizer: å„ªåŒ–å™¨
            auxiliary_net: è¼”åŠ©ç¶²çµ¡ï¼Œç”¨æ–¼é€šé“å‰ªæ (å¯é¸)
            device: è¨ˆç®—è¨­å‚™ (å¯é¸ï¼Œé»˜èªç‚ºæ¨¡å‹è¨­å‚™)
            print_freq: æ‰“å°é »ç‡ (å¯é¸ï¼Œé»˜èªæ¯10å€‹æ‰¹æ¬¡)
            scheduler: å­¸ç¿’ç‡èª¿åº¦å™¨ (å¯é¸)
            loss_weights: æå¤±æ¬Šé‡å­—å…¸ {'cls': 1.0, 'reg': 1.0} (å¯é¸)
            use_lcp_loss: æ˜¯å¦ä½¿ç”¨ LCP è«–æ–‡ä¸­çš„é‡å»ºæå¤± (å¯é¸)
            max_batches: æ¯å€‹ epoch è™•ç†çš„æœ€å¤§æ‰¹æ¬¡æ•¸ (å¯é¸)
            
        Returns:
            avg_loss: å¹³å‡æå¤±å€¼
            loss_components: æå¤±çµ„ä»¶å­—å…¸ {'cls': cls_loss, 'reg': reg_loss}
        """
        # 1. åˆå§‹åŒ–è¨“ç·´ç’°å¢ƒå’Œçµ±è¨ˆæ•¸æ“š
        device, loss_weights, stats = self._init_training_environment(
            auxiliary_net, device, loss_weights, use_lcp_loss)
        
        # 2. è¨­ç½®é€²åº¦æ¢
        progress_bar, num_batches = self._setup_progress_bar(train_loader, max_batches)
        
        # 3. è¿­ä»£è¨“ç·´è³‡æ–™
        for batch_idx, batch_data in enumerate(train_loader):
            if max_batches is not None and batch_idx >= max_batches:
                break
                
            try:
                # 3.1 è§£ææ‰¹æ¬¡è³‡æ–™
                images, boxes, class_ids = self._parse_batch_data(batch_data, device)
                
                # 3.2 æå–é¡åˆ¥åœ–åƒ
                class_images = self._extract_class_images(images, boxes, device)
                
                # 3.3 å‰å‘å‚³æ’­å’Œè¨ˆç®—æå¤±
                loss, cls_loss, reg_loss, recon_loss = self._forward_and_compute_loss(
                    images, class_images, boxes, class_ids, 
                    optimizer, auxiliary_net, loss_weights, use_lcp_loss, device)
                
                # 3.4 åå‘å‚³æ’­å’Œå„ªåŒ–
                self._backward_and_optimize(loss, optimizer, scheduler)
                
                # 3.5 æ›´æ–°çµ±è¨ˆè³‡æ–™
                stats = self._update_training_stats(
                    stats, loss, cls_loss, reg_loss, recon_loss, use_lcp_loss)
                
                # 3.6 æ›´æ–°é€²åº¦æ¢
                self._update_progress_bar(
                    progress_bar, batch_idx, print_freq, loss, cls_loss, reg_loss, optimizer)
                
            except Exception as e:
                print(f"âš ï¸ è™•ç†æ‰¹æ¬¡ {batch_idx} æ™‚å‡ºéŒ¯: {e}")
                traceback.print_exc()
                continue
        
        # 4. çµæŸè¨“ç·´ä¸¦è¿”å›çµæœ
        return self._finalize_training(progress_bar, stats, use_lcp_loss)

    def _init_training_environment(self, auxiliary_net, device, loss_weights, use_lcp_loss):
        """åˆå§‹åŒ–è¨“ç·´ç’°å¢ƒå’Œçµ±è¨ˆæ•¸æ“š"""
        # è¨­ç½®æ¨¡å‹ç‚ºè¨“ç·´æ¨¡å¼
        self.train()
        print(f"â„¹ï¸ ä¸»æ¨¡å‹è¨­ç‚ºè¨“ç·´æ¨¡å¼")
        
        # æª¢æŸ¥ä¸¦è¨­ç½®è¼”åŠ©ç¶²çµ¡ç‚ºè¨“ç·´æ¨¡å¼ (å¦‚æœå­˜åœ¨)
        if auxiliary_net is not None:
            auxiliary_net = auxiliary_net.train()
            print(f"â„¹ï¸ è¼”åŠ©ç¶²çµ¡è¨­ç‚ºè¨“ç·´æ¨¡å¼ï¼Œè¼¸å…¥é€šé“æ•¸: {auxiliary_net.get_current_channels()}")
        
        # è¨­ç½®è¨­å‚™
        if device is None:
            device = next(self.parameters()).device
            print(f"â„¹ï¸ ä½¿ç”¨æ¨¡å‹é è¨­è¨­å‚™: {device}")
        else:
            print(f"â„¹ï¸ ä½¿ç”¨æŒ‡å®šè¨­å‚™: {device}")
        
        # è¨­ç½®æå¤±æ¬Šé‡
        if loss_weights is None:
            loss_weights = {'cls': 1.0, 'reg': 1.0}
            if use_lcp_loss:
                loss_weights['recon'] = 0.1
        print(f"â„¹ï¸ æå¤±æ¬Šé‡è¨­ç½®: {loss_weights}")
        
        # åˆå§‹åŒ–çµ±è¨ˆæ•¸æ“š
        stats = {
            'start_time': time.time(),
            'batch_count': 0,
            'total_loss': 0.0,
            'cls_loss_total': 0.0,
            'reg_loss_total': 0.0,
            'recon_loss_total': 0.0 if use_lcp_loss else None
        }
        
        return device, loss_weights, stats

    def _setup_progress_bar(self, train_loader, max_batches):
        """è¨­ç½®é€²åº¦æ¢"""
        from tqdm import tqdm
        num_batches = len(train_loader)
        if max_batches is not None:
            num_batches = min(max_batches, num_batches)
        progress_bar = tqdm(total=num_batches, desc="Training", unit="batch")
        return progress_bar, num_batches

    def _parse_batch_data(self, batch_data, device):
        """è§£ææ‰¹æ¬¡è³‡æ–™"""
        # æ ¹æ“šæ‰¹æ¬¡æ•¸æ“šçµæ§‹è§£ææ•¸æ“š
        if len(batch_data) == 4:  # (images, boxes, labels, class_images)
            images, boxes, class_ids, _ = batch_data  # å¿½ç•¥åŸå§‹çš„ class_images
        elif len(batch_data) == 3:  # (images, boxes, labels)
            images, boxes, class_ids = batch_data
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ‰¹æ¬¡æ•¸æ“šæ ¼å¼: é•·åº¦ç‚º {len(batch_data)}")
        
        # å°‡æ•¸æ“šç§»è‡³ç›®æ¨™è¨­å‚™
        if isinstance(boxes, list):
            boxes = [box.to(device) if isinstance(box, torch.Tensor) else box for box in boxes]
        else:
            boxes = boxes.to(device)
        
        if isinstance(class_ids, list):
            class_ids = [cls_id.to(device) if isinstance(cls_id, torch.Tensor) else cls_id for cls_id in class_ids]
        else:
            class_ids = class_ids.to(device) if isinstance(class_ids, torch.Tensor) else class_ids
        
        # ç¢ºä¿ images æ˜¯æ‰¹æ¬¡å¼µé‡
        if isinstance(images, list):
            images = torch.stack(images).to(device)
        else:
            images = images.to(device)
            
        return images, boxes, class_ids

    def _extract_class_images(self, images, boxes, device, class_size=(64, 64)):
        """å¾åœ–åƒå’Œé‚Šç•Œæ¡†æå–é¡åˆ¥åœ–åƒ"""
        class_images = []
        
        for i in range(images.shape[0]):
            img = images[i]  # ç•¶å‰åœ–åƒ
            
            # ç²å–ç•¶å‰åœ–åƒçš„é‚Šç•Œæ¡†
            current_boxes = boxes[i] if isinstance(boxes, list) else boxes[i] if boxes.dim() > 1 else boxes
            
            # å¦‚æœæœ‰æœ‰æ•ˆçš„é‚Šç•Œæ¡†
            if current_boxes is not None and current_boxes.numel() > 0:
                class_img = self._crop_class_image_from_box(img, current_boxes, class_size)
            else:
                # ç„¡é‚Šç•Œæ¡† - ä½¿ç”¨åœ–åƒä¸­å¿ƒ
                class_img = self._crop_class_image_from_center(img, class_size)
            
            class_images.append(class_img)
        
        # å°‡é¡åˆ¥åœ–åƒå †ç–Šç‚ºæ‰¹æ¬¡å¼µé‡
        class_images = torch.stack(class_images).to(device)
        return class_images

    def _crop_class_image_from_box(self, img, boxes, class_size=(64, 64)):
        """å¾é‚Šç•Œæ¡†ä¸­è£å‰ªé¡åˆ¥åœ–åƒ"""
        # ä½¿ç”¨ç¬¬ä¸€å€‹æ¡†ä½œç‚ºé¡åˆ¥åœ–åƒæº
        x1, y1, x2, y2 = boxes[0].cpu().int().tolist()
        
        # ç¢ºä¿åº§æ¨™æœ‰æ•ˆ
        h, w = img.shape[1], img.shape[2]
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x2), min(h, y2)
        
        if x2 > x1 and y2 > y1:
            # æå–é¡åˆ¥åœ–åƒå€åŸŸ
            class_img = img[:, y1:y2, x1:x2].clone()
        else:
            # ç„¡æ•ˆæ¡†å€åŸŸ - ä½¿ç”¨åœ–åƒä¸­å¿ƒ
            class_img = self._crop_class_image_from_center(img, class_size)
            return class_img
            
        # èª¿æ•´å°ºå¯¸ç‚ºæ¨™æº–é¡åˆ¥åœ–åƒå°ºå¯¸
        class_img = torch.nn.functional.interpolate(
            class_img.unsqueeze(0),
            size=class_size,
            mode='bilinear',
            align_corners=False
        ).squeeze(0)
        
        return class_img

    def _crop_class_image_from_center(self, img, class_size=(64, 64)):
        """å¾åœ–åƒä¸­å¿ƒè£å‰ªé¡åˆ¥åœ–åƒ"""
        h, w = img.shape[1], img.shape[2]
        center_h, center_w = h // 2, w // 2
        size_h, size_w = h // 4, w // 4
        
        y1, y2 = max(0, center_h - size_h), min(h, center_h + size_h)
        x1, x2 = max(0, center_w - size_w), min(w, center_w + size_w)
        
        class_img = img[:, y1:y2, x1:x2].clone()
        class_img = torch.nn.functional.interpolate(
            class_img.unsqueeze(0),
            size=class_size,
            mode='bilinear',
            align_corners=False
        ).squeeze(0)
        
        return class_img

    def _forward_and_compute_loss(self, images, class_images, boxes, class_ids, 
                                optimizer, auxiliary_net, loss_weights, use_lcp_loss, device):
        """å‰å‘å‚³æ’­å’Œè¨ˆç®—æå¤±"""
        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()
        
        # å‰å‘å‚³æ’­
        outputs = self(images, class_images=class_images)
        
        # è¨ˆç®—åˆ†é¡æå¤±å’Œå›æ­¸æå¤±
        cls_loss = self.compute_classification_loss(outputs, class_ids)
        reg_loss = self.compute_box_regression_loss(outputs, boxes)
        
        # è¨ˆç®—é‡å»ºæå¤±ï¼ˆå¦‚æœä½¿ç”¨ LCPï¼‰
        recon_loss = torch.tensor(0.0, device=device)
        if use_lcp_loss and auxiliary_net is not None:
            recon_loss = self._compute_reconstruction_loss(images, boxes, auxiliary_net, device)
        
        # è¨ˆç®—ç¸½æå¤±
        loss = loss_weights['cls'] * cls_loss + loss_weights['reg'] * reg_loss
        if use_lcp_loss:
            # ç¢ºä¿ recon_loss æ˜¯æ¨™é‡å¼µé‡
            if recon_loss.dim() == 0:
                recon_loss = recon_loss.unsqueeze(0)
            loss = loss + loss_weights['recon'] * recon_loss.float()
        
        return loss, cls_loss, reg_loss, recon_loss

    def _compute_reconstruction_loss(self, images, boxes, auxiliary_net, device):
        """
        LCP è«–æ–‡æ¨™æº–é‡å»ºæå¤±ï¼šå­¸ç”Ÿå’Œæ•™å¸«æ¨¡å‹åŒå±¤ feature map çš„ MSE
        """
        # å­¸ç”Ÿæ¨¡å‹ feature map 
        student_feature_maps = self.get_feature_map(images)
        
        # æ•™å¸«æ¨¡å‹ feature mapï¼ˆä¸éœ€æ¢¯åº¦ï¼‰
        with torch.no_grad():
            teacher_feature_maps = self.teacher_model.get_feature_map(images)
        
        # è¨ˆç®— MSE 
        criterion = nn.MSELoss()
        recon_loss = criterion(student_feature_maps, teacher_feature_maps)
        
        # ç¢ºä¿æå¤±å€¼æœ‰æ•ˆ
        if torch.isnan(recon_loss) or torch.isinf(recon_loss):
            print("âš ï¸ é‡å»ºæå¤±ç„¡æ•ˆï¼Œä½¿ç”¨å‚™ç”¨å€¼")
            recon_loss = torch.tensor(0.1, device=device)
        
        return recon_loss

    def _resize_feature_maps(self, feature_maps, target_size):
        """èª¿æ•´ç‰¹å¾µåœ–å¤§å°"""
        # æª¢æŸ¥ä¸¦ç¢ºä¿è¼¸å…¥å¼µé‡å…·æœ‰æ­£ç¢ºçš„ç¶­åº¦ [N, C, H, W]
        if feature_maps.dim() < 4:
            # å¦‚æœç¶­åº¦ä¸è¶³ï¼Œæ·»åŠ å¿…è¦çš„ç¶­åº¦
            if feature_maps.dim() == 2:
                feature_maps = feature_maps.unsqueeze(0).unsqueeze(0)
            elif feature_maps.dim() == 3:
                feature_maps = feature_maps.unsqueeze(0)
        
        # ä½¿ç”¨é›™ç·šæ€§æ’å€¼èª¿æ•´å¤§å°
        resized_maps = torch.nn.functional.interpolate(
            feature_maps,
            size=target_size,
            mode='bilinear',
            align_corners=False
        )
        
        return resized_maps

    def _backward_and_optimize(self, loss, optimizer, scheduler=None):
        """åå‘å‚³æ’­å’Œå„ªåŒ–"""
        loss.backward()
        optimizer.step()
        
        # æ›´æ–°å­¸ç¿’ç‡ï¼ˆå¦‚æœæœ‰èª¿åº¦å™¨ï¼‰
        if scheduler is not None:
            scheduler.step()

    def _update_training_stats(self, stats, loss, cls_loss, reg_loss, recon_loss, use_lcp_loss):
        """æ›´æ–°è¨“ç·´çµ±è¨ˆè³‡æ–™"""
        stats['batch_count'] += 1
        stats['total_loss'] += loss.item()
        stats['cls_loss_total'] += cls_loss.item()
        stats['reg_loss_total'] += reg_loss.item()
        
        if use_lcp_loss:
            stats['recon_loss_total'] += recon_loss.item() if isinstance(recon_loss, torch.Tensor) else recon_loss
        
        return stats

    def _update_progress_bar(self, progress_bar, batch_idx, print_freq, loss, cls_loss, reg_loss, optimizer):
        """æ›´æ–°é€²åº¦æ¢"""
        # æ›´æ–°é€²åº¦æ¢
        if batch_idx % print_freq == 0:
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'cls_loss': f'{cls_loss.item():.4f}', 
                'reg_loss': f'{reg_loss.item():.4f}',
                'lr': f'{optimizer.param_groups[0]["lr"]:.6f}'
            })
        progress_bar.update(1)

    def _finalize_training(self, progress_bar, stats, use_lcp_loss):
        """çµæŸè¨“ç·´ä¸¦è¿”å›çµæœ"""
        # é—œé–‰é€²åº¦æ¢
        progress_bar.close()
        
        # è¨ˆç®—å¹³å‡æå¤±
        batch_count = stats['batch_count']
        avg_loss = stats['total_loss'] / batch_count if batch_count > 0 else float('inf')
        
        loss_components = {
            'cls': stats['cls_loss_total'] / batch_count if batch_count > 0 else float('inf'),
            'reg': stats['reg_loss_total'] / batch_count if batch_count > 0 else float('inf')
        }
        
        if use_lcp_loss:
            loss_components['recon'] = stats['recon_loss_total'] / batch_count if batch_count > 0 else float('inf')
        
        # æ‰“å°è¨“ç·´çµ±è¨ˆ
        elapsed_time = time.time() - stats['start_time']
        print(f"\nâœ“ è¨“ç·´å®Œæˆ: {batch_count} æ‰¹æ¬¡ï¼Œå¹³å‡æå¤±: {avg_loss:.4f}ï¼Œè€—æ™‚: {elapsed_time:.2f}ç§’")
        print(f"  åˆ†é¡æå¤±: {loss_components['cls']:.4f}, å›æ­¸æå¤±: {loss_components['reg']:.4f}")
        
        if use_lcp_loss:
            print(f"  é‡å»ºæå¤±: {loss_components['recon']:.4f}")
        
        return avg_loss, loss_components
    
    def finetune(self, train_loader, auxiliary_net, prune_layers, prune_ratio=0.3,
                optimizer=None, device=None, epochs_per_layer=1, print_freq=1, max_batches=3):
        """
        é€å±¤å‰ªæ+æ¯å±¤å‰ªæå¾Œå¾®èª¿ï¼ˆLCPè«–æ–‡æµç¨‹ï¼‰
        """
        import torch

        if device is None:
            device = self.device if hasattr(self, "device") else torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if optimizer is None:
            optimizer = torch.optim.Adam(list(self.parameters()) + list(auxiliary_net.parameters()), lr=1e-3)

        self.train()
        auxiliary_net.train()

        for layer_name in prune_layers:
            print(f"\nğŸ”ª å‰ªæå±¤: {layer_name}")
            # å–ä¸€å€‹ batch ä½œç‚ºå‰ªæä¾æ“š
            images, boxes, labels, class_images = next(iter(train_loader))
            images = self._normalize_batch_images(images, device=device)
            class_images = self._normalize_batch_images(class_images, device=device, target_size=(64, 64))
            boxes = [b.to(device) for b in boxes]
            labels = [l.to(device) for l in labels]

            # å‰ªæ
            success = self.prune_channel(
                layer_name=layer_name,
                prune_ratio=prune_ratio,
                images=images,
                boxes=boxes,
                labels=labels,
                auxiliary_net=auxiliary_net
            )
            assert success, f"å‰ªæ {layer_name} å¤±æ•—"

            # å‰ªæå¾Œå¾®èª¿
            for epoch in range(epochs_per_layer):
                print(f"  å¾®èª¿ Epoch {epoch+1}/{epochs_per_layer}")
                avg_loss, loss_components = self.train_one_epoch(
                    train_loader=train_loader,
                    optimizer=optimizer,
                    auxiliary_net=auxiliary_net,
                    device=device,
                    print_freq=print_freq,
                    max_batches=max_batches
                )
                print(f"  å¾®èª¿å®Œæˆï¼Œå¹³å‡æå¤±: {avg_loss:.4f}ï¼Œæå¤±çµ„ä»¶: {loss_components}")
        self.save_checkpoint("finetune_checkpoint.pth")
        print("\nâœ… LCP finetune pipeline å®Œæˆ")
    
    def load_checkpoint(self, checkpoint_path, device=None):
        """
        å¾æª¢æŸ¥é»è¼‰å…¥å­¸ç”Ÿæ¨¡å‹ï¼ŒåŒ…æ‹¬è™•ç†å‰ªæå¾Œçš„çµæ§‹
        
        Args:
            checkpoint_path: æª¢æŸ¥é»æ–‡ä»¶è·¯å¾‘
            device: è¨­å‚™ (CPU/GPU)
            
        Returns:
            æˆåŠŸè¼‰å…¥è¿”å› Trueï¼Œå¦å‰‡ False
        """
        if device is None:
            device = self.device if hasattr(self, 'device') else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            
        try:
            # è¼‰å…¥æª¢æŸ¥é»
            print(f"ğŸ“‚ é–‹å§‹å¾ {checkpoint_path} è¼‰å…¥æª¢æŸ¥é»...")
            checkpoint = torch.load(checkpoint_path, map_location=device)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ¨¡å‹çµæ§‹è³‡è¨Š
            if 'model_structure' not in checkpoint:
                print(f"âš ï¸ æª¢æŸ¥é»ä¸­æ²’æœ‰æ¨¡å‹çµæ§‹è³‡è¨Šï¼Œå˜—è©¦ç›´æ¥è¼‰å…¥...")
                try:
                    # ç›´æ¥è¼‰å…¥ï¼Œä½†å…è¨±ä¸åŒ¹é…çš„éµå€¼
                    # åªè¼‰å…¥å­¸ç”Ÿæ¨¡å‹ç›¸é—œçš„åƒæ•¸
                    student_state_dict = {}
                    for k, v in checkpoint['model_state_dict'].items():
                        if not k.startswith('teacher_model'):
                            student_state_dict[k] = v
                    
                    result = self.load_state_dict(student_state_dict, strict=False)
                    # å¦‚æœæœ‰ç¼ºå¤±æˆ–æ„å¤–çš„éµå€¼ï¼Œè¼¸å‡ºè­¦å‘Š
                    if result.missing_keys or result.unexpected_keys:
                        print(f"âš ï¸ è¼‰å…¥æ™‚ç™¼ç¾åŒ¹é…å•é¡Œ:")
                        print(f"   ç¼ºå¤±éµå€¼: {len(result.missing_keys)} å€‹")
                        print(f"   å¤šé¤˜éµå€¼: {len(result.unexpected_keys)} å€‹")
                    print(f"âš ï¸ æ¨¡å‹å·²è¼‰å…¥ä½†å¯èƒ½å­˜åœ¨åƒæ•¸ä¸åŒ¹é…å•é¡Œ")
                    return True
                except Exception as e:
                    print(f"âŒ ç›´æ¥è¼‰å…¥å¤±æ•—: {e}")
                    return False
                
            # æ ¹æ“šæª¢æŸ¥é»ä¸­çš„çµæ§‹é‡æ§‹æ¨¡å‹
            print("ğŸ”„ æ ¹æ“šä¿å­˜çš„çµæ§‹é‡å»ºæ¨¡å‹...")
            success = self._reconstruct_model_from_structure(checkpoint['model_structure'])
            if not success:
                print("âŒ æ¨¡å‹é‡æ§‹å¤±æ•—")
                return False
            
            # ç¾åœ¨æ¨¡å‹çµæ§‹æ‡‰è©²åŒ¹é…ï¼Œå¯ä»¥è¼‰å…¥æ¬Šé‡
            print("â³ è¼‰å…¥æ¬Šé‡...")
            
            # ç¢ºä¿åªè¼‰å…¥å­¸ç”Ÿæ¨¡å‹çš„åƒæ•¸
            student_state_dict = {}
            for k, v in checkpoint['model_state_dict'].items():
                # éæ¿¾æ‰æ•™å¸«æ¨¡å‹çš„åƒæ•¸
                if not k.startswith('teacher_model'):
                    student_state_dict[k] = v
                    
            # è¼‰å…¥éæ¿¾å¾Œçš„ç‹€æ…‹å­—å…¸
            self.load_state_dict(student_state_dict, strict=False)
            
            # å¦‚æœå­˜åœ¨æ•™å¸«æ¨¡å‹ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–
            if hasattr(self, 'teacher_model'):
                print("ğŸ”„ æ­£åœ¨é‡æ–°åˆå§‹åŒ–æ•™å¸«æ¨¡å‹...")
                # ä½¿ç”¨ç•¶å‰æ¨¡å‹ï¼ˆå­¸ç”Ÿæ¨¡å‹ï¼‰çš„ç‹€æ…‹å‰µå»ºæ–°çš„æ•™å¸«æ¨¡å‹
                self.teacher_model = copy.deepcopy(self)
                self.teacher_model.eval()
                for p in self.teacher_model.parameters():
                    p.requires_grad = False
            
            # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥è¼‰å…¥è¼”åŠ©ç¶²çµ¡
            if hasattr(self, 'auxiliary_net') and 'auxiliary_net_state_dict' in checkpoint and checkpoint['auxiliary_net_state_dict'] is not None:
                self.auxiliary_net.load_state_dict(checkpoint['auxiliary_net_state_dict'])
                print("âœ“ å·²è¼‰å…¥è¼”åŠ©ç¶²çµ¡")
                
            # è¨ˆç®—ä¸¦é¡¯ç¤ºå­¸ç”Ÿæ¨¡å‹çš„åƒæ•¸é‡
            student_params = sum(p.numel() for name, p in self.named_parameters() 
                            if not name.startswith('teacher_model'))
                
            print(f"âœ… æˆåŠŸå¾ {checkpoint_path} è¼‰å…¥æ¨¡å‹")
            print(f"è¼‰å…¥å¾Œå­¸ç”Ÿæ¨¡å‹åƒæ•¸é‡: {student_params:,}")
            return True
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ¨¡å‹æ™‚å‡ºéŒ¯: {e}")
            traceback.print_exc()
            return False

    def _reconstruct_model_from_structure(self, structure):
        """
        æ ¹æ“šä¿å­˜çš„çµæ§‹è³‡è¨Šé‡å»ºæ¨¡å‹
        
        Args:
            structure: æ¨¡å‹çµæ§‹å­—å…¸
        
        Returns:
            é‡å»ºæ˜¯å¦æˆåŠŸ
        """
        # é¦–å…ˆæ‰¾å‡ºæ‰€æœ‰éœ€è¦èª¿æ•´çš„å·ç©å±¤
        conv_layers_to_adjust = {}
        
        for name, config in structure.items():
            if 'type' not in config or config['type'] != 'Conv2d':
                continue
                
            # æª¢æŸ¥é€™å€‹å±¤æ˜¯å¦å­˜åœ¨æ–¼ç•¶å‰æ¨¡å‹ä¸­
            try:
                module = self._get_module_by_name(name)
                if module is not None and isinstance(module, nn.Conv2d):
                    # æª¢æŸ¥é€šé“æ•¸æ˜¯å¦ä¸åŒ
                    if module.out_channels != config['out_channels']:
                        conv_layers_to_adjust[name] = config
            except (AttributeError, IndexError):
                print(f"âš ï¸ ç„¡æ³•æ‰¾åˆ°å±¤ {name}ï¼Œè·³éé‡å»º")
                continue
        
        # è¼¸å‡ºæ‰€æœ‰éœ€è¦é‡å»ºçš„å±¤
        print(f"ğŸ“Š éœ€è¦èª¿æ•´çš„å·ç©å±¤æ•¸é‡: {len(conv_layers_to_adjust)}")
        
        # æŒ‰ç…§åç¨±æ’åºé‡å»ºå±¤ï¼Œç¢ºä¿æŒ‰æ­£ç¢ºé †åºé‡å»º
        for name in sorted(conv_layers_to_adjust.keys()):
            config = conv_layers_to_adjust[name]
            print(f"  èª¿æ•´å±¤ {name}: è¼¸å‡ºé€šé“å¾ {self._get_module_by_name(name).out_channels} åˆ° {config['out_channels']}")
            
            # ç²å–backboneç›¸å°è·¯å¾‘
            if "backbone." in name:
                layer_name = name.replace("backbone.", "")
            elif "net_feature_maps." in name:
                layer_name = name.replace("net_feature_maps.", "")
            else:
                layer_name = name
                
            # ä½¿ç”¨set_layer_out_channelsæ–¹æ³•èª¿æ•´é€šé“æ•¸
            success = self.set_layer_out_channels(layer_name, config['out_channels'])
            if not success:
                print(f"âŒ èª¿æ•´å±¤ {layer_name} å¤±æ•—")
                return False
        
        print(f"âœ… æ¨¡å‹çµæ§‹é‡å»ºå®Œæˆ")
        return True

    def _get_module_by_name(self, name):
        """é€šéåç¨±ç²å–æ¨¡çµ„"""
        parts = name.split('.')
        module = self
        
        for part in parts:
            if part.isdigit():
                module = module[int(part)]
            else:
                module = getattr(module, part)
                
        return module
    
    def save_checkpoint(self, checkpoint_path):
        """ä¿å­˜æ¨¡å‹æª¢æŸ¥é»ï¼Œç¢ºä¿èˆ‡ OS2D æ¡†æ¶å®Œå…¨ç›¸å®¹"""
        import traceback
        import logging
        import os
        
        # å‰µå»ºè‡¨æ™‚loggerï¼Œé¡ä¼¼æ–¼çˆ¶é¡ä¸­çš„logger
        temp_logger = logging.getLogger("OS2D.save_checkpoint")
        
        # å®šç¾©ä¿å­˜è·¯å¾‘
        pruned_path = checkpoint_path.replace('.pth', '_pruned.pth')  # å‰ªæç‰ˆæœ¬
        os2d_path = checkpoint_path  # OS2D å…¼å®¹ç‰ˆæœ¬
        
        # æ”¶é›†å‰ªæå¾Œçš„æ¨¡å‹çµæ§‹è³‡è¨Š
        model_structure = self._get_model_structure(exclude_teacher=True)
        
        # æº–å‚™æª¢æŸ¥é»å­—å…¸ - åªåŒ…å«å­¸ç”Ÿæ¨¡å‹çš„åƒæ•¸
        student_state_dict = {
            k: v for k, v in self.state_dict().items() 
            if not k.startswith('teacher_model')
        }
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ optimizer å±¬æ€§
        optimizer_state = None
        if hasattr(self, 'optimizer'):
            optimizer_state = self.optimizer.state_dict()
        
        # ä½¿ç”¨èˆ‡åŸå§‹ OS2D å®Œå…¨ç›¸åŒçš„æª¢æŸ¥é»çµæ§‹
        os2d_checkpoint = {
            'net': student_state_dict,  # OS2D init_model_from_file é¦–å…ˆæŸ¥æ‰¾çš„éµ
            'optimizer': optimizer_state,
            'scheduler': None,
            'iteration': 0,
            'epoch': self.epoch if hasattr(self, 'epoch') else 0,
            'loss': 0.0,  # OS2D _load_network æ–¹æ³•å¯èƒ½éœ€è¦é€™å€‹
            'config': {
                'model': {
                    'backbone': {'arch': "resnet50"},
                    'merge_branch_parameters': True,
                    'use_group_norm': False,
                    'use_inverse_geom_model': False,
                    'use_simplified_affine_model': True
                },
            },
            'best_score': 0.0,
            
            # ä¿ç•™æˆ‘å€‘è‡ªå·±çš„é¡å¤–è³‡è¨Š
            'model_state_dict': student_state_dict,
            'optimizer_state_dict': optimizer_state,
            'auxiliary_net_state_dict': self.auxiliary_net.state_dict() if hasattr(self, 'auxiliary_net') else None,
            'model_structure': model_structure,
            'backbone_arch': self.backbone_arch if hasattr(self, 'backbone_arch') else "resnet50"
        }
        
        # 1. ä¿å­˜å‰ªæç‰ˆæœ¬ (çµ¦æˆ‘å€‘è‡ªå·±ç”¨)
        pruned_checkpoint = {
            'model_state_dict': student_state_dict,
            'model_structure': model_structure,
            'optimizer_state_dict': optimizer_state,
            'auxiliary_net_state_dict': self.auxiliary_net.state_dict() if hasattr(self, 'auxiliary_net') else None,
            'backbone_arch': self.backbone_arch if hasattr(self, 'backbone_arch') else "resnet50",
            'epoch': self.epoch if hasattr(self, 'epoch') else 0
        }
        
        try:
            # ä¿å­˜å…©å€‹æª¢æŸ¥é»
            torch.save(os2d_checkpoint, os2d_path)
            torch.save(pruned_checkpoint, pruned_path)
            
            print(f"\nâœ… æª¢æŸ¥é»å·²ä¿å­˜:")
            print(f"  - OS2D ç›¸å®¹æª¢æŸ¥é»: {os2d_path}")
            print(f"  - å‰ªææ¨¡å‹æª¢æŸ¥é»: {pruned_path}")
            
            # è¨ˆç®—åƒæ•¸é‡
            student_params = sum(p.numel() for name, p in self.named_parameters() 
                            if not name.startswith('teacher_model'))
            print(f"  - å­¸ç”Ÿæ¨¡å‹åƒæ•¸é‡: {student_params:,}")
            
            # æ¸¬è©¦èƒ½å¦åŠ è¼‰ (åš´æ ¼çš„æ¸¬è©¦)
            print("\nğŸ§ª æ¸¬è©¦æª¢æŸ¥é»ç›¸å®¹æ€§...")
            os2d_compat_result = False
            pruned_compat_result = False
            
            print("\n1. æ¸¬è©¦OS2Dæ¡†æ¶ç›¸å®¹æ€§ (ä½¿ç”¨çˆ¶é¡ Os2dModel):")
            try:
                from os2d.modeling.model import Os2dModel
                # å‰µå»ºåŸå§‹ OS2D æ¨¡å‹å¯¦ä¾‹
                os2d_model = Os2dModel(logger=temp_logger, is_cuda=self.is_cuda)
                # ä½¿ç”¨çˆ¶é¡çš„ init_model_from_file æ–¹æ³•æ¸¬è©¦
                optimizer_result = os2d_model.init_model_from_file(os2d_path)
                print(f"âœ… OS2D æ¡†æ¶ç›¸å®¹æ€§æ¸¬è©¦: âœ“ é€šé")
                os2d_compat_result = True
            except Exception as e:
                print(f"âŒ OS2D æ¡†æ¶ç›¸å®¹æ€§æ¸¬è©¦: âœ— å¤±æ•—")
                print(f"   éŒ¯èª¤åŸå› : {e}")
            
            print("\n2. æ¸¬è©¦å‰ªææ¨¡å‹è¼‰å…¥ (ä½¿ç”¨ Os2dModelInPrune):")
            try:
                # è‡ªå·±çš„æ¨¡å‹è¼‰å…¥æ¸¬è©¦
                pruned_model = type(self)(pretrained_path=None, is_cuda=self.is_cuda)
                success = pruned_model.load_checkpoint(pruned_path)
                if success:
                    print(f"âœ… å‰ªææ¨¡å‹è¼‰å…¥æ¸¬è©¦: âœ“ é€šé")
                    pruned_compat_result = True
                else:
                    print(f"âŒ å‰ªææ¨¡å‹è¼‰å…¥æ¸¬è©¦: âœ— å¤±æ•—")
            except Exception as e:
                print(f"âŒ å‰ªææ¨¡å‹è¼‰å…¥æ¸¬è©¦: âœ— å¤±æ•—")
                print(f"   éŒ¯èª¤åŸå› : {e}")
            
            print("\n3. æ¸¬è©¦ä½¿ç”¨æœ¬é¡çš„ init_model_from_file:")
            try:
                temp_model2 = type(self)(pretrained_path=None, is_cuda=self.is_cuda)
                optimizer_result = temp_model2.init_model_from_file(os2d_path)
                print(f"âœ… init_model_from_file æ¸¬è©¦: âœ“ é€šé")
                if optimizer_result is not None:
                    print("   å„ªåŒ–å™¨ç‹€æ…‹ä¹ŸæˆåŠŸè¼‰å…¥")
            except Exception as e:
                print(f"âŒ init_model_from_file æ¸¬è©¦: âœ— å¤±æ•—")
                print(f"   éŒ¯èª¤åŸå› : {e}")
                    
            print("\n===== ç›¸å®¹æ€§æ¸¬è©¦æ‘˜è¦ =====")
            print(f"OS2D æ¡†æ¶ç›¸å®¹æ€§: {'âœ… é€šé' if os2d_compat_result else 'âŒ å¤±æ•—'}")
            print(f"å‰ªææ¨¡å‹è¼‰å…¥æ¸¬è©¦: {'âœ… é€šé' if pruned_compat_result else 'âŒ å¤±æ•—'}")
            print("==========================")
            
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æª¢æŸ¥é»æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            traceback.print_exc()
            return False

    def _get_model_structure(self, exclude_teacher=True):
        """ç²å–è©³ç´°çš„æ¨¡å‹çµæ§‹è³‡è¨Šï¼ˆå¯é¸æ’é™¤æ•™å¸«æ¨¡å‹ï¼‰"""
        structure = {}
        
        # è¨˜éŒ„æ‰€æœ‰å±¤çš„çµæ§‹è³‡è¨Š
        for name, module in self.named_modules():
            # å¦‚æœè¨­å®šæ’é™¤æ•™å¸«æ¨¡å‹ï¼Œå‰‡è·³éæ•™å¸«æ¨¡å‹ç›¸é—œçš„å±¤
            if exclude_teacher and name.startswith('teacher_model'):
                continue
                
            if isinstance(module, nn.Conv2d):
                structure[name] = {
                    'type': 'Conv2d',
                    'in_channels': module.in_channels,
                    'out_channels': module.out_channels,
                    'kernel_size': module.kernel_size,
                    'stride': module.stride,
                    'padding': module.padding,
                    'dilation': module.dilation,
                    'groups': module.groups,
                    'bias': module.bias is not None
                }
            elif isinstance(module, nn.BatchNorm2d):
                structure[name] = {
                    'type': 'BatchNorm2d',
                    'num_features': module.num_features,
                    'eps': module.eps,
                    'momentum': module.momentum,
                    'affine': module.affine,
                    'track_running_stats': module.track_running_stats
                }
            elif isinstance(module, nn.Linear):
                structure[name] = {
                    'type': 'Linear',
                    'in_features': module.in_features,
                    'out_features': module.out_features,
                    'bias': module.bias is not None
                }
        
        return structure    