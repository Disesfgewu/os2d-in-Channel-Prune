import torch
import torch.nn as nn
import os
import time
import datetime
import logging
import traceback
import copy
import numpy as np
from tqdm import tqdm
from collections import defaultdict
from os2d.modeling.model import Os2dModel
from os2d.modeling.feature_extractor import build_feature_extractor
from src.lcp_channel_selector import OS2DChannelSelector
from src.gIoU_loss import GIoULoss

class Os2dModelInPrune(Os2dModel):
    """
    æ“´å±• OS2D æ¨¡å‹ä»¥æ”¯æŒé€šé“å‰ªæåŠŸèƒ½
    """
    def __init__(self, logger=None, is_cuda=False, backbone_arch="resnet50", 
                 use_group_norm=False, img_normalization=None, 
                 pretrained_path=None, pruned_checkpoint=None, **kwargs):
        # å¦‚æœæ²’æœ‰æä¾› loggerï¼Œå‰µå»ºä¸€å€‹
        if logger is None:
            logger = logging.getLogger("OS2D")
        
        # èª¿ç”¨çˆ¶é¡åˆå§‹åŒ–
        self.device = torch.device('cuda' if is_cuda else 'cpu')
        
        super(Os2dModelInPrune, self).__init__(
            logger=logger,
            is_cuda=is_cuda,
            backbone_arch=backbone_arch,
            use_group_norm=use_group_norm,
            img_normalization=img_normalization,
            **kwargs
        )
        
        self.backbone = self.net_feature_maps
        # å°‡æ¨¡å‹ç§»è‡³æŒ‡å®šè¨­å‚™
        if is_cuda:
            self.cuda()
            self.backbone = self.backbone.cuda()
            # ç¢ºä¿æ‰€æœ‰å­æ¨¡å¡Šéƒ½åœ¨ CUDA ä¸Š
            for module in self.modules():
                if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
                    module.cuda()
        # è¼‰å…¥é è¨“ç·´æ¬Šé‡
        if pretrained_path:
            self.init_model_from_file(pretrained_path)
        self.device = torch.device('cuda' if is_cuda else 'cpu')
        self.original_device = self.device  # ä¿å­˜åŸå§‹è¨­å‚™
        
        self.teacher_model = copy.deepcopy(self)
        self.teacher_model.eval()
        for p in self.teacher_model.parameters():
            p.requires_grad = False

        if pruned_checkpoint:
           self.load_checkpoint(pruned_checkpoint)
        if is_cuda:
            try:
                self.cuda()
                self.backbone = self.backbone.cuda()
                # ç¢ºä¿æ‰€æœ‰å­æ¨¡å¡Šéƒ½åœ¨ CUDA ä¸Š
                for module in self.modules():
                    if isinstance(module, (nn.Conv2d, nn.BatchNorm2d)):
                        try:
                            module.cuda()
                        except RuntimeError as e:
                            print(f"âš ï¸ æ¨¡å¡Š {type(module)} ç„¡æ³•ç§»è‡³ CUDA: {e}")
                            # å¦‚æœå¤±æ•—ï¼Œå…ˆç”¨ CPU
                            module.cpu()
            except RuntimeError as e:
                print(f"âš ï¸ CUDA åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨ CPU ä½œç‚ºå‚™é¸: {e}")
                self.device = torch.device('cpu')
                self.cpu()
                
    
    def _safe_forward(self, x, device=None, class_images=None):
        """å®‰å…¨çš„å‰å‘å‚³æ’­ï¼Œå¦‚æœ GPU åŸ·è¡Œå¤±æ•—æœƒfallbackåˆ°CPU"""
        if x.dim() == 3:  # å¦‚æœæ˜¯ [C, H, W]
            x = x.unsqueeze(0)  # è½‰æ›ç‚º [1, C, H, W]
        try:
            if class_images is not None:
                return self(x, class_images=class_images)
            return self(x)
        except RuntimeError as e:
            if "Input type" in str(e) and device and device.type == 'cuda':
                print("âš ï¸ GPU åŸ·è¡Œå¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ CPU...")
                # æš«æ™‚å°‡æ¨¡å‹å’Œè¼¸å…¥ç§»åˆ° CPU
                original_device = next(self.parameters()).device
                cpu_model = self.cpu()
                cpu_x = x.cpu()
                cpu_class_images = class_images.cpu() if class_images is not None else None
                
                try:
                    # åœ¨ CPU ä¸ŠåŸ·è¡Œ
                    with torch.no_grad():
                        if cpu_class_images is not None:
                            output = cpu_model(cpu_x, class_images=cpu_class_images)
                        else:
                            output = cpu_model(cpu_x)
                    
                    # åŸ·è¡ŒæˆåŠŸå¾Œç§»å› GPU
                    self.to(original_device)
                    output = output.to(original_device)
                    print("âœ“ CPU åŸ·è¡ŒæˆåŠŸï¼Œå·²ç§»å› GPU")
                    return output
                
                except Exception as cpu_e:
                    print(f"âŒ CPU åŸ·è¡Œä¹Ÿå¤±æ•—: {cpu_e}")
                    # ç¢ºä¿æ¨¡å‹ç§»å›åŸå§‹è¨­å‚™
                    self.to(original_device)
                    raise cpu_e
            else:
                raise e  # å¦‚æœä¸æ˜¯è¨­å‚™å•é¡Œï¼Œå‰‡é‡æ–°å¼•ç™¼éŒ¯èª¤
    def set_layer_out_channels(self, layer_name, new_out_channels):
        """è¨­ç½®æŒ‡å®šå±¤çš„è¼¸å‡ºé€šé“æ•¸"""
        # å°‹æ‰¾ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name:
                target_layer = module
                break
                
        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False
            
        if not isinstance(target_layer, nn.Conv2d):
            print(f"âŒ å±¤ {layer_name} ä¸æ˜¯å·ç©å±¤")
            return False
            
        # å‰µå»ºæ–°çš„å·ç©å±¤
        new_conv = nn.Conv2d(
            in_channels=target_layer.in_channels,
            out_channels=new_out_channels,
            kernel_size=target_layer.kernel_size,
            stride=target_layer.stride,
            padding=target_layer.padding,
            dilation=target_layer.dilation,
            groups=target_layer.groups,
            bias=target_layer.bias is not None
        ).to(target_layer.weight.device)
        
        # å°‡åŸå§‹æ¬Šé‡è¤‡è£½åˆ°æ–°å±¤
        with torch.no_grad():
            # åªè¤‡è£½éœ€è¦çš„é€šé“
            min_channels = min(new_out_channels, target_layer.weight.size(0))
            new_conv.weight.data[:min_channels] = target_layer.weight.data[:min_channels]
            if target_layer.bias is not None:
                new_conv.bias.data[:min_channels] = target_layer.bias.data[:min_channels]
        
        # è¨­ç½®æ–°çš„å±¤
        parts = layer_name.split('.')
        parent = self.backbone
        for i in range(len(parts)-1):
            if parts[i].isdigit():
                parent = parent[int(parts[i])]
            else:
                parent = getattr(parent, parts[i])
        setattr(parent, parts[-1], new_conv)
        
        print(f"âœ“ {layer_name} out_channels å¼·åˆ¶è¨­ç‚º {new_out_channels}")
        
        # åŒæ­¥æ›´æ–° BatchNorm
        if parts[-1].startswith('conv'):
            bn_name = parts[-1].replace('conv', 'bn')
            if hasattr(parent, bn_name):
                old_bn = getattr(parent, bn_name)
                new_bn = nn.BatchNorm2d(new_out_channels).to(old_bn.weight.device)
                min_channels = min(new_out_channels, old_bn.num_features)
                new_bn.weight.data[:min_channels] = old_bn.weight.data[:min_channels]
                new_bn.bias.data[:min_channels] = old_bn.bias.data[:min_channels]
                new_bn.running_mean[:min_channels] = old_bn.running_mean[:min_channels]
                new_bn.running_var[:min_channels] = old_bn.running_var[:min_channels]
                setattr(parent, bn_name, new_bn)
                print(f"âœ“ {'.'.join(parts[:-1])}.{bn_name} é€šé“æ•¸åŒæ­¥è¨­ç‚º {new_out_channels}")
        
        # æ›´æ–°ä¸‹ä¸€å±¤çš„è¼¸å…¥é€šé“
        if parts[-1].startswith('conv'):
            conv_idx = int(parts[-1][-1])
            if conv_idx < 3:  # åªè™•ç† conv1/conv2 åˆ°ä¸‹ä¸€å±¤
                next_conv_name = f"conv{conv_idx+1}"
                if hasattr(parent, next_conv_name):
                    old_next_conv = getattr(parent, next_conv_name)
                    new_next_conv = nn.Conv2d(
                        new_out_channels,
                        old_next_conv.out_channels,
                        old_next_conv.kernel_size,
                        old_next_conv.stride,
                        old_next_conv.padding,
                        old_next_conv.dilation,
                        old_next_conv.groups,
                        bias=old_next_conv.bias is not None
                    ).to(old_next_conv.weight.device)
                    min_channels = min(new_out_channels, old_next_conv.weight.size(1))
                    new_next_conv.weight.data[:, :min_channels] = old_next_conv.weight.data[:, :min_channels]
                    if old_next_conv.bias is not None:
                        new_next_conv.bias.data = old_next_conv.bias.data.clone()
                    setattr(parent, next_conv_name, new_next_conv)
                    print(f"âœ“ {'.'.join(parts[:-1])}.{next_conv_name} in_channels åŒæ­¥è¨­ç‚º {new_out_channels}")
        
        return True
    
    def _should_skip_pruning(self, layer_name):
        """ç¢ºå®šæ˜¯å¦æ‡‰è·³éè©²å±¤çš„å‰ªæä»¥ç¶­æŒ OS2D æ¶æ§‹"""
        import re
        
        # è§£æå±¤åç¨±
        m = re.match(r'(layer\d+)\.(\d+)\.(conv\d+)', layer_name)
        if not m:
            print(layer_name + " ä¸ç¬¦åˆé æœŸæ ¼å¼ï¼Œç„¡æ³•è§£æ")
            return False
        
        layer_prefix, block_idx_str, conv_name = m.groups()
        block_idx = int(block_idx_str)
        
        # æª¢æŸ¥è©²å±¤çš„ block
        layer = getattr(self.backbone, layer_prefix)
        block = layer[block_idx]
        
        # å¦‚æœæ˜¯ layer4ï¼Œæ›´è¬¹æ…ï¼Œå› ç‚ºå®ƒæ˜¯ OS2D çš„æœ€çµ‚è¼¸å‡º
        if layer_prefix == 'layer4':
            return True  # ç‚ºäº† OS2D å…¼å®¹æ€§è·³éå‰ªæ layer4
        
        # å¼·åˆ¶è·³éæ‰€æœ‰ conv3 å±¤
        if conv_name == 'conv3':
            print(f"âš ï¸ è·³éå‰ªæ {layer_name} (åŸå› : conv3å±¤)")
            return True
        
        # å¦‚æœæ˜¯å¸¶æœ‰ downsample çš„ block ä¸­çš„ conv1ï¼Œä½¿ç”¨å°ˆç”¨çš„è™•ç†æ–¹å¼
        if conv_name == 'conv1' and hasattr(block, 'downsample') and block.downsample is not None:
            print(f"â„¹ï¸ ç™¼ç¾å¸¶æœ‰ downsample çš„ conv1: {layer_name}ï¼Œå°‡ä½¿ç”¨å°ˆç”¨è™•ç†æ–¹å¼")
            return False # 
        
        # å¦‚æœæ˜¯æœ€å¾Œä¸€å€‹ blockï¼Œè·³éæ‰€æœ‰å‰ªæä»¥ä¿è­·è·¨å±¤é€£æ¥
        if block_idx == len(layer) - 1:
            return True
        
        return False
    
    def _handle_residual_connection(self, layer_name, keep_indices):
        """è™•ç†æ®˜å·®é€£æ¥"""
        print(f"\nğŸ” é–‹å§‹è™•ç†æ®˜å·®é€£æ¥: {layer_name}")
        
        # è§£æå±¤åç¨±
        parts = layer_name.split('.')
        if len(parts) < 3:
            print(f"âš ï¸ ç„¡æ•ˆçš„å±¤åç¨±æ ¼å¼: {layer_name}")
            return
        parts = layer_name.split('.')
        if parts[-1] == 'conv1':
            print(f"â„¹ï¸ conv1 å‰ªæä¸å½±éŸ¿ downsample è¼¸å‡ºé€šé“")
            return True
        layer_str, block_idx, conv_type = parts[0], int(parts[1]), parts[2]
        
        # ç²å–ç•¶å‰ block
        layer = getattr(self.backbone, layer_str)
        current_block = layer[block_idx]
        
        if conv_type == 'conv1':
            # æ›´æ–° conv2 çš„è¼¸å…¥é€šé“
            next_conv_name = f"{layer_str}.{block_idx}.conv2"
            next_conv = None
            for name, module in self.backbone.named_modules():
                if name == next_conv_name and isinstance(module, nn.Conv2d):
                    next_conv = module
                    break
            
            if next_conv is not None:
                # æª¢æŸ¥ç´¢å¼•ç¯„åœ
                keep_indices = keep_indices[keep_indices < next_conv.weight.size(1)]
                if len(keep_indices) == 0:
                    print(f"âš ï¸ æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºç¯„åœï¼Œè·³éæ›´æ–° {next_conv_name}")
                    return
                # æ›´æ–° conv2 çš„è¼¸å…¥é€šé“
                new_conv = nn.Conv2d(
                    len(keep_indices),
                    next_conv.out_channels,
                    next_conv.kernel_size,
                    next_conv.stride,
                    next_conv.padding,
                    next_conv.dilation,
                    next_conv.groups,
                    bias=next_conv.bias is not None
                ).to(next_conv.weight.device)
                
                # æ›´æ–°æ¬Šé‡ï¼Œç¢ºä¿ç´¢å¼•åœ¨æœ‰æ•ˆç¯„åœå…§
                try:
                    # æ›´æ–°æ¬Šé‡æ™‚ç¢ºä¿ç¶­åº¦åŒ¹é…
                    if next_conv.weight.size(1) != len(keep_indices):
                        # å¦‚æœè¼¸å…¥é€šé“æ•¸ä¸åŒ¹é…ï¼Œéœ€è¦èª¿æ•´æ¬Šé‡
                        old_weight = next_conv.weight.data
                        new_weight = torch.zeros(
                            old_weight.size(0),
                            len(keep_indices),
                            old_weight.size(2),
                            old_weight.size(3),
                            device=old_weight.device
                        )
                        # åªè¤‡è£½æœ‰æ•ˆçš„é€šé“
                        valid_indices = keep_indices[keep_indices < old_weight.size(1)]
                        new_weight[:, :len(valid_indices)] = old_weight[:, valid_indices]
                        new_conv.weight.data = new_weight
                    else:
                        new_conv.weight.data = next_conv.weight.data[:, keep_indices].clone()
                    
                    if next_conv.bias is not None:
                        new_conv.bias.data = next_conv.bias.data.clone()
                except IndexError as e:
                    print(f"âš ï¸ æ›´æ–°æ¬Šé‡æ™‚ç™¼ç”Ÿç´¢å¼•éŒ¯èª¤: {e}")
                    print(f"next_conv.weight.shape: {next_conv.weight.shape}")
                    print(f"keep_indices max: {keep_indices.max()}")
                    print(f"keep_indices min: {keep_indices.min()}")
                    print(f"keep_indices shape: {keep_indices.shape}")
                    return
                
                # æ›¿æ› conv2
                parts2 = next_conv_name.split('.')
                parent = self.backbone
                for i in range(len(parts2) - 1):
                    if parts2[i].isdigit():
                        parent = parent[int(parts2[i])]
                    else:
                        parent = getattr(parent, parts2[i])
                
                setattr(parent, parts2[-1], new_conv)
                print(f"âœ“ æ›´æ–° conv2 è¼¸å…¥é€šé“: {next_conv_name} (in_channels: {new_conv.in_channels})")
                
                # åŒæ­¥æ›´æ–° BatchNorm
                bn_name = next_conv_name.replace('conv', 'bn')
                if hasattr(parent, bn_name.split('.')[-1]):
                    old_bn = getattr(parent, bn_name.split('.')[-1])
                    new_bn = nn.BatchNorm2d(new_conv.out_channels).to(old_bn.weight.device)
                    new_bn.weight.data = old_bn.weight.data.clone()
                    new_bn.bias.data = old_bn.bias.data.clone()
                    new_bn.running_mean = old_bn.running_mean.clone()
                    new_bn.running_var = old_bn.running_var.clone()
                    setattr(parent, bn_name.split('.')[-1], new_bn)
                    print(f"âœ“ æ›´æ–° BatchNorm: {bn_name}")
        
        elif conv_type == 'conv2':
            # æ›´æ–° conv3 çš„è¼¸å…¥é€šé“
            next_conv_name = f"{layer_str}.{block_idx}.conv3"
            next_conv = None
            for name, module in self.backbone.named_modules():
                if name == next_conv_name and isinstance(module, nn.Conv2d):
                    next_conv = module
                    break
            
            if next_conv is not None:
                # æª¢æŸ¥ç´¢å¼•ç¯„åœ
                keep_indices = keep_indices[keep_indices < next_conv.weight.size(1)]
                if len(keep_indices) == 0:
                    print(f"âš ï¸ æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºç¯„åœï¼Œè·³éæ›´æ–° {next_conv_name}")
                    return
                    
                new_conv = nn.Conv2d(
                    len(keep_indices),
                    next_conv.out_channels,
                    next_conv.kernel_size,
                    next_conv.stride,
                    next_conv.padding,
                    next_conv.dilation,
                    next_conv.groups,
                    bias=next_conv.bias is not None
                ).to(next_conv.weight.device)
                
                try:
                    new_conv.weight.data = next_conv.weight.data[:, keep_indices, :, :].clone()
                    if next_conv.bias is not None:
                        new_conv.bias.data = next_conv.bias.data.clone()
                except IndexError as e:
                    print(f"âš ï¸ æ›´æ–°æ¬Šé‡æ™‚ç™¼ç”Ÿç´¢å¼•éŒ¯èª¤: {e}")
                    print(f"next_conv.weight.shape: {next_conv.weight.shape}")
                    print(f"keep_indices max: {keep_indices.max()}")
                    print(f"keep_indices min: {keep_indices.min()}")
                    print(f"keep_indices shape: {keep_indices.shape}")
                    return
                
                parts2 = next_conv_name.split('.')
                parent = self.backbone
                for i in range(len(parts2) - 1):
                    if parts2[i].isdigit():
                        parent = parent[int(parts2[i])]
                    else:
                        parent = getattr(parent, parts2[i])
                
                setattr(parent, parts2[-1], new_conv)
                print(f"âœ“ æ›´æ–° conv3 è¼¸å…¥é€šé“: {next_conv_name}")
    
    def _prune_conv_layer(self, layer_name, keep_indices):
        """
        å‰ªææŒ‡å®šçš„å·ç©å±¤

        Args:
            layer_name: è¦å‰ªæçš„å±¤åç¨±
            keep_indices: è¦ä¿ç•™çš„é€šé“ç´¢å¼•
        """
        # ç²å–ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name and isinstance(module, nn.Conv2d):
                target_layer = module
                break

        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False

        # è¨­ç½®æ–°çš„è¼¸å‡ºé€šé“æ•¸
        self.set_layer_out_channels(layer_name, len(keep_indices))

        # è™•ç†æ®˜å·®é€£æ¥
        self._handle_residual_connection(layer_name, keep_indices)

        # è™•ç† downsample é€£æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
        # parts = layer_name.split('.')
        # layer_str, block_idx, conv_type = parts[0], int(parts[1]), parts[2]
        # block = getattr(self.backbone, layer_str)[block_idx]
        # if conv_type != "conv1" and hasattr(block, "downsample") and block.downsample is not None:
        #     self._handle_downsample_connection(layer_name, keep_indices)

        return True
    
    def _reset_batchnorm_stats(self):
        """é‡ç½®æ‰€æœ‰ BatchNorm å±¤çš„çµ±è¨ˆæ•¸æ“š"""
        for m in self.backbone.modules():
            if isinstance(m, nn.BatchNorm2d):
                m.reset_running_stats()
        print("âœ“ å·²é‡ç½®æ‰€æœ‰ BatchNorm å±¤çš„çµ±è¨ˆæ•¸æ“š")
    
    def prune_channel(self, layer_name, prune_ratio=0.3, images=None, boxes=None, labels=None, auxiliary_net=None):
        """
        å°æŒ‡å®šå±¤é€²è¡Œé€šé“å‰ªæ
        
        Args:
            layer_name: è¦å‰ªæçš„å±¤åç¨±
            prune_ratio: å‰ªææ¯”ä¾‹ (0.0-1.0)
            images, boxes, labels: ç”¨æ–¼è¨ˆç®—é€šé“é‡è¦æ€§çš„æ•¸æ“š
            auxiliary_net: è¼”åŠ©ç¶²è·¯ï¼Œç”¨æ–¼è©•ä¼°é€šé“é‡è¦æ€§
        """
        # æª¢æŸ¥æ˜¯å¦æ‡‰è·³éå‰ªæ
        if self._should_skip_pruning(layer_name):
            print(f"âš ï¸ è·³éå‰ªæå±¤ {layer_name}")
            return "SKIPPED"
        
        # ç²å–ç›®æ¨™å±¤
        target_layer = None
        for name, module in self.backbone.named_modules():
            if name == layer_name and isinstance(module, nn.Conv2d):
                target_layer = module
                break
        
        if target_layer is None:
            print(f"âŒ æ‰¾ä¸åˆ°å±¤: {layer_name}")
            return False
        
        # è¨ˆç®—é€šé“é‡è¦æ€§
        if images is not None and boxes is not None and auxiliary_net is not None:
            # åˆå§‹åŒ–é€šé“é¸æ“‡å™¨
            channel_selector = OS2DChannelSelector(
                model=self, 
                auxiliary_net=auxiliary_net, 
                device=self.device
            )
            
            # è¨ˆç®—é€šé“é‡è¦æ€§
            importance_scores = channel_selector.compute_importance(layer_name, images, boxes, boxes, labels)
            
            # é¸æ“‡è¦ä¿ç•™çš„é€šé“
            keep_indices = channel_selector.select_channels(layer_name, importance_scores, prune_ratio)
            
            if keep_indices is None:
                print(f"âŒ ç„¡æ³•é¸æ“‡ {layer_name} çš„ä¿ç•™é€šé“")
                return False
        else:
            # å¦‚æœæ²’æœ‰æä¾›æ•¸æ“šï¼Œå‰‡éš¨æ©Ÿé¸æ“‡é€šé“
            num_channels = target_layer.out_channels
            num_keep = int(num_channels * (1 - prune_ratio))
            keep_indices = torch.randperm(num_channels)[:num_keep]
        
        # åŸ·è¡Œå‰ªæ
        success = self._prune_conv_layer(layer_name, keep_indices)
        
        return success
    
    def prune_model(self, prune_ratio=0.3, images=None, boxes=None, labels=None, auxiliary_net=None, prunable_layers=None):
        """
        å‰ªææ•´å€‹æ¨¡å‹ï¼Œæ ¹æ“š LCP å’Œ DCP è«–æ–‡çš„æ–¹æ³•
        
        Args:
            prune_ratio: å‰ªææ¯”ä¾‹ (0.0-1.0)
            images, boxes, labels: ç”¨æ–¼è¨ˆç®—é€šé“é‡è¦æ€§çš„æ•¸æ“š
            auxiliary_net: è¼”åŠ©ç¶²è·¯ï¼Œç”¨æ–¼è©•ä¼°é€šé“é‡è¦æ€§
            prunable_layers: è¦å‰ªæçš„å±¤åˆ—è¡¨ï¼Œå¦‚æœç‚º None å‰‡è‡ªå‹•é¸æ“‡
        """
        if prunable_layers is None:
            # ä¸»è¦é‡å° OS2D ä½¿ç”¨çš„ layer2 å’Œ layer3 é€²è¡Œå‰ªæ
            prunable_layers = []
            for layer_name in ['layer2', 'layer3']:
                layer = getattr(self.backbone, layer_name)
                for block_idx in range(len(layer)):
                    # å°æ–¼æ¯å€‹æ®˜å·®å¡Š
                    for conv_idx in [1, 2]:  # åªå‰ªæ conv1 å’Œ conv2 ä»¥ç¶­æŒæ¶æ§‹
                        conv_name = f"{layer_name}.{block_idx}.conv{conv_idx}"
                        prunable_layers.append(conv_name)
        
        print(f"ğŸ” é–‹å§‹æ¨¡å‹å‰ªæ (LCP + DCP)ï¼Œå‰ªææ¯”ä¾‹: {prune_ratio}...")
        print(f"ğŸ“‹ å¯å‰ªæå±¤: {prunable_layers}")
        
        # æŒ‰é †åºå‰ªææ¯ä¸€å±¤
        pruned_layers = []
        for layer_name in prunable_layers:
            print(f"\nğŸ”§ è™•ç†å±¤: {layer_name}")
            
            # æª¢æŸ¥æ˜¯å¦æ‡‰è·³éå‰ªæ
            if self._should_skip_pruning(layer_name):
                print(f"âš ï¸ è·³éå‰ªæå±¤ {layer_name}")
                continue
            
            # å‰ªæå±¤
            success = self.prune_channel(
                layer_name, 
                prune_ratio=prune_ratio, 
                images=images, 
                boxes=boxes, 
                labels=labels, 
                auxiliary_net=auxiliary_net
            )
            
            if success and success != "SKIPPED":
                pruned_layers.append(layer_name)
        
        # é‡ç½® BatchNorm çµ±è¨ˆæ•¸æ“š
        self._reset_batchnorm_stats()
        
        print(f"âœ… æ¨¡å‹å‰ªæå®Œæˆ! å…±å‰ªæ {len(pruned_layers)}/{len(prunable_layers)} å±¤")
        print(f"æˆåŠŸå‰ªæçš„å±¤: {pruned_layers}")
        
        return pruned_layers
    
    def visualize_model_architecture(self, output_path="model_architecture.png", input_shape=(1, 3, 224, 224)):
        """
        è¦–è¦ºåŒ–æ¨¡å‹æ¶æ§‹ä¸¦ä¿å­˜ç‚ºåœ–ç‰‡
        
        Args:
            output_path: è¼¸å‡ºåœ–ç‰‡è·¯å¾‘
            input_shape: è¼¸å…¥å¼µé‡å½¢ç‹€ï¼Œé»˜èªç‚º (1, 3, 224, 224)
        """
        try:
            import torchviz
            from graphviz import Digraph
            from tqdm import tqdm
            import os
            import time
            from datetime import datetime
            import traceback
        except ImportError:
            print("è«‹å…ˆå®‰è£å¿…è¦çš„å¥—ä»¶: pip install torchviz graphviz")
            return False
        
        # å‰µå»ºè¼¸å…¥å¼µé‡
        x = torch.randn(input_shape).to(self.device)
        
        # ç²å–è¼¸å‡º
        y = self(x)
        
        # ä½¿ç”¨ torchviz ç”Ÿæˆè¨ˆç®—åœ–
        dot = torchviz.make_dot(y, params=dict(self.named_parameters()))
        
        # è¨­ç½®åœ–çš„å±¬æ€§
        dot.attr('node', fontsize='12')
        dot.attr('graph', rankdir='TB')  # å¾ä¸Šåˆ°ä¸‹çš„ä½ˆå±€
        dot.attr('graph', size='12,12')  # åœ–çš„å¤§å°
        
        # ä¿å­˜åœ–ç‰‡
        dot.render(output_path.replace('.png', ''), format='png', cleanup=True)
        
        print(f"âœ… æ¨¡å‹æ¶æ§‹å·²ä¿å­˜è‡³: {output_path}")
        
        # è¼¸å‡ºæ¨¡å‹æ‘˜è¦
        self._print_model_summary()
        
        return True

    def get_feature_map(self, x):
        """ç²å–ç‰¹å¾µåœ–"""
        feature_maps = self.backbone(x)
        return feature_maps
        
    def forward(self, images=None, class_images=None, class_head=None, feature_maps=None, **kwargs):
        """
        æ”¯æ´ OS2D pipeline (class_head, feature_maps) åŠæ¨™æº– (images, class_images)
        """
        # OS2D pipeline: detection
        if class_head is not None and feature_maps is not None:
            # èª¿ç”¨çˆ¶é¡çš„ detection forward
            return super().forward(class_head=class_head, feature_maps=feature_maps, **kwargs)
        # æ¨™æº–è¨“ç·´/æ¨è«–
        if images is not None:
            if class_images is not None:
                return super().forward(images, class_images=class_images, **kwargs)
            else:
                return super().forward(images, **kwargs)
        raise ValueError("forward() éœ€è¦ (images) æˆ– (class_head, feature_maps)")

    
    def _print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦ä¿¡æ¯ï¼ŒåŒ…å«æ¯å±¤çš„é€šé“æ•¸"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)

        # ç²å–æ¯å±¤çš„åƒæ•¸å’Œé€šé“ä¿¡æ¯
        layer_info = {}
        for name, module in self.backbone.named_modules():
            if isinstance(module, nn.Conv2d):
                layer_info[name] = {
                    'type': 'Conv2d',
                    'params': sum(p.numel() for p in module.parameters()),
                    'in_channels': module.in_channels,
                    'out_channels': module.out_channels
                }
            elif isinstance(module, nn.BatchNorm2d):
                layer_info[name] = {
                    'type': 'BatchNorm2d',
                    'params': sum(p.numel() for p in module.parameters()),
                    'num_features': module.num_features
                }

        # æ‰“å°æ‘˜è¦
        print("\n====== æ¨¡å‹æ‘˜è¦ ======")
        print(f"ç¸½åƒæ•¸é‡: {total_params:,}")
        print(f"å¯è¨“ç·´åƒæ•¸é‡: {trainable_params:,}")
        print("\nå±¤ç´šçµæ§‹åˆ†æ:")

        # æŒ‰å±¤åæ’åºæ‰“å°
        for layer_name in sorted(layer_info.keys()):
            info = layer_info[layer_name]
            if info['type'] == 'Conv2d':
                # æª¢æŸ¥æ˜¯å¦ç‚º downsample å±¤
                is_downsample = 'downsample' in layer_name
                layer_type = 'æ®˜å·®åˆ†æ”¯' if is_downsample else 'ä¸»åˆ†æ”¯'
                print(f"\n- {layer_name} ({layer_type}):")
                print(f"  é¡å‹: {info['type']}")
                print(f"  è¼¸å…¥é€šé“: {info['in_channels']}")
                print(f"  è¼¸å‡ºé€šé“: {info['out_channels']}")
                print(f"  åƒæ•¸é‡: {info['params']:,}")
            else:  # BatchNorm2d
                print(f"\n- {layer_name}:")
                print(f"  é¡å‹: {info['type']}")
                print(f"  ç‰¹å¾µæ•¸: {info['num_features']}")
                print(f"  åƒæ•¸é‡: {info['params']:,}")

        # æ·»åŠ å±¤ç´šé—œä¿‚åˆ†æ
        print("\n====== å±¤ç´šé€£æ¥åˆ†æ ======")
        for layer_idx in range(1, 4):  # layer1 åˆ° layer4
            layer = getattr(self.backbone, f'layer{layer_idx}')
            print(f"\n[Layer {layer_idx}]")
            for block_idx in range(len(layer)):
                block = layer[block_idx]
                print(f"\nBlock {block_idx}:")
                # æ‰“å°ä¸»åˆ†æ”¯
                if hasattr(block, 'conv1'):
                    print(f"  Conv1: {block.conv1.in_channels} -> {block.conv1.out_channels}")
                if hasattr(block, 'conv2'):
                    print(f"  Conv2: {block.conv2.in_channels} -> {block.conv2.out_channels}")
                if hasattr(block, 'conv3'):
                    print(f"  Conv3: {block.conv3.in_channels} -> {block.conv3.out_channels}")
                # æ‰“å°æ®˜å·®åˆ†æ”¯
                if hasattr(block, 'downsample') and block.downsample is not None:
                    downsample_conv = block.downsample[0]
                    print(f"  Downsample: {downsample_conv.in_channels} -> {downsample_conv.out_channels}")
                    print(f"  Downsample Type: {type(downsample_conv).__name__}")
                    
    def _normalize_batch_images(self, images, device=None, target_size=(224, 224)):
        """
        æ¨™æº–åŒ–è™•ç†åœ–åƒæ‰¹æ¬¡ï¼Œç¢ºä¿æ‰€æœ‰åœ–åƒå°ºå¯¸ä¸€è‡´ä¸¦è½‰æ›ç‚ºæ‰¹æ¬¡å¼µé‡
        
        Args:
            images: åœ–åƒåˆ—è¡¨æˆ–å–®ä¸€å¼µé‡
            device: ç›®æ¨™è¨­å‚™
            target_size: ç›®æ¨™å°ºå¯¸ (H, W)
            
        Returns:
            torch.Tensor: æ‰¹æ¬¡åœ–åƒå¼µé‡ [B, C, H, W]
        """
        if device is None:
            device = self.device
            
        # è™•ç†å–®ä¸€å¼µé‡æƒ…æ³
        if isinstance(images, torch.Tensor):
            if images.dim() == 3:  # [C, H, W]
                images = images.unsqueeze(0)  # [1, C, H, W]
            return images.to(device)
            
        # è™•ç†åœ–åƒåˆ—è¡¨
        if isinstance(images, list):
            # éæ¿¾æœ‰æ•ˆåœ–åƒ
            valid_images = []
            for img in images:
                if img is None or not isinstance(img, torch.Tensor):
                    continue
                
                # ç¢ºä¿åœ–åƒæ˜¯ 3D å¼µé‡ [C, H, W]
                if img.dim() == 3:
                    # èª¿æ•´åœ–åƒå°ºå¯¸ç‚ºæ¨™æº–å°ºå¯¸ä¸¦ç§»è‡³è¨­å‚™
                    if img.shape[1] != target_size[0] or img.shape[2] != target_size[1]:
                        img = torch.nn.functional.interpolate(
                            img.unsqueeze(0),  # æ·»åŠ æ‰¹æ¬¡ç¶­åº¦
                            size=target_size,
                            mode='bilinear',
                            align_corners=False
                        ).squeeze(0)  # ç§»é™¤æ‰¹æ¬¡ç¶­åº¦
                    
                    valid_images.append(img.to(device))
                
            if len(valid_images) == 0:
                return None
                
            # å †ç–Šç‚ºæ‰¹æ¬¡å¼µé‡
            batch_tensor = torch.stack(valid_images)
            return batch_tensor
        
        return None
    
    def compute_classification_loss(self, outputs, class_ids):
        """
        è¨ˆç®—åˆ†é¡æå¤±
        
        Args:
            outputs: æ¨¡å‹è¼¸å‡º
            class_ids: ç›®æ¨™é¡åˆ¥ ID
            
        Returns:
            torch.Tensor: åˆ†é¡æå¤±
        """
        # å¾è¼¸å‡ºä¸­ç²å–åˆ†é¡åˆ†æ•¸
        if isinstance(outputs, dict) and 'class_scores' in outputs:
            class_scores = outputs['class_scores']
        else:
            # å¦‚æœè¼¸å‡ºä¸åŒ…å«åˆ†é¡åˆ†æ•¸ï¼Œå˜—è©¦ä½¿ç”¨æ•´å€‹è¼¸å‡ºä½œç‚ºåˆ†é¡åˆ†æ•¸
            class_scores = outputs

        # å°‡ç›®æ¨™è½‰æ›ç‚ºé©ç•¶çš„æ ¼å¼
        if isinstance(class_ids, list):
            # ç¢ºä¿åˆ—è¡¨ä¸­çš„æ¯å€‹å…ƒç´ éƒ½æ˜¯å¼µé‡ï¼Œä¸¦å°‡å®ƒå€‘æ‹¼æ¥
            tensor_items = [item for item in class_ids if isinstance(item, torch.Tensor)]
            if tensor_items:
                target = torch.cat(tensor_items).long()
            else:
                # å¦‚æœåˆ—è¡¨ä¸­æ²’æœ‰å¼µé‡ï¼Œå‰µå»ºä¸€å€‹é»˜èªå¼µé‡
                target = torch.zeros(1).long()
        elif isinstance(class_ids, tuple):
            # è™•ç†å…ƒçµ„é¡å‹ï¼Œå°‡å…¶è½‰æ›ç‚ºåˆ—è¡¨ä¸¦å†æ¬¡è™•ç†
            tensor_items = [item for item in class_ids if isinstance(item, torch.Tensor)]
            if tensor_items:
                target = torch.cat(tensor_items).long()
            else:
                target = torch.zeros(1).long()
        else:
            target = class_ids.long()
        

        device = next(self.parameters()).device
        target = target.to(device)
        
        # ä½¿ç”¨äº¤å‰ç†µæå¤±
        loss_fn = torch.nn.CrossEntropyLoss()
        
        # è™•ç† class_scores å¯èƒ½æ˜¯å…ƒçµ„çš„æƒ…æ³
        if isinstance(class_scores, tuple):
            # ä½¿ç”¨ç¬¬ä¸€å€‹å…ƒç´ ï¼Œé€šå¸¸åŒ…å«åˆ†é¡åˆ†æ•¸
            class_scores = class_scores[0]
        elif not isinstance(class_scores, torch.Tensor):
            print(f"Warning: class_scores é¡å‹ç‚º {type(class_scores)}ï¼Œç„¡æ³•è¨ˆç®—åˆ†é¡æå¤±")
            return torch.tensor(0.0, device=device)
        
        # æª¢æŸ¥ä¸¦ä¿®æ­£æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…çš„å•é¡Œ
        batch_size = class_scores.size(0)
        
        # ç¢ºä¿ target è‡³å°‘æ˜¯ 1D å¼µé‡
        if target.dim() == 0:
            target = target.unsqueeze(0)
            
        if target.size(0) != batch_size:
            # print(f"Warning: ä¿®æ­£ç›®æ¨™æ‰¹æ¬¡å¤§å° ({target.size(0)}) ä¸åŒ¹é…è¼¸å‡ºæ‰¹æ¬¡å¤§å° ({batch_size})")
            if target.size(0) > batch_size:
                # å¦‚æœç›®æ¨™æ‰¹æ¬¡è¼ƒå¤§ï¼Œæˆªæ–·ä»¥åŒ¹é…è¼¸å‡ºæ‰¹æ¬¡
                target = target[:batch_size]
            else:
                # å¦‚æœç›®æ¨™æ‰¹æ¬¡è¼ƒå°ï¼Œä½¿ç”¨é‡è¤‡ä¾†æ“´å±•
                repeats = (batch_size + target.size(0) - 1) // target.size(0)
                target = target.repeat(repeats)[:batch_size]
        
        # ç¢ºä¿ class_scores å½¢ç‹€æ­£ç¢º (batch_size, num_classes)
        if class_scores.dim() > 2:
            class_scores = class_scores.view(batch_size, -1)
            
        return loss_fn(class_scores, target)
    
    def compute_box_regression_loss(self, outputs, boxes):
        """
        è¨ˆç®—é‚Šç•Œæ¡†å›æ­¸æå¤±
        
        Args:
            outputs: æ¨¡å‹è¼¸å‡º
            boxes: ç›®æ¨™é‚Šç•Œæ¡† (å¯èƒ½æ˜¯BoxListå°è±¡ã€å¼µé‡æˆ–å…¶åˆ—è¡¨)
                
        Returns:
            torch.Tensor: å›æ­¸æå¤±
        """
        # å¾è¼¸å‡ºä¸­ç²å–é æ¸¬æ¡†
        if isinstance(outputs, dict) and 'boxes' in outputs:
            pred_boxes = outputs['boxes']
        elif isinstance(outputs, tuple):
            # å¦‚æœè¼¸å‡ºæ˜¯å…ƒçµ„ï¼Œå‡è¨­ç¬¬äºŒå€‹å…ƒç´ åŒ…å«é‚Šç•Œæ¡†
            pred_boxes = outputs[1] if len(outputs) > 1 else torch.zeros(1, 4).to(self.device)
        else:
            # å¦‚æœæ²’æœ‰æ˜ç¢ºçš„é‚Šç•Œæ¡†è¼¸å‡ºï¼Œä½¿ç”¨é»˜èªå€¼
            pred_boxes = torch.zeros(1, 4).to(self.device)
            
        # å°‡ç›®æ¨™æ¡†è½‰æ›ç‚ºé©ç•¶çš„æ ¼å¼
        if isinstance(boxes, list):
            # è™•ç† BoxList å°è±¡
            valid_boxes = []
            for b in boxes:
                if b is None:
                    continue
                    
                # æª¢æŸ¥æ˜¯å¦ç‚º BoxList å°è±¡
                if hasattr(b, 'bbox') and hasattr(b, 'size'):  # BoxList é€šå¸¸æœ‰é€™äº›å±¬æ€§
                    box_tensor = b.bbox  # ç²å–åº•å±¤å¼µé‡
                    if isinstance(box_tensor, torch.Tensor) and box_tensor.numel() > 0:
                        valid_boxes.append(box_tensor)
                # æª¢æŸ¥æ˜¯å¦ç‚ºå¼µé‡
                elif isinstance(b, torch.Tensor) and b.numel() > 0:
                    valid_boxes.append(b)
                    
            if not valid_boxes:
                return torch.tensor(0.0, device=pred_boxes.device)
            
            target_boxes = torch.cat(valid_boxes)
        else:
            # è™•ç†å–®å€‹ BoxList å°è±¡
            if hasattr(boxes, 'bbox') and hasattr(boxes, 'size'):
                target_boxes = boxes.bbox
            else:
                target_boxes = boxes
            
        # ç¢ºä¿å¼µé‡åœ¨åŒä¸€è¨­å‚™ä¸Š
        target_boxes = target_boxes.to(pred_boxes.device)
        
        # å¦‚æœæ²’æœ‰æœ‰æ•ˆçš„ç›®æ¨™æ¡†ï¼Œè¿”å›é›¶æå¤±
        if not isinstance(target_boxes, torch.Tensor) or target_boxes.numel() == 0:
            return torch.tensor(0.0, device=pred_boxes.device)
            
        # ç¢ºä¿é æ¸¬æ¡†å’Œç›®æ¨™æ¡†çš„å½¢ç‹€åŒ¹é…
        if pred_boxes.size(-1) != 4:
            pred_boxes = pred_boxes.view(-1, 4)
        if target_boxes.size(-1) != 4:
            target_boxes = target_boxes.view(-1, 4)
            
        # èª¿æ•´æ‰¹æ¬¡å¤§å°ä»¥åŒ¹é…
        if pred_boxes.size(0) != target_boxes.size(0):
            # å¦‚æœé æ¸¬æ¡†æ¯”ç›®æ¨™æ¡†å¤šï¼Œå–å‰Nå€‹
            if pred_boxes.size(0) > target_boxes.size(0):
                pred_boxes = pred_boxes[:target_boxes.size(0)]
            # å¦‚æœç›®æ¨™æ¡†æ¯”é æ¸¬æ¡†å¤šï¼Œå–å‰Nå€‹
            else:
                target_boxes = target_boxes[:pred_boxes.size(0)]
                
        # ä½¿ç”¨ L1 æå¤±ï¼Œå› ç‚ºå®ƒæ›´ç©©å®š
        loss_fn = torch.nn.L1Loss()
        try:
            loss = loss_fn(pred_boxes, target_boxes)
        except RuntimeError as e:
            print(f"è­¦å‘Š: L1æå¤±è¨ˆç®—å¤±æ•— - pred_boxes: {pred_boxes.shape}, target_boxes: {target_boxes.shape}")
            return torch.tensor(0.0, device=pred_boxes.device)
            
        return loss
    

    from tqdm import tqdm

    def train_one_epoch(self, train_loader, optimizer, 
                    auxiliary_net=None, device=None, 
                    print_freq=10, scheduler=None, 
                    loss_weights=None, use_lcp_loss=True, 
                    max_batches=None):
        """
        è¨“ç·´æ¨¡å‹ä¸€å€‹ epoch
        """
        import torch
        import traceback

        self.train()
        if auxiliary_net is not None:
            auxiliary_net.train()

        if device is None:
            device = self.device
        print( "Start training...")
        print(f"Using device: {device}")
        if loss_weights is None:
            loss_weights = {'cls': 1.0, 'box_reg': 1.0, 'teacher': 0.5}

        giou_loss = GIoULoss()
        loss_history = []

        num_batches = len(train_loader)
        if max_batches is not None:
            num_batches = min(num_batches, max_batches)

        pbar = tqdm(range(num_batches), total=num_batches)

        for batch_idx in pbar:
            try:
                batch_data = train_loader.get_batch(batch_idx)
                # ä¾æ“š OS2D dataloader çš„ batch çµæ§‹
                images, class_images, loc_targets, class_targets, batch_class_ids, class_image_sizes, box_inverse_transform, batch_boxes, batch_img_size = batch_data

                # ç§»è‡³è¨­å‚™
                images = images.to(device)
                # # class_images æ˜¯ list of tensors [B_class, 3, 64, 64]
                # class_images_tensor = torch.stack(class_images).to(device) if isinstance(class_images, list) and isinstance(class_images[0], torch.Tensor) else class_images
                if auxiliary_net is not None:
                    # å– backbone è¼¸å‡º feature map channel
                    feature_maps = self.get_feature_map(images)
                    if isinstance(feature_maps, torch.Tensor):
                        current_channels = feature_maps.shape[1]
                        if auxiliary_net.get_current_channels() != current_channels:
                            auxiliary_net.update_input_channels(current_channels)
                # è™•ç† boxes å’Œ class_targets
                boxes = batch_boxes
                class_ids = batch_class_ids

                optimizer.zero_grad()
                print("æ•™å¸«æ¨¡å‹é æ¸¬...")
                # æ•™å¸«æ¨¡å‹é æ¸¬ï¼ˆå¯é¸ï¼‰
                with torch.no_grad():
                    teacher_outputs = self.teacher_model(images, class_images=class_images)
                # å­¸ç”Ÿæ¨¡å‹é æ¸¬
                print("å­¸ç”Ÿæ¨¡å‹é æ¸¬...")
                # ç›´æ¥å°‡ class_images (list of tensor) å‚³çµ¦ model
                outputs = self(images, class_images=class_images)


                # åˆ†é¡æå¤±
                cls_loss = self.compute_classification_loss(outputs, class_ids)
                # å›æ­¸æå¤±
                box_loss = self.compute_box_regression_loss(outputs, boxes)
                # æ•™å¸«-å­¸ç”Ÿæå¤±
                if isinstance(outputs, dict) and isinstance(teacher_outputs, dict):
                    teacher_loss = torch.nn.functional.mse_loss(outputs['class_scores'], teacher_outputs['class_scores']) + \
                                torch.nn.functional.mse_loss(outputs['boxes'], teacher_outputs['boxes'])
                elif isinstance(outputs, tuple) and isinstance(teacher_outputs, tuple):
                    teacher_loss = torch.nn.functional.mse_loss(outputs[0], teacher_outputs[0])
                    if len(outputs) > 1 and len(teacher_outputs) > 1:
                        teacher_loss += torch.nn.functional.mse_loss(outputs[1], teacher_outputs[1])
                else:
                    teacher_loss = torch.tensor(0.0, device=device)
                # LCP loss
                lcp_loss = 0
                if use_lcp_loss and auxiliary_net is not None:
                    feature_maps = self.get_feature_map(images)
                    if isinstance(feature_maps, torch.Tensor):
                        aux_outputs = auxiliary_net(feature_maps, boxes, gt_boxes=boxes)
                        if isinstance(aux_outputs, tuple):
                            lcp_loss = torch.nn.functional.mse_loss(aux_outputs[1], torch.cat([b for b in boxes if b.numel() > 0], dim=0))
                        else:
                            lcp_loss = torch.nn.functional.mse_loss(aux_outputs, torch.cat([b for b in boxes if b.numel() > 0], dim=0))
                    elif isinstance(feature_maps, dict):
                        last_feature = list(feature_maps.values())[-1]
                        aux_outputs = auxiliary_net(last_feature, boxes, gt_boxes=boxes)
                        if isinstance(aux_outputs, tuple):
                            lcp_loss = torch.nn.functional.mse_loss(aux_outputs[1], torch.cat([b for b in boxes if b.numel() > 0], dim=0))
                        else:
                            lcp_loss = torch.nn.functional.mse_loss(aux_outputs, torch.cat([b for b in boxes if b.numel() > 0], dim=0))
                    else:
                        last_feature = feature_maps[-1] if isinstance(feature_maps, (tuple, list)) else feature_maps
                        aux_outputs = auxiliary_net(last_feature, boxes, gt_boxes=boxes)
                        if isinstance(aux_outputs, tuple):
                            lcp_loss = torch.nn.functional.mse_loss(aux_outputs[1], torch.cat([b for b in boxes if b.numel() > 0], dim=0))
                        else:
                            lcp_loss = torch.nn.functional.mse_loss(aux_outputs, torch.cat([b for b in boxes if b.numel() > 0], dim=0))
                print("è¨ˆç®—æå¤±...")
                loss = loss_weights['cls'] * cls_loss + \
                    loss_weights['box_reg'] * box_loss + \
                    loss_weights['teacher'] * teacher_loss + \
                    0.1 * lcp_loss

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=10.0)
                if auxiliary_net is not None:
                    torch.nn.utils.clip_grad_norm_(auxiliary_net.parameters(), max_norm=10.0)
                optimizer.step()
                if scheduler is not None:
                    scheduler.step()

                loss_value = loss.item()
                loss_history.append(loss_value)
                if print_freq > 0 and (batch_idx % print_freq == 0 or batch_idx == num_batches - 1):
                    pbar.set_description(
                        f"Loss: {loss_value:.4f} (cls: {cls_loss.item():.4f}, box: {box_loss.item():.4f}, "
                        f"teacher: {teacher_loss.item():.4f}, lcp: {lcp_loss if isinstance(lcp_loss, float) else lcp_loss.item():.4f})"
                    )
            except Exception as e:
                print(f"æ‰¹æ¬¡ {batch_idx} è™•ç†å¤±æ•—: {e}")
                traceback.print_exc()
                continue

        return loss_history
    
    def finetune(self, train_loader, auxiliary_net, prune_layers, prune_ratio=0.3,
                optimizer=None, device=None, epochs_per_layer=1, print_freq=1, max_batches=3):
        pass

    
    def load_checkpoint(self, checkpoint_path, device=None):
        pass

    
    def save_checkpoint(self, checkpoint_path):
        pass

    def _eval(self, dataloader, iou_thresh=0.5, batch_size=4, cfg=None, criterion=None, print_per_class_results=False):
        """
        ä½¿ç”¨ os2d.engine.evaluate.evaluate é€²è¡Œè‡ªå‹•åŒ– mAP è©•ä¼°
        """
        pass