import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import torchvision.ops as ops
import argparse
import os
import re
import xml.etree.ElementTree as ET
import numpy as np
from PIL import Image
from tqdm import tqdm
import copy
import time
import tarfile
from torchvision.models import resnet50
import urllib.request
import shutil
# from channel_selector import *
# from auxiliary_network import *
# from check_point import *
# from context_roi_align import *
# from gIoU_loss import *
# from lcp_channel_selector import *
# from os2d_resnet import *

# å…¨å±€è®Šæ•¸ - ä½¿é¡åˆ¥å°è±¡å¯è¨ªå•é€™äº›é¡åˆ¥
VOC_CLASSES = [
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat',
    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',
    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

class VOCDataset(torch.utils.data.Dataset):
    # æ·»åŠ éœæ…‹é¡è®Šæ•¸ä»¥ä¾¿æ¸¬è©¦å¯ä»¥è¨ªå•
    CLASSES = VOC_CLASSES
    
    def __init__(self, data_path, split='train', transform=None, target_transform=None, download=True, 
                 random_seed=None, img_size=(224,224), class_mapping=None):
        # å¦‚æœéœ€è¦ä¸‹è¼‰ä¸”è·¯å¾‘ä¸å­˜åœ¨æœ‰æ•ˆæ•¸æ“šé›†ï¼Œå‰‡ä¸‹è¼‰
        if download:
            self.data_path = self._download_voc(data_path)
        else:
            self.data_path = self._resolve_data_root(data_path)
            
        self.split = split
        self.transform = transform
        self.target_transform = target_transform
        self.samples = []
        self.cache = {}
        self.img_size = img_size
        self.class_mapping = class_mapping or {i: i for i in range(len(VOC_CLASSES))}
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿ç¢ºå®šæ€§è¡Œç‚º
        if random_seed is not None:
            torch.manual_seed(random_seed)
            np.random.seed(random_seed)

        # è·¯å¾‘çµæ§‹èˆ‡åˆ†å‰²æª”æ¡ˆæª¢æŸ¥
        self._fix_nested_path_structure()
        if not self._validate_dataset_structure():
            if download:
                print(f"âš ï¸ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„VOCè³‡æ–™é›†ï¼Œå˜—è©¦ä¸‹è¼‰...")
                self.data_path = self._download_voc(data_path)
            else:
                raise FileNotFoundError(f"ç„¡æ•ˆçš„VOCè³‡æ–™é›†çµæ§‹æ–¼ {self.data_path}ï¼Œè«‹è¨­ç½® download=True è‡ªå‹•ä¸‹è¼‰")
        
        self._verify_split_files()
        self._precache_metadata()
        print(f"ğŸ“¦ è¼‰å…¥ VOC2007 {split}é›†: {len(self.samples)} å€‹æ¨£æœ¬ (å¿«å–ç‰ˆ)")

    def _download_voc(self, path):
        """ä¸‹è¼‰ä¸¦è§£å£“ Pascal VOC æ•¸æ“šé›†"""
        import tarfile
        import urllib.request
        import os
        
        # å‰µå»ºç›®æ¨™ç›®éŒ„
        os.makedirs(path, exist_ok=True)
        
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ VOC2007 è³‡æ–™å¤¾
        voc_path = os.path.join(path, 'VOC2007')
        if os.path.exists(voc_path) and os.path.exists(os.path.join(voc_path, 'JPEGImages')):
            print(f"âœ… æ‰¾åˆ°å·²å­˜åœ¨çš„ VOC2007 è³‡æ–™é›†: {voc_path}")
            return voc_path
            
        voc_devkit_path = os.path.join(path, 'VOCdevkit', 'VOC2007')
        if os.path.exists(voc_devkit_path) and os.path.exists(os.path.join(voc_devkit_path, 'JPEGImages')):
            print(f"âœ… æ‰¾åˆ°å·²å­˜åœ¨çš„ VOC2007 è³‡æ–™é›†: {voc_devkit_path}")
            return voc_devkit_path
            
        # VOC ä¸‹è¼‰ URL
        DOWNLOAD_URLS = [
            ('http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar',
             '34ed68851bce2a36e2a223fa52c661d592c66b3c'),
            ('http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar',
             '41a8d6e12baa5ab18ee7f8f8029b9e11805b4ef1')
        ]
        
        for url, checksum in DOWNLOAD_URLS:
            filename = os.path.join(path, url.split('/')[-1])
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç¶“å­˜åœ¨
            if not os.path.exists(filename):
                print(f"ğŸ“¥ ä¸‹è¼‰ {url}")
                try:
                    urllib.request.urlretrieve(url, filename)
                    print(f"âœ… ä¸‹è¼‰å®Œæˆ: {filename}")
                except Exception as e:
                    print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
                    continue
                    
            # è§£å£“æ–‡ä»¶
            print(f"ğŸ“¦ è§£å£“ {filename}")
            try:
                with tarfile.open(filename) as tar:
                    tar.extractall(path=path)
                print(f"âœ… è§£å£“å®Œæˆ")
            except Exception as e:
                print(f"âŒ è§£å£“å¤±æ•—: {e}")
                continue
                
        # æª¢æŸ¥æœ€çµ‚è·¯å¾‘
        if os.path.exists(voc_devkit_path):
            print(f"ğŸ“‚ VOC2007 è³‡æ–™é›†ä½ç½®: {voc_devkit_path}")
            return voc_devkit_path
        else:
            print(f"âŒ ä¸‹è¼‰å¾Œæ‰¾ä¸åˆ° VOC2007 è³‡æ–™é›†!")
            raise FileNotFoundError(f"ä¸‹è¼‰å¾Œæ‰¾ä¸åˆ° VOC2007 è³‡æ–™é›†!")

    def _resolve_data_root(self, data_root):
        """è‡ªå‹•ä¿®æ­£ VOCdevkit è·¯å¾‘"""
        potential_paths = [
            data_root,
            os.path.join(data_root, 'VOCdevkit/VOC2007'),
            os.path.join(data_root, 'VOC2007'),
            os.path.join(data_root, 'VOCdevkit/VOCdevkit/VOC2007')
        ]
        for path in potential_paths:
            if os.path.exists(os.path.join(path, 'Annotations')):
                print(f"ğŸ” åµæ¸¬åˆ°æœ‰æ•ˆè³‡æ–™é›†è·¯å¾‘: {path}")
                return path
        return data_root

    def _fix_nested_path_structure(self):
        if 'VOCdevkit/VOCdevkit' in self.data_path:
            corrected = self.data_path.replace('VOCdevkit/VOCdevkit', 'VOCdevkit')
            if os.path.exists(corrected):
                print(f"ğŸ› ï¸ è‡ªå‹•ä¿®æ­£åµŒå¥—è·¯å¾‘: {self.data_path} â†’ {corrected}")
                self.data_path = corrected

    def _validate_dataset_structure(self):
        required_dirs = ['Annotations', 'JPEGImages', 'ImageSets/Main']
        return all(os.path.isdir(os.path.join(self.data_path, d)) for d in required_dirs)

    def _verify_split_files(self):
        # æ”¯æŒçš„åˆ†å‰²æ–‡ä»¶
        valid_splits = ['train', 'val', 'test', 'trainval']
        if self.split not in valid_splits:
            raise ValueError(f"ç„¡æ•ˆçš„åˆ†å‰²é¡å‹: {self.split}ï¼Œæ”¯æŒçš„é¡å‹: {valid_splits}")
            
        split_file = os.path.join(self.data_path, 'ImageSets/Main', f'{self.split}.txt')
        if not os.path.isfile(split_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°åˆ†å‰²æª”æ¡ˆ: {split_file}")

    def _precache_metadata(self):
        """ä¸¦è¡Œé è¼‰æ‰€æœ‰æ¨™è¨»è³‡æ–™åˆ°è¨˜æ†¶é«”"""
        from concurrent.futures import ThreadPoolExecutor, as_completed

        split_file = os.path.join(self.data_path, 'ImageSets/Main', f'{self.split}.txt')
        with open(split_file, 'r') as f:
            ids = [line.strip() for line in f if line.strip()]

        self.samples = []
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            futures = {executor.submit(self._safe_parse_annotation, img_id): img_id for img_id in ids}
            for future in tqdm(as_completed(futures), total=len(ids), desc="ğŸš€ é è¼‰æ¨™è¨»è³‡æ–™"):
                img_id = futures[future]
                try:
                    result = future.result()
                    if result:
                        self.samples.append(img_id)
                        self.cache[img_id] = result
                except Exception as e:
                    print(f"âš ï¸ è·³éç„¡æ•ˆæ¨£æœ¬ {img_id}: {str(e)}")

    def _safe_parse_annotation(self, img_id):
        img_path = os.path.join(self.data_path, 'JPEGImages', f'{img_id}.jpg')
        annot_path = os.path.join(self.data_path, 'Annotations', f'{img_id}.xml')
        if not os.path.isfile(img_path):
            raise FileNotFoundError(f"åœ–ç‰‡æª”æ¡ˆä¸å­˜åœ¨: {img_path}")
        if not os.path.isfile(annot_path):
            raise FileNotFoundError(f"æ¨™è¨»æª”æ¡ˆä¸å­˜åœ¨: {annot_path}")

        tree = ET.parse(annot_path)
        root = tree.getroot()
        boxes = []
        labels = []
        for obj in root.findall('object'):
            name = obj.find('name').text
            if name not in VOC_CLASSES:
                continue
            bbox = obj.find('bndbox')
            box = [
                float(bbox.find('xmin').text),
                float(bbox.find('ymin').text),
                float(bbox.find('xmax').text),
                float(bbox.find('ymax').text)
            ]
            boxes.append(box)
            label_idx = VOC_CLASSES.index(name)
            labels.append(label_idx)
            
        return img_path, boxes, labels

    def __getitem__(self, idx):
        """ç²å–è³‡æ–™é›†ä¸­çš„ä¸€å€‹æ¨£æœ¬ï¼Œä¸¦æå–é¡åˆ¥åœ–åƒ"""
        img_id = self.samples[idx]
        img_path, boxes, labels = self.cache[img_id]
        
        # è¼‰å…¥åœ–åƒ
        img = Image.open(img_path).convert('RGB')
        img = np.asarray(img).copy()
        
        # ç¢ºä¿æ‰€æœ‰æ•¸æ“šéƒ½æ˜¯å¼µé‡æ ¼å¼
        if not isinstance(img, torch.Tensor):
            # å°‡åœ–åƒè½‰æ›ç‚ºå¼µé‡
            if isinstance(img, np.ndarray):
                img = torch.from_numpy(img).float()
            else:
                img = torch.tensor(img, dtype=torch.float)
        
        # ç¢ºä¿é‚Šç•Œæ¡†æ˜¯å¼µé‡
        if isinstance(boxes, list):
            if len(boxes) > 0:
                boxes = torch.tensor(boxes, dtype=torch.float)
            else:
                boxes = torch.zeros((0, 4), dtype=torch.float)
        
        # ç¢ºä¿æ¨™ç±¤æ˜¯å¼µé‡
        if isinstance(labels, list):
            if len(labels) > 0:
                # æ‡‰ç”¨é¡åˆ¥æ˜ å°„
                mapped_labels = [self.class_mapping.get(l, l) for l in labels]
                labels = torch.tensor(mapped_labels, dtype=torch.long)
            else:
                labels = torch.zeros((0,), dtype=torch.long)
        
        # è½‰æ›åœ–åƒé€šé“é †åº
        if img.shape[0] == 3:  # å·²ç¶“æ˜¯ [C, H, W] æ ¼å¼
            pass
        elif img.shape[2] == 3:  # å¦‚æœæ˜¯ [H, W, C] æ ¼å¼
            img = img.permute(2, 0, 1)
        
        # æ­£è¦åŒ–åƒç´ å€¼åˆ° [0, 1]
        if img.max() > 1.0:
            img = img / 255.0
        
        # èª¿æ•´åœ–åƒå°ºå¯¸ (å¦‚æœæŒ‡å®šäº†ç›®æ¨™å°ºå¯¸)
        original_size = (img.shape[2], img.shape[1])  # (W, H)
        if self.img_size is not None:
            # èª¿æ•´åœ–åƒå°ºå¯¸
            img = F.interpolate(img.unsqueeze(0), size=self.img_size, 
                               mode='bilinear', align_corners=False).squeeze(0)
            
            # èª¿æ•´æ¡†åº§æ¨™ - æ ¹æ“šç¸®æ”¾æ¯”ä¾‹
            if boxes.numel() > 0:
                scale_w = self.img_size[0] / original_size[0]
                scale_h = self.img_size[1] / original_size[1]
                
                # æ‡‰ç”¨ç¸®æ”¾
                boxes[:, 0] *= scale_w  # x_min
                boxes[:, 2] *= scale_w  # x_max
                boxes[:, 1] *= scale_h  # y_min
                boxes[:, 3] *= scale_h  # y_max
                
                # ç¢ºä¿æ¡†åº§æ¨™åœ¨æœ‰æ•ˆç¯„åœå…§
                boxes[:, 0].clamp_(0, self.img_size[0])
                boxes[:, 2].clamp_(0, self.img_size[0])
                boxes[:, 1].clamp_(0, self.img_size[1])
                boxes[:, 3].clamp_(0, self.img_size[1])
                
        # æ‡‰ç”¨åœ–åƒè½‰æ›
        if self.transform:
            img = self.transform(img)
            
        # æ‡‰ç”¨ç›®æ¨™è½‰æ›
        if self.target_transform:
            boxes, labels = self.target_transform(boxes, labels)
        
        # é—œéµæ”¹é€²ï¼šå¾ç›®æ¨™å€åŸŸæå–é¡åˆ¥åœ–åƒ
        class_images = []
        class_size = 64  # é¡åˆ¥åœ–åƒæ¨™æº–å°ºå¯¸
        
        if len(boxes) > 0:
            for i in range(min(len(boxes), 3)):  # æœ€å¤šå–3å€‹ç›®æ¨™å€åŸŸä½œç‚ºé¡åˆ¥åœ–åƒ
                # ç²å–æ¡†åº§æ¨™
                x1, y1, x2, y2 = boxes[i].tolist()
                
                # ç¢ºä¿åº§æ¨™æ˜¯æ•´æ•¸ä¸”åœ¨æœ‰æ•ˆç¯„åœå…§
                h, w = img.shape[1], img.shape[2]
                x1, y1 = max(0, int(x1)), max(0, int(y1))
                x2, y2 = min(w - 1, int(x2)), min(h - 1, int(y2))
                
                if x2 > x1 and y2 > y1:
                    # æå–ç›®æ¨™å€åŸŸ
                    class_img = img[:, y1:y2, x1:x2].clone()
                    
                    # èª¿æ•´å°ºå¯¸ç‚ºæ¨™æº–é¡åˆ¥åœ–åƒå°ºå¯¸
                    class_img = torch.nn.functional.interpolate(
                        class_img.unsqueeze(0),  # [1, C, H, W]
                        size=(class_size, class_size),
                        mode='bilinear',
                        align_corners=False
                    ).squeeze(0)  # [C, class_size, class_size]
                    
                    class_images.append(class_img)
        
        # å¦‚æœæ²’æœ‰æœ‰æ•ˆé¡åˆ¥åœ–åƒï¼Œå‰µå»ºä¸€å€‹é»˜èªçš„
        if not class_images:
            # ä½¿ç”¨ä¸­å¿ƒå€åŸŸä½œç‚ºé»˜èªé¡åˆ¥åœ–åƒ
            h, w = img.shape[1], img.shape[2]
            center_h, center_w = h // 2, w // 2
            size_h, size_w = h // 4, w // 4
            
            y1, y2 = max(0, center_h - size_h), min(h, center_h + size_h)
            x1, x2 = max(0, center_w - size_w), min(w, center_w + size_w)
            
            default_class = img[:, y1:y2, x1:x2].clone()
            default_class = torch.nn.functional.interpolate(
                default_class.unsqueeze(0),
                size=(class_size, class_size),  # ä¿®æ­£ï¼šæ·»åŠ å¤§å°åƒæ•¸
                mode='bilinear',  # ä¿®æ­£ï¼šæŒ‡å®šæ’å€¼æ¨¡å¼
                align_corners=False
            ).squeeze(0)
            
            class_images.append(default_class)
        
        # å°‡é¡åˆ¥åœ–åƒå †ç–Šç‚ºå–®ä¸€å¼µé‡
        if len(class_images) > 1:
            # å¤šå€‹é¡åˆ¥åœ–åƒï¼Œå †ç–Šç‚º [num_classes, C, H, W]
            class_images = torch.stack(class_images)
        else:
            # å–®å€‹é¡åˆ¥åœ–åƒï¼Œç¢ºä¿ç‚º [1, C, H, W]
            class_images = class_images[0].unsqueeze(0)
        
        # print(f"VOC è¿”å›: åœ–åƒå½¢ç‹€={img.shape}, æ¡†å½¢ç‹€={boxes.shape}, é¡åˆ¥å½¢ç‹€={labels.shape}, é¡åˆ¥åœ–åƒå½¢ç‹€={class_images.shape}")
        
        # è¿”å›åœ–åƒã€ç›®æ¨™æ¡†ã€é¡åˆ¥æ¨™ç±¤å’Œé¡åˆ¥åœ–åƒ
        return img, boxes, labels, class_images

    def __len__(self):
        return len(self.samples)
        
    @staticmethod
    def collate_fn(batch):
        """è‡ªå®šç¾©æ‰¹æ¬¡æ•´åˆå‡½æ•¸ï¼Œå°‡ä¸»åœ–åƒresizeç‚º224Ã—224ï¼Œé¡åˆ¥åœ–åƒresizeç‚º64Ã—64ï¼ˆæˆ–224Ã—224ï¼‰ï¼Œä¸¦çµ„æˆbatch"""
        images = []
        boxes_list = []
        labels_list = []
        class_images_list = []

        for img, boxes, labels, class_images in batch:
            # ä¿è­‰ä¸»åœ–åƒç‚º [3, 224, 224]
            if img.shape[1:] != (224, 224):
                img = torch.nn.functional.interpolate(
                    img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False
                ).squeeze(0)
            images.append(img)

            boxes_list.append(boxes)
            labels_list.append(labels)

            # ä¿è­‰æ¯å€‹ class image ç‚º [N, 3, 64, 64] æˆ– [N, 3, 224, 224]
            resized_class_images = []
            for cimg in class_images:
                if cimg.shape[1:] != (64, 64):  # ä½ ä¹Ÿå¯ä»¥ç”¨ (224, 224)
                    cimg = torch.nn.functional.interpolate(
                        cimg.unsqueeze(0), size=(64, 64), mode='bilinear', align_corners=False
                    ).squeeze(0)
                resized_class_images.append(cimg)
            # å †ç–Š
            resized_class_images = torch.stack(resized_class_images)
            class_images_list.append(resized_class_images)

        # çµ„ batch
        images = torch.stack(images)  # [B, 3, 224, 224]
        # é¡åˆ¥åœ–åƒåˆä½µæˆä¸€å€‹å¤§ tensor [sum_N, 3, 64, 64]
        all_class_images = torch.cat(class_images_list, dim=0)

        return [images, boxes_list, labels_list, all_class_images]
